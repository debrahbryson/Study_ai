[
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nChapter 8:  Main Memory  \n8.2 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nChapter 8:  Memory Management  \n\uf06eBackground \n\uf06eSwapping  \n\uf06eContiguous Memory Allocation \n\uf06eSegmentation \n\uf06ePaging \n\uf06eStructure of the Page Table \n\uf06eExample: The Intel 32 and 64-bit Architectures  \n\uf06eExample: ARM Architecture  \n8.3 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nObjectives \n\uf06eTo provide a detailed description of various ways of \norganizing memory hardware \n\uf06eTo discuss various memory -management techniques, \nincluding paging and segmentation \n\uf06eTo provide a detailed description of the Intel Pentium, which supports both pure segmentation and segmentation with paging \n8.4 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nBackground  \n\uf06eProgram must be brought (from disk)  into memory and \nplaced within a process for it to be run  \n\uf06eMain memory and registers are only storage CPU can access directly  \n\uf06eMemory unit only sees a stream of addresses + read requests, or address + data and write requests\n \n\uf06eRegister access in one CPU clock (or less)  \n\uf06eMain memory can take many cycles, causing a stall  \n\uf06eCache sits between main memory and CPU registers  \n\uf06eProtection of memory required to ensure correct operation \n \n8.5 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nBase and Limit Registers \n\uf06eA pair of base and limit registers define the logical address space \n\uf06eCPU must check every memory access generated in user mode to \nbe sure it is between base and limit for that user"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.6 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nHardware Address Protection  \n8.7 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nAddress Binding  \n\uf06ePrograms on disk, ready to be brought into memory to execute form an \ninput queue  \n\uf06cWithout support, must be loaded into address 0000 \n\uf06eInconvenient to have first user process physical address always at 0000  \n\uf06cHow can it not be? \n\uf06eFurther, addresses represented in different ways at different stages of a \nprogram \u2019s life \n\uf06cSource code addresses usually symbolic  \n\uf06cCompiled code addresses bind to relocatable addresses  \n\uf034i.e. \u201c 14 bytes from beginning of this module\u201d  \n\uf06cLinker or loader will bind relocatable addresses to absolute addresses  \n\uf034i.e. 74014 \n\uf06cEach binding maps one address space to another  \n \n \n8.8 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nBinding of Instructions and Data to Memory  \n \n\uf06eAddress binding of instructions and data to memory addresses \ncan happen at three different stages  \n\uf06cCompile time :  If memory location known a priori, absolute \ncode  can be generated; must recompile code if starting \nlocation changes  \n\uf06cLoad time :  Must generate relocatable code if memory \nlocation is not known at compile time \n\uf06cExecution time:  Binding delayed until run time if the process can be moved during its execution from one memory segment to another  \n\uf034Need hardware support for address maps (e.g., base and limit registers) \n8.9 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nMultistep Processing of a User Program"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.10 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nLogical vs. Physical Address Space \n\uf06eThe concept of a logical address space that is bound to a \nseparate physical address space is central to proper memory \nmanagement  \n\uf06cLogical address \u2013 generated by the CPU; also referred to \nas virtual address \n\uf06cPhysical address \u2013 address seen by the memory unit  \n\uf06eLogical and physical addresses are the same in compile-time and load-time address -binding schemes; logical (virtual) and \nphysical addresses differ in execution-time address -binding \nscheme  \n\uf06eLogical address space is the set of all logical addresses \ngenerated by a program  \n\uf06ePhysical address space is the set of all physical addresses generated by a program  \n \n \n8.11 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nMemory-Management Unit ( MMU) \n\uf06eHardware device that at run time maps virtual to physical \naddress  \n\uf06eMany methods possible, covered in the rest of this chapter  \n\uf06eTo start, consider simple scheme where the value in the relocation register is added to every address generated by a user process at the time it is sent to memory  \n\uf06cBase register now called relocation register  \n\uf06cMS-DOS on Intel 80x86 used 4 relocation registers  \n\uf06eThe user program deals with logical  addresses; it never sees the \nreal physical addresses  \n\uf06cExecution-time binding occurs when reference is made to location in memory  \n\uf06cLogical address bound to physical addresses  \n8.12 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nDynamic relocation using a relocation register  \n\uf06e Routine is not loaded until it is \ncalled \n\uf06e Better memory -space utilization; \nunused routine is never loaded \n\uf06e All routines kept on disk in relocatable load format  \n\uf06e Useful when large amounts of code are needed to handle infrequently occurring cases  \n\uf06e No special support from the operating system is required \n\uf06c Implemented through program \ndesign  \n\uf06c OS can help by providing libraries to implement dynamic loading  \n8.13 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nDynamic Linking  \n\uf06eStatic linking \u2013 system libraries and program code combined by \nthe loader into the binary program image \n\uf06eDynamic linking \u2013linking postponed until execution time  \n\uf06eSmall piece of code, stub , used to locate the appropriate \nmemory -resident library routine  \n\uf06eStub replaces itself with the address of the routine, and executes \nthe routine  \n\uf06eOperating system checks if routine is in processes \u2019 memory \naddress  \n\uf06cIf not in address space, add to address space  \n\uf06eDynamic linking is particularly useful for libraries  \n\uf06eSystem also known as shared libraries \n\uf06eConsider applicability to patching system libraries  \n\uf06cVersioning may be needed \n8.14 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSwapping  \n\uf06eA process can be swapped  temporarily out of memory to a \nbacking store, and then brought back into memory for continued \nexecution \n\uf06cTotal physical memory space of processes can exceed physical memory  \n\uf06eBacking store \u2013 fast disk large enough to accommodate copies \nof all memory images for all users; must provide direct access to these memory images  \n\uf06eRoll out, roll in \u2013 swapping variant used for priority -based \nscheduling algorithms; lower -priority process is swapped out so \nhigher -priority process can be loaded and executed \n\uf06eMajor part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped \n\uf06eSystem maintains a ready queue  of ready -to-run processes \nwhich have memory images on disk  \n8.15 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSwapping (Cont.) \n\uf06eDoes the swapped out process need to swap back in to same \nphysical addresses? \n\uf06eDepends on address binding method \n\uf06cPlus consider pending I/O to / from process memory space \n\uf06eModified versions of swapping are found on many systems (i.e., UNIX, Linux, and Windows)  \n\uf06cSwapping normally disabled \n\uf06cStarted if more than threshold amount of memory allocated \n\uf06cDisabled again once memory demand reduced below \nthreshold \n \n8.16 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSchematic View of Swapping"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.17 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nContext Switch Time including Swapping  \n\uf06eIf next processes to be put on CPU is not in memory, need to \nswap out a process and swap in target process  \n\uf06eContext switch time can then be very high \n\uf06e100MB process swapping to hard disk with transfer rate of 50MB/sec  \n\uf06cSwap out time of 2000 ms  \n\uf06cPlus swap in of same sized process  \n\uf06cTotal context switch swapping component time of 4000ms (4 seconds)  \n\uf06eCan reduce if reduce size of memory swapped \u2013 by knowing \nhow much memory really being used \n\uf06cSystem calls to inform OS of memory use via \nrequest_memory() and release_memory() \n8.18 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nContext Switch Time and Swapping (Cont.)  \n\uf06eOther constraints as well on swapping \n\uf06cPending I/O \u2013 can\u2019t swap out as I/O would occur to wrong \nprocess  \n\uf06cOr always transfer I/O to kernel space, then to I/O device \n\uf034Known as double buffering , adds overhead \n\uf06eStandard swapping not used in modern operating systems  \n\uf06cBut modified version common \n\uf034Swap only when free memory extremely low  \n8.19 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSwapping on Mobile Systems  \n\uf06eNot typically supported \n\uf06cFlash memory based \n\uf034Small amount of space \n\uf034Limited number of write cycles  \n\uf034Poor throughput between flash memory and CPU on mobile \nplatform  \n\uf06eInstead use other methods to free memory if low  \n\uf06ciOS asks apps to voluntarily relinquish allocated memory  \n\uf034Read-only data thrown out and reloaded from flash if needed \n\uf034Failure to free can result in termination \n\uf06cAndroid terminates apps if low free memory, but first writes application state to flash for fast restart  \n\uf06cBoth OSes support paging as discussed below  \n8.20 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nContiguous Allocation  \n\uf06eMain memory must support both OS and user processes  \n\uf06eLimited resource, must allocate efficiently  \n\uf06eContiguous allocation is one early method \n\uf06eMain memory usually into two partitions : \n\uf06cResident operating system, usually held in low memory with \ninterrupt vector  \n\uf06cUser processes then held in high memory  \n\uf06cEach process contained in single contiguous section of memory  \n \n8.21 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nContiguous Allocation (Cont.) \n\uf06eRelocation registers used to protect user processes from each \nother, and from changing operating-system code and data \n\uf06cBase register contains value of smallest physical address  \n\uf06cLimit register contains range of logical addresses \u2013 each \nlogical address must be less than the limit register  \n\uf06cMMU maps logical address dynamically  \n\uf06cCan then allow actions such as kernel code being transient \nand kernel changing size \n8.22 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nHardware Support for Relocation and Limit Registers"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.23 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nMultiple -partition allocation  \n \n\uf06eMultiple -partition allocation \n\uf06cDegree of multiprogramming limited by number of partitions  \n\uf06cVariable- partition sizes for efficiency (sized to a given process\u2019 needs)  \n\uf06cHole \u2013 block of available memory; holes of various size are scattered \nthroughout memory  \n\uf06cWhen a process arrives, it is allocated memory from a hole large enough to accommodate it \n\uf06cProcess exiting frees its partition, adjacent free partitions combined \n\uf06cOperating system maintains information about: a) allocated partitions    b) free partitions (hole)"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.24 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nDynamic Storage -Allocation Problem  \n\uf06eFirst -fit:  Allocate the first hole that is big enough \n \n\uf06eBest -fit:  Allocate the smallest  hole that is big enough; must \nsearch entire list, unless ordered by size   \n\uf06cProduces the smallest leftover hole \n \n\uf06eWorst -fit:  Allocate the largest  hole; must also search entire list   \n\uf06cProduces the largest leftover hole How to satisfy a request of size n from a list of free holes? \nFirst-fit and best -fit better than worst -fit in terms of speed and storage \nutilization  \n8.25 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nFragmentation  \n\uf06eExternal Fragmentation  \u2013 total memory space exists to \nsatisfy a request, but it is not contiguous  \n\uf06eInternal Fragmentation  \u2013 allocated memory may be slightly \nlarger than requested memory; this size difference is memory \ninternal to a partition, but not being used \n\uf06eFirst fit analysis reveals that given N blocks allocated, 0.5 N  \nblocks lost to fragmentation \n\uf06c1/3 may be unusable -> 50-percent rule \n8.26 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nFragmentation (Cont.) \n\uf06eReduce external fragmentation by compaction  \n\uf06cShuffle memory contents to place all free memory together \nin one large block  \n\uf06cCompaction is possible only if relocation is dynamic, and is \ndone at execution time \n\uf06cI/O problem  \n\uf034Latch job in memory while it is involved in I/O  \n\uf034Do I/O only into OS buffers  \n\uf06eNow consider that backing store has same fragmentation problems  \n8.27 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSegmentation  \n\uf06eMemory -management scheme that supports user view of memory  \n\uf06eA program is a collection of segments  \n\uf06cA segment is a logical unit such as:  \n  main program  \n  procedure  \n  function \n  method \n  object  \n  local variables, global variables  \n  common block  \n  stack \n  symbol table \n  arrays  \n8.28 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nUser \u2019s View of a Program"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.29 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nLogical View of Segmentation  \n1 \n3 2 \n4 1 \n4 \n2 \n3 \nuser space  physical memory space \n8.30 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSegmentation Architecture  \n\uf06eLogical address consists of a two tuple:  \n  <segment -number, offset>,  \n \n\uf06eSegment table \u2013 maps two-dimensional physical addresses; each \ntable entry has:  \n\uf06cbase \u2013 contains the starting physical address where the \nsegments reside in memory  \n\uf06climit \u2013 specifies the length of the segment  \n \n\uf06eSegment -table base register (STBR)  points to the segment \ntable\u2019s location in memory  \n \n\uf06eSegment -table length register (STLR)  indicates number of \nsegments used by a program;  \n                   segment number s  is legal if s  < STLR  \n8.31 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSegmentation Architecture (Cont.) \n\uf06eProtection \n\uf06cWith each entry in segment table associate:  \n\uf034validation bit = 0 \u21d2 illegal segment  \n\uf034read/write/execute privileges  \n\uf06eProtection bits associated with segments; code sharing \noccurs at segment level  \n\uf06eSince segments vary in length, memory allocation is a dynamic storage-allocation problem  \n\uf06eA segmentation example is shown in the following diagram  \n8.32 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nSegmentation Hardware"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.33 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging  \n\uf06ePhysical  address space of a process can be noncontiguous; \nprocess is allocated physical memory whenever the latter is available \n\uf06cAvoids external fragmentation \n\uf06cAvoids problem of varying sized memory chunks  \n\uf06eDivide physical memory into fixed-sized blocks called frames \n\uf06cSize is power of 2, between 512 bytes and 16 Mbytes  \n\uf06eDivide logical memory into blocks of same size called pages  \n\uf06eKeep track of all free frames  \n\uf06eTo run a program of size N  pages, need to find N  free frames and \nload program  \n\uf06eSet up a page table to translate logical to physical addresses  \n\uf06eBacking store likewise split into pages  \n\uf06eStill have Internal fragmentation \n8.34 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nAddress Translation Scheme  \n\uf06eAddress generated by CPU is divided into:  \n\uf06cPage number (p) \u2013 used as an index into a page table which \ncontains base address of each page in physical memory  \n\uf06cPage offset (d) \u2013 combined with base address to define the \nphysical memory address that is sent to the memory unit  \n \n \n \n \n\uf06cFor given logical address space 2m and page size 2n page numberpage offset\np d\nm -n n\n8.35 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging Hardware"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.36 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging Model of Logical and  Physical Memory"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.37 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging Example  \nn=2 and m =4   32- byte memory and 4- byte pages"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.38 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging (Cont.) \n\uf06eCalculating internal fragmentation \n\uf06cPage size = 2,048 bytes  \n\uf06cProcess size = 72,766 bytes  \n\uf06c35 pages + 1,086 bytes  \n\uf06cInternal fragmentation of 2,048 - 1,086 = 962 bytes  \n\uf06cWorst case fragmentation = 1 frame \u2013 1 byte \n\uf06cOn average fragmentation = 1 / 2 frame size \n\uf06cSo small frame sizes desirable? \n\uf06cBut each page table entry takes memory to track  \n\uf06cPage sizes growing over time \n\uf034Solaris supports two page sizes \u2013 8 KB and 4 MB  \n\uf06eProcess view and physical memory now very different  \n\uf06eBy implementation process can only access its own memory  \n8.39 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nFree Frames \nBefore allocation After allocation"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.40 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nImplementation of Page Table  \n\uf06ePage table is kept in main memory  \n\uf06ePage-table base register (PTBR) points to the page table \n\uf06ePage-table length register (PTLR ) indicates size of the page \ntable \n\uf06eIn this scheme every data/instruction access requires two \nmemory accesses \n\uf06cOne for the page table and one for the data / instruction \n\uf06eThe two memory access problem can be solved by the use of a special fast -lookup hardware cache called associative \nmemory or translation look -aside buffers (TLBs ) \n8.41 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nImplementation of Page Table (Cont.) \n\uf06eSome TLBs store  address-space identifiers (ASIDs ) in each \nTLB entry \u2013 uniquely identifies each process to provide \naddress -space protection for that process  \n\uf06cOtherwise need to flush at every context switch \n\uf06eTLBs typically small (64 to 1,024 entries)  \n\uf06eOn a TLB miss, value is loaded into the TLB for faster access \nnext time \n\uf06cReplacement policies must be considered \n\uf06cSome entries can be wired down for permanent fast \naccess \n8.42 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nAssociative Memory \n\uf06eAssociative memory \u2013 parallel search  \n \n \n \n \n \n\uf06eAddress translation (p, d)  \n\uf06cIf p is in associative register, get frame # out  \n\uf06cOtherwise get frame # from page table in memory  \n Page # Frame #\n8.43 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nPaging Hardware With TLB"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.44 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nEffective Access Time \n\uf06eAssociative Lookup = \u03b5  time unit  \n\uf06cCan be < 10% of memory access time \n\uf06eHit ratio = \u03b1 \n\uf06cHit ratio \u2013 percentage of times that a page number is found in the \nassociative registers; ratio related to number of associative \nregisters  \n\uf06eConsider \u03b1 = 80%, \u03b5 = 20ns for TLB search, 100ns for memory access  \n\uf06eEffective Access Time (EAT ) \n  EAT = (1 + \u03b5) \u03b1 + (2 + \u03b5)(1 \u2013 \u03b1) \n   = 2 + \u03b5 \u2013 \u03b1 \n\uf06e Consider \u03b1 = 80%, \u03b5 = 20ns for TLB search, 100ns for memory access  \n\uf06cEAT = 0.80 x 100 + 0.20 x 200 = 120ns  \n\uf06eConsider more realistic hit ratio ->  \u03b1 = 99%, \u03b5 = 20ns for TLB search, \n100ns for memory access  \n\uf06cEAT = 0.99 x 100 + 0.01 x 200 = 101ns  \n \n \n8.45 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nMemory Protection  \n\uf06eMemory protection implemented by associating protection bit \nwith each frame to indicate if read-only or read- write access is \nallowed \n\uf06cCan also add more bits to indicate page execute-only, and so on \n\uf06eValid-invalid bit attached to each entry in the page table:  \n\uf06c\u201cvalid \u201d indicates that the associated page is in the \nprocess \u2019 logical address space, and is thus a legal page \n\uf06c\u201cinvalid \u201d indicates that the page is not in the process \u2019 \nlogical address space \n\uf06cOr use page -table length register (PTLR ) \n\uf06eAny violations result in a trap to the kernel  \n8.46 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nValid (v) or Invalid (i) Bit In A Page Table"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.47 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nShared Pages  \n\uf06eShared code  \n\uf06cOne copy of read-only ( reentrant ) code shared among \nprocesses (i.e., text editors, compilers, window systems) \n\uf06cSimilar to multiple threads sharing the same process space \n\uf06cAlso useful for interprocess communication if sharing of \nread-write pages is allowed \n\uf06ePrivate code and data  \n\uf06cEach process keeps a separate copy of the code and data \n\uf06cThe pages for the private code and data can appear anywhere in the logical address space \n8.48 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nShared Pages Example"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.49 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nStructure of the Page Table  \n\uf06eMemory structures for paging can get huge using straight -\nforward methods  \n\uf06cConsider a 32-bit logical address space as on modern \ncomputers  \n\uf06cPage size of 4 KB (212) \n\uf06cPage table would have 1 million entries (232 / 212) \n\uf06cIf each entry is 4 bytes -> 4 MB of physical address space / \nmemory for page table alone \n\uf034That amount of memory used to cost a lot  \n\uf034Don\u2019t want to allocate that contiguously in main memory  \n\uf06eHierarchical Paging \n\uf06eHashed Page Tables  \n\uf06eInverted Page Tables  \n8.50 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nHierarchical Page Tables \n\uf06eBreak up the logical address space into multiple page \ntables  \n\uf06eA simple technique is a two-level page table \n\uf06eWe then page the page table \n8.51 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nTwo-Level Page-Table Scheme"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.52 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nTwo-Level Paging Example  \n\uf06eA logical address (on 32-bit machine with 1K page size) is divided into:  \n\uf06ca page number consisting of 22 bits  \n\uf06ca page offset consisting of 10 bits  \n \n\uf06eSince the page table is paged, the page number is further divided into:  \n\uf06ca 12-bit page number  \n\uf06ca 10-bit page offset  \n \n\uf06eThus, a logical address is as follows:  \n \n \n \n \n \n \n\uf06ewhere p1 is an index into the outer page table, and p2 is the \ndisplacement within the page of the inner page table \n\uf06eKnown as forward -mapped page table"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.53 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nAddress -Translation Scheme"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.54 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \n64-bit Logical Address Space  \n\uf06eEven two-level paging scheme not sufficient  \n\uf06eIf page size is 4 KB (212) \n\uf06cThen page table has 252 entries  \n\uf06cIf two level scheme, inner page tables could be 210 4-byte entries  \n\uf06cAddress would look like \n \n \n \n\uf06cOuter page table has 242 entries or 244 bytes  \n\uf06cOne solution is to add a 2nd outer page table \n\uf06cBut in the following example the 2nd outer page table is still 234 bytes in \nsize \n\uf034And possibly 4 memory access to get to one physical memory \nlocation"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.55 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nThree -level Paging Scheme"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.56 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nHashed Page Tables \n\uf06eCommon in address spaces > 32 bits  \n\uf06eThe virtual page number is hashed into a page table \n\uf06cThis page table contains a chain of elements hashing to the same \nlocation \n\uf06eEach element contains (1) the virtual page number (2) the value of the mapped page frame (3) a pointer to the next element  \n\uf06eVirtual page numbers are compared in this chain searching for a match  \n\uf06cIf a match is found, the corresponding physical frame is extracted \n\uf06eVariation for 64-bit addresses is clustered page tables \n\uf06cSimilar to hashed but each entry refers to several pages (such as 16) rather than 1 \n\uf06cEspecially useful for sparse address spaces (where memory \nreferences are non-contiguous and scattered)  \n8.57 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nHashed Page Table"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.58 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nInverted Page Table  \n\uf06eRather than each process having a page table and keeping track \nof all possible logical pages, track all physical pages  \n\uf06eOne entry for each real page of memory  \n\uf06eEntry consists of the virtual address of the page stored in that real memory location, with information about the process that owns that page \n\uf06eDecreases memory needed to store each page table, but increases time needed to search the table when a page reference occurs  \n\uf06eUse hash table to limit the search to one \u2014 or at most a few \u2014 \npage-table entries  \n\uf06cTLB can accelerate access  \n\uf06eBut how to implement shared memory? \n\uf06cOne mapping of a virtual address to the shared physical address  \n8.59 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nInverted Page Table Architecture"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.60 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nOracle SPARC Solaris  \n\uf06eConsider modern, 64-bit operating system example with tightly \nintegrated HW  \n\uf06cGoals are efficiency, low overhead \n\uf06eBased on hashing, but more complex  \n\uf06eTwo hash tables  \n\uf06cOne kernel and one for all user processes  \n\uf06cEach maps memory addresses from virtual to physical memory  \n\uf06cEach entry represents a contiguous area of mapped virtual memory,  \n\uf034More efficient than having a separate hash-table entry for each page \n\uf06cEach entry has  base address and  span (indicating the number of pages the entry represents)  \n \n8.61 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nOracle SPARC Solaris (Cont.) \n\uf06eTLB holds translation table entries (TTEs) for fast hardware lookups  \n\uf06cA cache of TTEs reside in a translation storage buffer (TSB)  \n\uf034Includes an entry per recently accessed page \n\uf06eVirtual address reference causes TLB search  \n\uf06cIf miss, hardware walks the in-memory TSB looking for the TTE \ncorresponding to the address  \n\uf034If match found, the CPU copies the TSB entry into the TLB and translation completes  \n\uf034If no match found, kernel interrupted to search the hash table \n\u2013The kernel then creates a TTE from the appropriate hash table and stores it in the TSB, Interrupt handler returns control to the MMU, which completes the address translation.  \n \n8.62 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nExample: The Intel 32 and 64- bit Architectures \n\uf06eDominant industry chips  \n \n\uf06ePentium CPUs are 32-bit and called IA -32 architecture \n \n\uf06eCurrent Intel CPUs are 64-bit and called IA -64 architecture \n \n\uf06eMany variations in the chips, cover the main ideas here \n \n8.63 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nExample: The Intel IA -32 Architecture \n\uf06eSupports both segmentation and segmentation with paging \n\uf06cEach segment can be 4 GB  \n\uf06cUp to 16 K segments per process  \n\uf06cDivided into two partitions  \n\uf034First partition of up to 8 K segments are private to \nprocess (kept in local descriptor table (LDT )) \n\uf034Second partition of up to 8K segments shared among all processes (kept in global descriptor table (GDT )) \n \n8.64 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nExample: The Intel IA -32 Architecture (Cont.)  \n\uf06eCPU generates logical address  \n\uf06cSelector given to segmentation unit  \n\uf034Which produces linear addresses  \n \n \n\uf06cLinear address given to paging unit  \n\uf034Which generates physical address in main memory  \n\uf034Paging units form equivalent of MMU  \n\uf034Pages sizes can be 4 KB or 4 MB"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.65 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nLogical to Physical Address Translation in IA -32"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.66 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nIntel IA -32 Segmentation"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.67 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nIntel IA -32 Paging Architecture"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "8.68 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nIntel IA -32 Page Address Extensions \n\uf06e 32-bit address limits led Intel to create page address extension (PAE), \nallowing 32- bit apps access to more than 4GB of memory space \n\uf06cPaging went to a 3- level scheme \n\uf06cTop two bits refer to a page directory pointer table  \n\uf06cPage- directory and page- table entries moved to 64- bits in size \n\uf06cNet effect is increasing address space to 36 bits \u2013 64GB of physical \nmemory  \n \n \n8.69 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nIntel x86-64 \n\uf06e Current generation Intel x86 architecture \n\uf06e 64 bits is ginormous (> 16 exabytes)  \n\uf06e In practice only implement 48 bit addressing \n\uf06cPage sizes of 4 KB, 2 MB, 1 GB  \n\uf06cFour levels of paging hierarchy  \n\uf06e Can also use PAE so virtual addresses are 48 bits and physical \naddresses are 52 bits  \n8.70 Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nExample: ARM Architecture  \n\uf06e Dominant mobile platform chip \n(Apple iOS and Google Android devices for example)  \n\uf06e Modern, energy efficient, 32 -bit \nCPU \n\uf06e 4 KB and 16 KB pages  \n\uf06e 1 MB and 16 MB pages (termed sections)  \n\uf06e One-level paging for sections, two -\nlevel for smaller pages  \n\uf06e Two levels of TLBs  \n\uf06c Outer level has two micro TLBs (one data, one instruction)  \n\uf06c Inner is single main TLB  \n\uf06c First inner is checked, on miss outers are checked, and on miss page table walk performed by CPU"
  },
  {
    "source": "ch8 - Main Memory.pdf",
    "text": "Silberschatz, Galvin and Gagne \u00a92013 Operating System Concepts \u2013 9th Edition  \nEnd of Chapter 8"
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 2 - Arithmetic and Logic Unit (ALU)\nby\nE. Kondela\n                                 \nALU is responsible to perform the operation in the computer. The basic operations are implemented in hardware\nlevel. ALU is having collection of two types of operations: \n\uf0b7Arithmetic operations\n\uf0b7Logic operations\nConsider an ALU having 4 arithmetic operations and 4 logical operation. To identify any one of these four logical\noperations or four arithmetic operations, two control lines are needed. Also to identify the any one of these two\ngroups- arithmetic or logical, another control line is needed. So, with the help  of three control lines, any one of\nthese  eight  operations  can  be  identified.  Consider  an  ALU  is  having  four  arithmetic  operations.  Addition,\nsubtraction, multiplication and division. Also  consider that the ALU  is having four logical operations: OR, AND,\nNOT & EX-OR. \nWe need three control lines to identify any one of these operations. The input combination of these control lines\nare shown below:\nControl line C2 is used to identify the group: logical or arithmetic, ie\nC2 = 0: arithmetic operation C2 = 1 logical operation\nControl lines C0 and C1 are used to identify any one of the four operations in a group. One possible combination\nis given here.\nA 3 x 8 decoder is used to decode the instruction. The block diagram of the ALU is shown in the\nfigure."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "A block diagram of ALU\nThe ALU has got two input registers named as A and B and one output storage register, named as C.\nIt performs the operation as:\nC = A op B\nThe input data are stored in A and B, and according to the operation specified in the control lines,\nthe ALU perform the operation and put the result in register C.\nAs for example, if the contents of controls lines are, 000, then the decoder enables the addition\noperation and it activates the adder circuit and the addition operation is performed on the data that\nare available in storage register A and B . After the completion of the operation, the result is stored\nin register C.\nWe should have some hardware implementations for basic operations. These basic operations can\nbe used to implement some complicated operations which are not feasible to implement directly in\nhardware.\nThere are several logic gates  exists  in digital logic circuit. These logic gates  can be used to\nimplement the logical operation. Some of the common logic gates are mentioned here. AND gate:\nThe output is high if both the inputs are high.\nOR gate: The output is high if any one of the input is high."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "EX-OR gate: The output is high if either of the input is high.\nIf we want to construct a circuit which will perform the AND operation on two 4-bit number, the\nimplementation of the 4-bit AND operation is shown in the figure.\n4-bit AND operator\nArithmetic Circuit\nBinary Adder : Binary adder is used to add two binary numbers.\nIn general, the adder circuit needs two binary inputs and two binary outputs. The input variables\ndesignate the augends and addend bits; The output variables produce the sum and carry. The binary\naddition operation of single bit is shown in the truth table"
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "The simplified sum of products expressions are\nS = x\u2019y + xy\u2019\nS = xy\nThe circuit implementation is\nThis circuit can not handle the carry input, so it is termed as half adder.\nFull Adder:\nA full adder is a combinational circuit that forms the arithmetic sum of three bits. It consists of three\ninputs and two outputs.\nTwo of the input variables, denoted by x and y, represent the two bits to be added. The third input Z,\nrepresents the carry from the previous lower position. The two outputs are designated by the\nsymbols S for sum and C for carry.\nS = x\u2019y\u2019z + x\u2019yz\u2019 + xy\u2019z\u2019 + xyz\nC = xy + xy\u2019z + x\u2019yz\nThe circuit diagram full adder is shown in the figure."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "n -such single bit full adder blocks are used to make n -bit full adder.\nTo demonstrate the binary addition of four bit numbers, let us consider a specific example.\nConsider two binary numbers\n                                                                         A =1 0 0 1 B = 0 0 1 1\nTo get the four bit adder, we have to use 4 full adder block. The carry output the lower bit is used as\na carry input to the next higher bit.\nThe circuit of 4-bit adder shown in the figure."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "Binary subtractor :  The subtraction operation can be implemented with the help of binary adder\ncircuit, because \nA \u2013 B = A + (-B)\nWe know that 2's complement representation of a number is treated as a negative number of the\ngiven number.\nWe can get the 2's complements of a given number by complementing each bit and adding 1 to it.\nThe circuit for subtracting A-B consist of an added with inverter placed between each data input B\nand the  corresponding input  of the full adder. The input  carry Co must  be equal  to 1 when\nperforming subtraction.\nThe operation thus performed becomes A , plus the 1's complement of B , plus 1. This is equal to A\nplus 2's complement of B .\nWith this principle, a single circuit can be used for both addition and subtraction. The 4 bit adder\nsubtractor circuit is shown in the figure. It has got one mode ( M ) selection input line, which will\ndetermine the operation,\nif M=0, then A + B\nif M = 1 then A \u2013 B = A + (-B)\n                                = A + 1\u2019s compliment of B + 1\n4-bit adder subtractor\nMultiplication\nMultiplication of two numbers in binary representation can be performed by a process of SHIFT\nand ADD operations. Since the binary number system allows only 0 and 1's, the digit multiplication\ncan be replaced by SHIFT and ADD operation only, because multiplying by 1 gives the number\nitself and multiplying by 0 produces 0 only.\nThe multiplication process is illustrated with a numerical example."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "The process consists of looking at successive bits of the multiplier, least significant bit first. If the\nmultiplier bit is a 1, the multiplicand is copied down, otherwise, zeros are copied down. The\nnumbers copied down in successive lines are shifted one position to the left from the previous\nnumber. Finally, the numbers are added and their sum forms the product.\nWhen multiplication is implemented in a digital computer, the process is changed slightly.\nInstead of providing registers to store and add simultaneously as many binary numbers as there are\nbits in the multiplier, it is convenient to provide an adder for the summation of only two binary\nnumbers  and  successively  accumulate  the  partial  products  in  a  register.  It  will  reduce  the\nrequirements of registers. Instead of sifting the multiplicand to the left, the partial product is shifted\nto right.\nWhen the corresponding bit of the multiplier is 0, there is no need to add all zeros to the partial\nproduct. An algorithm to multiply two binary numbers. Consider that the ALU does not provide the\nmultiplication operation, but it is having the addition operation and shifting operation. Then we can\nwrite a micro program for multiplication operation and provide the micro program code in memory.\nWhen a multiplication operation is encountered, it will execute this micro code to perform the\nmultiplication.\nThe micro code is nothing but the collection of some instructions. ALU must have those operation;\notherwise we must have micro code again for those operations which are not supported in ALU.\nConsider a situation such that we do not have the multiplication operation in a primitive computer.\nIs it possible to perform the multiplication. Of course, yes, provided the addition operation is\navailable.  We  can  perform  the  multiplication  with  the  help  of  repeated  addition  method;  for\nexample, if we want to multiply 4 by 5 ( 4 5), then simply add 4 five times to get the result.\nIf it is possible by addition operation, then why we need a multiplication operation.\nConsider a machine, which can handle 8 bit numbers, then we can represent number from 0 to 255.\nIf we want to multiply 175 225, then there will be at least 175 addition operation.\nBut if we use the multiplication algorithm that involves shifting and addition, it can be done in 8\nsteps, because we are using an 8-bit machine.\nAgain, the micro program execution is slightly slower, because we have to access the code from\nmicro controller memory, and memory is a slower device than CPU. It is possible to implement the\nmultiplication algorithm in hardware.\nBinary Multiplier, Hardware Implementation"
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "The block diagram of binary multiplier is shown in the figure.\nThe multiplicand is stored in register B and the multiplier is stored in register Q.\nThe partial product is formed in register A and stored in A and Q\nThe counter P is initially set to a number equal to the number of bits in the multiplier. The counter is\ndecremented by 1 after forming each partial product. When the content of the counter reaches zero,\nthe product is formed and the process stops.\nInitially, the multiplicand is in register B and the multiplier in Q. The register A is reset to 0.\nThe sum of A and B forms a partial product- which is transferred to the EA register.\nBoth partial product and multiplier are shifted to the right. The least significant bit of A is shifted\ninto the most significant position of Q; and 0 is shifted into E.\nAfter the shift, one bit of the partial product is shifted into Q, pushing the multiplier bits one\nposition to the right.\nThe right most flip flop in register Q, designated by Q 0 will hold the bit of the multiplier which\nmust be inspected next. If the content of this bit is 0, then it is not required to add the multiplicand,\nonly shifting is needed. If the content of this bit is 1, then both addition and shifting are needed.\nAfter each shifter, value of counter P is decremented and the process continues till the counter value\nbecomes 0. The final result is available in ( EAQ ) registers combination. To control the operation,\nit is required to design the appropriate control logic that is shown in the block diagram.\nThe flow chart of the multiplication operation is given in the figure."
  },
  {
    "source": "Lecture 2 - ALU.pdf",
    "text": "The working of multiplication algorithm is shown here with the help of an example."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 3 - Memory Organization\nby\nE. Kondela\nConcept of Memory\nWe have already mentioned that digital computer works on stored programmed concept introduced\nby V on Neumann. We use memory to store the information, which includes both program and data.\nDue to several reasons, we have different kind of memories. We use different kind of memory at\ndifferent lavel.\nThe memory of computer is broadly categories into two categories:\n\u2022Internal memory\n\u2022External memory-p\nInternal memory is used by CPU to perform task and external memory is used to store bulk\ninformation, which includes large software and data.\nThe memory unit is an essential component in any digital computer since it is needed for storing\nprograms and data. A very small computer with a limited application may be able to fulfill its\nintended task without the need of additional storage capacity. Most general-purpose computers\nwould run more efficiently if they were equipped with additional storage beyond the capacity of the\nmain memory. There is just not enough space in one memory unit to accommodate all the programs\nused in a typical computer. Moreover, most computer users accumulate and continue to accumulate\nlarge amounts  of data-processing software. Not all accumulated information  is  needed  by the\nprocessor at the same time. Therefore, it is more economical to use low-cost storage devices to\nserve as a backup for storing the information that is not currently used by the CPU.\nMemory is used to store the information in digital form. The memory hierarchy is given by:\n1.Register\n2.Cache Memory\n3.Main Memory\n4.Magnetic Disk\n5.Removable media (Magnetic tape)"
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "Register:\nThis is a part of Central Processor Unit, so they reside inside the CPU. The information from main\nmemory is brought to CPU and keep the information in register. Due to space and cost constraints,\nwe have got a limited number of registers in a CPU. These are basically faster devices.\nCache Memory:\nCache  memory  is  a  storage  device  placed  in  between  CPU  and  main  memory.  These  are\nsemiconductor memories. These are basically fast memory device, faster than main memory.\nWe can not have a big volume of cache memory due to its higher cost and some constraints of the\nCPU. Due to higher cost we can not replace the whole main memory by faster memory. Generally,\nthe most recently used information is kept in the cache memory. It is brought from the main\nmemory and placed in the cache memory. Now a days, we get CPU with internal cache.\nMain Memory:\nLike  cache  memory,  main  memory  is  also  semiconductor  memory.  But  the  main  memory  is\nrelatively slower memory. We have to first bring the information (whether it is data or program), to\nmain memory. CPU can work with the information available in main memory only.\nMagnetic Disk:\nThis is bulk storage device. We have to deal with huge amount of data in many application. But we\ndon't have so much semiconductor memory to keep these information in our computer. On the other\nhand, semiconductor memories are volatile in nature. It loses its content once we switch off the\ncomputer. For permanent storage, we use magnetic disk. The storage capacity of magnetic disk is\nvery high.\nRemovable media:\nFor different application, we use different data. It may not be possible to keep all the information in\nmagnetic disk. So, which ever data we are not using currently, can be kept in removable media.\nMagnetic tape is one kind of removable medium. CD is also a removable media, which is an optical\ndevice.\nRegister, cache memory and main memory are internal memory. Magnetic Disk, removable media\nare external memory. Internal memories are semiconductor memory. Semiconductor memories are\ncategoried as volatile memory and non-volatile memory.\nRAM: Random Access Memories are volatile in nature. As soon as the computer is switched off, the\ncontents of memory are also lost.\nROM: Read only memories are non volatile in nature. The storage is permanent, but it is read only\nmemory. We can not store new information in ROM.\nSeveral types of ROM are available:\n\u2022PROM:  Programmable  Read  Only  Memory;  it  can  be  programmed  once  as  per  user\nrequirements.\n\u2022EPROM: Erasable Programmable Read Only Memory; the contents of the memory can be\nerased and store new data into the memory. In this case, we have to erase whole information.\n\u2022EEPROM: Electrically Erasable Programmable Read Only Memory; in this type of memory\nthe contents of a particular location can be changed without effecting the contents of other\nlocation.\n2) Main Memory\nThe main memory of a computer is semiconductor memory. The main memory unit of computer is\nbasically consists of two kinds of memory:\n\u2022RAM : Random access memory; which is volatile in nature.\n\u2022ROM: Read only memory; which is non-volatile.\nThe permanent information are kept in ROM and the user space is basically in RAM.\nThe smallest unit of information is known as bit (binary digit), and in one memory cell we can store\none bit of information. 8 bit together is termed as a byte.\nThe maximum size of main memory that can be used in any computer is determined by the\naddressing scheme.\nA computer that generates 16-bit address is capable of addressing upto 2^16 which is equal to 64K\nmemory location. Similarly, for 32 bit addresses, the total capacity will be 2^32 which is equal to\n4G memory location.\nIn some computer, the smallest addressable unit of information is a memory word and the machine\nis called word-addressable.\nIn some computer, individual address is assigned for each byte of information, and it is called byte-\naddressable computer . In this computer, one memory word contains one or more memory bytes\nwhich can be addressed individually.\nA byte addressable 32-bit computer, each memory word contains 4 bytes. A possible way of address\nassignment is shown in figure. The address of a word is always integer multiple of 4.\nThe main memory is usually designed to store and retrieve data in word length quantities. The word\nlength of a computer is generally defined by the number of bits actually stored or retrieved in one\nmain memory access.\nConsider a machine with 32 bit address bus. If the word size is 32 bit, then the high order 30 bit will\nspecify the address of a word. If we want to access any byte of the word, then it can be specified by\nthe lower two bit of the address bus."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "The data transfer between main memory and the CPU takes place through two CPU registers.\n\u2022MAR : Memory Address Register\n\u2022MDR : Memory Data Register.\nIf the MAR is k-bit long, then the total addressable memory location will be 2^k .\nIf the MDR is n-bit long, then the n bit of data is transferred in one memory cycle.\nThe transfer of data takes place through memory bus, which consist of address bus and data bus. In\nthe above example, size of data bus is n-bit and size of address bus is k bit.\nIt  also  includes  control  lines  like  Read,  Write  and  Memory  Function  Complete  (MFC)  for\ncoordinating data transfer. In the case of byte addressable computer, another control line to be added\nto indicate the byte transfer instead of the whole word.\nFor memory operation, the CPU initiates a memory operation by loading the appropriate data i.e.,\naddress to MAR.\nIf it is a memory read operation, then it sets the read memory control line to 1. Then the contents of\nthe memory location is brought to MDR and the memory control circuitry indicates this to the CPU\nby setting MFC to 1. If the operation is a memory write operation, then the CPU places the data into\nMDR and sets the write memory control line to 1. Once the contents of MDR are stored in specified\nmemory location, then the memory control circuitry indicates the end of operation by setting MFC\nto 1.\nA useful measure of the speed of memory unit is the time that elapses between the initiation of an\noperation and the completion of the operation (for example, the time between Read and MFC). This\nis referred to as Memory Access Time . Another measure is memory cycle time. This is the\nminimum time delay between the initiation two independent memory operations (for example, two\nsuccessive memory read operation). Memory cycle time is slightly larger than memory access time.\nBinary Storage Cell:\nThe binary storage cell is the basic building block of a memory unit.\nThe binary storage cell that stores one bit of information can be modelled by an SR latch with\nassociated gates. This model of binary storage cell is shown in the figure."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "1 bit Binary Cell (BC)\nThe binary cell sotres one bit of information in its internal latch.\nControl input to binary cell\nThe storage part is modelled here with SR-latch, but in reality it is an electronics circuit made up of\ntransistors.\nThe memory constructed with the help of transistors is known as semiconductor memory. The\nsemiconductor memories are termed as Random Access Memory(RAM), because it is possible to\naccess any memory location in random.\nDepending on the technology used to construct a RAM, there are two types of RAM -\n\u2022SRAM: Static Random Access Memory.\n\u2022DRAM: Dynamic Random Access Memory.\nDynamic Ram (DRAM):\nA DRAM is made with cells that store data as charge on capacitors. The presence or absence of\ncharge in a capacitor is interpreted as binary 1 or 0.\nBecause capacitors have a natural tendency to discharge due to leakage current, dynamic RAM\nrequire  periodic  charge  refreshing  to  maintain  data  storage.  The  term  dynamic  refers  to  this\ntendency of the stored charge to leak away, even with power continuously applied.\nA typical DRAM structure for an individual cell that stores one bit information is shown in the\nfigure."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "For the write operation, a voltage signal is applied to the bit line B, a high voltage represents 1 and a\nlow voltage represents 0. A signal is then applied to the address line, which will turn on the\ntransistor T, allowing a charge to be transferred to the capacitor.\nFor the read operation, when a signal is applied to the address line, the transistor T turns on and the\ncharge stored on the capacitor is fed out onto the bit line B.\nStatic RAM (SRAM):\nIn an SRAM, binary values are stored using traditional flip-flop constructed with the help of\ntransistors. A static RAM will hold its data as long as power is supplied to it.\nA typical SRAM constructed with transistors is shown in the figure.\nFour transistors (T1 , T2 , T3 , T4 ) are cross connected in an arrangement that produces a stable\nlogic state. \nIn logic state 1, point A 1 is high and point A 2 is low; in this state T 1 and T 4 are off, and T2 and\nT3 are on .\nIn logic state 0, point A 1 is low and point A 2 is high; in this state T 1 and T 4 are on, and T 2 and T\n3 are off .\nBoth states are stable as long as the dc supply voltage is applied.\nThe address line is used to open or close a switch which is nothing but another transistor. The\naddress line controls two transistors(T 5 and T 6 ).\nWhen a signal is applied to this line, the two transistors are switched on, allowing a read or write\noperation. For a write operation, the desired bit value is applied to line B, and its complement is\napplied to line B\u2019. This forces the four transistors(T1 , T2 , T3 , T4 ) into the proper state.\nFor a read operation, the bit value is read from the line B. When a signal is applied to the address\nline, the signal of point A1 is available in the bit line B.\nSRAM Versus DRAM :\n\u2022Both static and dynamic RAMs are volatile, that is, it will retain the information as long as\npower supply is applied.\n\u2022A dynamic memory cell is simpler and smaller than a static memory cell. Thus a DRAM is\nmore dense, i.e., packing density is high(more cell per unit area). DRAM is less expensive\nthan corresponding SRAM."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "\u2022DRAM requires the supporting refresh circuitry. For larger memories, the fixed cost of the\nrefresh circuitry is more than compensated for by the less cost of DRAM cells\n\u2022SRAM  cells  are  generally  faster  than  the  DRAM  cells.  Therefore,  to  construct  faster\nmemory modules(like cache memory) SRAM is used.\nInternal Organization of Memory Chips\nA memory cell is capable of storing 1-bit of information. A number of memory cells are organized\nin the form of a matrix to form the memory chip. One such organization is shown in the Figure\n16 x 8 memory organization\nEach row of cells constitutes a memory word, and all cell of a row are connected to a common line\nwhich is referred as word line. An address decoder is used to drive the word line. At a particular\ninstant, one word line is enabled depending on the address present in the address bus. The cells in\neach column are connected by two lines. These are known as bit lines. These bit lines are connected\nto data input line and data output line through a Sense/ Write circuit. During a Read operation, the\nSense/Write circuit sense, or read the information stored in the cells selected by a word line and\ntransmit this information to the output data line. During a write operation, the sense/write circuit\nreceive information and store it in the cells of the selected word.\nA memory chip consisting of 16 words of 8 bits each, usually referred to as 16 x 8 organization. The\ndata input and data output line of each Sense/Write circuit are connected to a single bidirectional\ndata line in order to reduce the pin required. For 16 words, we need an address bus of size 4. In\naddition to address and data lines, two control lines, RW\u2019 and CS, are provided. The R/W\u2019 line is to\nused to specify the required operation about read or write. The CS (Chip Select) line is required to\nselect a given chip in a multi chip memory system.\n128 x 8 memory chips:"
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "If it is organised as a 128 x 8 memory chips, then it has got 128 memory words of size 8 bits. So the\nsize of data bus is 8 bits and the size of address bus is 7 bits (2^7 = 128).\n1024 x 1 memory chips:\nIf it is organized as a 1024 x 1 memory chips, then it has got 1024 memory words of size 1 bit only.\nTherefore, the size of data bus is 1 bit and the size of address bus is 10 bits (2^10 = 1024).\nA particular memory location is identified by the contents of memory address bus. A decoder is\nused  to  decode  the  memory  address. There  are  two  ways  of decoding  of a  memory  address\ndepending upon the organization of the memory module.\nIn one case, each memory word is organized in a row. In this case whole memory address bus is\nused together to decode the address of the specified location."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "In second case, several memory words are organized in one row. In this case, address bus\nis divided into two gropus.\nOne group is used to form the row address and the second group is used to form the column\naddress. Consider the memory organization of 1024 x 1 memory chip. The required 10-bit address\nis divided into two groups of 5 bits each to form the row and column address of the cell array. A row\naddress selects a row of 32 cells, all of which are accessed in parallel. However, according to the\ncolumn address, only one of these cells is connected to the external data line via the input output\nmultiplexers.\nThe commercially available memory chips contain a much larger number of cells. As for example, a\nmemory unit of 1MB (mega byte) size, organised as 1M x 8, contains 2^20 x 8 memory cells. It has\ngot memory location and each memory location contains 8 bits information. The size of address bus\nis 20 and the size of data bus is 8.\nThe number of pins of a memory chip depends on the data bus and address bus of the memory\nmodule. To reduce the number of pins required for the chip, we use another scheme for address\ndecoding. The cells are organized in the form of a square array. The address bus is divided into\ntwo groups, one for column address and other one is for row address. In this case, high- and low-\norder 10 bits of 20-bit address constitute of row and column address of a given cell, respectively. In\norder to reduce the number of pin needed for external connections, the row and column\naddresses are multiplexed on ten pins. During a Read or a Write operation, the row address is\napplied first. In response to a signal pulse on the Row Address Strobe (RAS)  input of the chip, this\npart of the address is loaded into the row address latch.\nAll cell of this particular row is selected. Shortly after the row address is latched, the column\naddress is applied to the address pins. It is loaded into the column address latch with the help of\nColumn Address Strobe (CAS) signal, similar to RAS. The information in this latch is decoded and\nthe appropriate Sense/Write circuit is selected."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "For a Write operation, the information at the input lines are transferred to the selected circuits.\nNow we discuss the design of memory subsystem using memory chips. Consider a memory chips of\ncapacity 16K x 8. The requirement is to design a memory subsystem of capacity 64K x 16. Each\nmemory chip has got eight lines for data bus, but the data bus size of memory subsytem is 16 bits."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "The total requiremet is for 64K memory location, so four such units are required to get the 64K\nmemory location. For 64K memory location, the size of address bus is 16. On the other hand, for\n16K memory location, size of address bus is 14 bits.\nEach chip has a control input line called Chip Select (CS). A chip can be enabled to accept data\ninput or to place the data on the output bus by setting its Chip Select input to 1. The address bus for\nthe 64K memory is 16 bits wide. The high order two bits of the address are decoded to obtain the\nfour chip select control signals. The remaining 14 address bits are connected to the address lines of\nall the chips. They are used to access a specific location inside each chip of the selected row. The\nR/W\u2019 inputs of all chips are tied together to provide a common READ/WRITE control.\n3) Cache Memory\nAnalysis of large number of programs has  shown that a number of instructions  are executed\nrepeatedly. This may be in the form of a simple loops, nested loops, or a few procedures that\nrepeatedly call each other. It is observed that many instructions in each of a few localized areas of\nthe program are repeatedly executed, while the remainder of the program is accessed relatively less.\nThis phenomenon is referred to as locality of reference."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "Now, if it can be arranged to have the active segments of a program in a fast memory, then the tolal\nexecution time can be significantly reduced. It is the fact that CPU is a faster device and memory is\na relatively slower device.\nMemory access is the main bottleneck for the performance efficiency. If a faster memory device can\nbe inserted between main memory and CPU, the efficiency can be increased. The faster memory\nthat  is  inserted  between  CPU  and Main  Memory  is  termed  as  Cache  memory.  To  make  this\narrangement effective, the cache must be considerably faster than the main memory, and typically it\nis 5 to 10 time faster than the main memory. This approach is more economical than the use of fast\nmemory device to implement the entire main memory. This is also a feasible due to the locality of\nreference that is present in most of the program, which reduces the frequent data transfer between\nmain memory and cache memory.\nOperation of Cache Memory\nThe memory control circuitry is designed to take advantage of the property of locality of reference.\nSome assumptions are made while designing the memory control circuitry:\n1.The CPU does not need to know explicitly about the existence of the cache.\n2.The CPU simply makes Read and Write request. The nature of these two operations are\nsame whether cache is present or not.\n3.The address generated by the CPU always refer to location of main memory.\n4.The memory access control circuitry determines whether or not the requested word currently\nexists in the cache.\nWhen  a  Read  request  is  received  from  the  CPU,  the  contents  of  a  block  of  memory  words\ncontaining the location specified are transferred into the cache. When any of the locations in this\nblock is referenced by the program, its contents are read directly from the cache.\nThe cache memory can store a number of such blocks at any given time.\nThe correspondence between the Main Memory Blocks and those in the cache is specified by means\nof a mapping function.\nWhen the cache is full and a memory word is referenced that is not in the cache, a decision must be\nmade as to which block should be removed from the cache to create space to bring the new block to\nthe cache that contains the referenced word. Replacement algorithms are used to make the proper\nselection of block that must be replaced by the new one.\nWhen a write request is received from the CPU, there are two ways that the system can proceed. In\nthe first case, the cache location and the main memory location are updated simultaneously. This is\ncalled the store through method or write through method .\nThe alternative is to update the cache location only. During replacement time, the cache block will\nbe written back to the main memory. If there is no new write operation in the cache block, it is not\nrequired to write back the cache block in the main memory. This information can be kept with the\nhelp of an associated bit. This bit it set while there is a write operation in the cache block. During\nreplacement, it checks this bit, if it is set, then write back the cache block in main memory\notherwise not. This bit is known as dirty bit . If the bit gets dirty (set to one), writing to main\nmemory is required.\nThis write through method is simpler, but it results in unnecessary write operations in the main\nmemory when a given cache word is updated a number of times during its cache residency period.\nConsider the case where the addressed word is not in the cache and the operation is a read. First the\nblock of the words is brought to the cache and then the requested word is forwarded to the CPU.\nBut it can be forwarded to the CPU as soon as it is available to the cache, instead of whole block to\nbe loaded into the cache. This is called load through, and there is some scope to save time while\nusing load through policy.\nDuring a write operation, if the address word is not in the cache, the information is written directly\ninto the main memory. A write operation normally refers to the location of data areas and the\nproperty of locality of reference is not as pronounced in accessing data when write operation is\ninvolved. Therefore, it is not advantageous to bring the data block to the cache when there a write\noperation, and the addressed word is not present in cache.\nMapping Functions\nThe mapping functions are used to map a particular block of main memory to a particular block of\ncache. This mapping function is used to transfer the block from main memory to cache memory.\nThree different mapping functions are available:\nDirect mapping:\nA particular block of main memory can be brought to a particular block of cache memory. So, it is\nnot flexible.\nAssociative mapping:\nIn this mapping function, any block of Main memory can potentially reside in any cache block\nposition. This is much more flexible mapping method.\nBlock-set-associative mapping:\nIn this method, blocks of cache are grouped into sets, and the mapping allows a block of main\nmemory to reside in any block of a specific set. From the flexibility point of view, it is in between to\nthe other two methods.\nConsider a cache of 4096 (4K) words with a block size of 32 words. Therefore, the cache is\norganized as 128 blocks. For 4K words, required address lines are 12 bits. To select one of the block\nout of 128 blocks, we need 7 bits of address lines and to select one word out of 32 words, we need 5\nbits of address lines. So the total 12 bits of address is divided for two groups, lower 5 bits are used\nto select a word within a block, and higher 7 bits of address are used to select any block of cache\nmemory.\nLet us consider a main memory system consisting 64K words. The size of address bus is 16 bits.\nSince the block size of cache is 32 words, so the main memory is also organized as block size of 32\nwords. Therefore, the total number of blocks in main memory is 2048 (2K x 32 words = 64K\nwords). To identify any one block of 2K blocks, we need 11 address lines. Out of 16 address lines of\nmain memory, lower 5 bits are used to select a word within a block and higher 11 bits are used to\nselect a block out of 2048 blocks.\nNumber of blocks in cache memory is 128 and number of blocks in main memory is 2048, so at any\ninstant of time only 128 blocks out of 2048 blocks can reside in cache menory. Therefore, we need\nmapping  function  to put  a  particular  block  of  main  memory  into  appropriate  block  of cache\nmemory.\nDirect Mapping Technique:\nThe simplest way of associating main memory blocks with cache block is the direct mapping\ntechnique. In this technique, block k of main memory maps into block k modulo m of the cache,\nwhere m is the total number of blocks in cache. In this example, the value of m is 128. In direct\nmapping technique, one particular block of main memory can be transfered to a particular block of\ncache which is derived by the modulo function.\nSince more than one main memory block is mapped onto a given cache block position, contention\nmay arise for that position. This situation may occurs even when the cache is not full. Contention is\nresolved by allowing the new block to overwrite the currently resident block. So the replacement\nalgorithm is trivial. The detail operation of direct mapping technique is as follows:\nThe main memory address is divided into three fields. The field size depends on the memory\ncapacity and the block size of cache. In this example, the lower 5 bits of address is used to identify a\nword within a block. Next 7 bits are used to select a block out of 128 blocks (which is the capacity\nof the cache). The remaining 4 bits are used as a TAG to identify the proper block of main memory\nthat is mapped to cache.\nWhen a new block is first brought into the cache, the high order 4 bits of the main memory address\nare stored in four TAG bits associated with its location in the cache. When the CPU generates a\nmemory request, the 7-bit block address determines the corresponding cache block. The TAG field\nof that block is compared to the TAG field of the address. If they match, the desired word specified\nby the low-order 5 bits of the address is in that block of the cache.\nIf there is no match, the required word must be accessed from the main memory, that is, the\ncontents of that block of the cache is replaced by the new block that is specified by the new address\ngenerated by the CPU and correspondingly the TAG bit will also be changed by the high order 4\nbits of the address. The whole arrangement for direct mapping technique is shown in the figure.\nAssociated Mapping Technique:\nIn the associative mapping technique, a main memory block can potentially reside in any cache\nblock position. In this case, the main memory address is divided into two groups, low-order bits\nidentifies the location of a word within a block and high-order bits identifies the block. In the\nexample here, 11 bits are required to identify a main memory block when it is resident in the cache ,"
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "high-order 11 bits are used as TAG bits and low-order 5 bits are used to identify a word within a\nblock. The TAG bits of an address received from the CPU must be compared to the TAG bits of\neach block of the cache to see if the desired block is present.\nIn the associative mapping, any block of main memory can go to any block of cache, so it has got\nthe complete flexibility and we have to use proper replacement policy to replace a block from cache\nif the currently accessed block of main memory is not present in cache. It might not be practical to\nuse this complete flexibility of associative mapping technique due to searching overhead, because\nthe TAG field of main memory address has to be compared with the TAG field of all the cache\nblock. In this example, there are 128 blocks in cache and the size of TAG is 11 bits. The whole\narrangement of Associative Mapping Technique is shown in the figure\nBlock-Set-Associative Mapping Technique:\nThis mapping technique is intermediate to the above two techniques. Blocks of the cache are\ngrouped into sets, and the mapping allows a block of main memory to reside in any block of a\nspecific set. Therefore, the flexibity of associative mapping is reduced from full freedom to a set of\nspecific blocks. This also reduces the searching overhead, because the search is restricted to number\nof sets, instead of number of blocks. Also the contention problem of the direct mapping is eased by\nhaving a few choices for block replacement.\nConsider  the  same  cache  memory  and  main  memory  organization  of  the  previous  example.\nOrganize the cache with 4 blocks in each set. The TAG field of associative mapping technique is\ndivided into two groups, one is termed as SET bit and the second one is termed as TAG bit. Since\neach set contains 4 blocks, total number of set is 32. The main memory address is grouped into"
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "three parts: low-order 5 bits are used to identifies a word within a block. Since there are total 32 sets\npresent, next 5 bits are used to identify the set. High-order 6 bits are used as TAG bits.\nThe 5-bit set field of the address determines which set of the cache might contain the desired block.\nThis is similar to direct mapping technique, in case of direct mapping, it looks for block, but in case\nof block-set-associative mapping, it looks for set. The TAG field of the address must then be\ncompared with the TAGs of the four blocks of that set. If a match occurs, then the block is present\nin the cache; otherwise the block containing the addressed word must be brought to the cache. This\nblock will potentially come to the corresponding set only. Since, there are four blocks in the set, we\nhave to choose appropriately which block to be replaced if all the blocks are occupied. Since the\nsearch  is  restricted  to  four  block  only,  so  the  searching  complexity  is  reduced.  The  whole\narrangement of block-set-associative mapping technique is shown in the figure.\nIt is clear that if we increase the number of blocks per set, then the number of bits in SET field is\nreduced. Due to the increase of blocks per set, complexity of search is also increased. The extreme\ncondition of 128 blocks per set requires no set bits and corresponds to the fully associative mapping\ntechnique with 11 TAG bits. The other extreme of one block per set is the direct mapping method.\nReplacement Algorithms\nWhen a new block must be brought into the cache and all the positions that it may occupy are full, a\ndecision must be made as to which of the old blocks is to be overwritten. In general, a policy is\nrequired to keep the block in cache when they are likely to be referenced in near future. However, it\nis not easy to determine directly which of the block in the cache are about to be referenced. The\nproperty of locality of reference gives some clue to design good replacement policy.\nLeast Recently Used (LRU) Replacement policy:\nSince program usually stay in localized areas for reasonable periods of time, it can be assumed that\nthere is a high probability that blocks which have been referenced recently will also be referenced in\nthe near future. Therefore, when a block is to be overwritten, it is a good decision to overwrite the\none that has gone for longest time without being referenced. This is defined as the least recently\nused (LRU) block. Keeping track of LRU block must be done as computation proceeds.\nConsider a specific example of a four-block set. It is required to track the LRU block of this four-\nblock set. A 2-bit counter may be used for each block.\nWhen a hit occurs, that is, when a read request is received for a word that is in the cache, the\ncounter of the block that is referenced is set to 0. All counters which values originally lower than\nthe referenced one are incremented by 1 and all other counters remain unchanged.\nWhen a miss occurs, that is, when a read request is received for a word and the word is not present\nin the cache, we have to bring the block to cache.\nThere are two possibilities in case of a miss:\nIf the set is not full, the counter associated with the new block loaded from the main memory is set\nto 0, and the values of all other counters are incremented by 1.\nIf the set is full and a miss occurs, the block with the counter value 3 is removed , and the new\nblock is put in its palce. The counter value is set to zero. The other three block counters are\nincremented by 1."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "It is easy to verify that the counter values of occupied blocks are always distinct. Also it is trivial\nthat highest counter value indicates least recently used block.\nFirst In First Out (FIFO) replacement policy:\nA reasonable rule may be to remove the oldest from a full set when a new block must be brought in.\nWhile using this technique, no updation is required when a hit occurs. When a miss occurs and the\nset is not full, the new block is put into an empty block and the counter values of the occupied block\nwill be increment by one. When a miss occurs and the set is full, the block with highest counter\nvalue is replaced by new block and counter is set to 0, counter value of all other blocks of that set is\nincremented by 1. The overhead of the policy is less, since no updation is required during hit.\nRandom replacement policy:\nThe simplest algorithm is to choose the block to be overwritten at random. Interestingly enough,\nthis simple algorithm has been found to be very effective in practice.\n4) Virtual Memory (Paging)\nBoth unequal fixed size and variable size partitions are inefficient in the use of memory. It has been\nobserved that both schemes lead to memory wastage. Therefore we are not using the memory\nefficiently. There is another scheme for use of memory which is known as paging. In this scheme,\nThe memory is partitioned into equal fixed size chunks that are relatively small. This chunk of\nmemory is known as frames or page frames. Each process is also divided into small fixed chunks of\nsame size. The chunks of a program is known as pages.\nA page of a program could be assigned to available page frame. In this scheme, the wastage space in\nmemory for a process is a fraction of a page frame which corresponds to the last page of the\nprogram. At a given point of time some of the frames in memory are in use and some are free. The\nlist of free frame is maintained by the operating system.\nProcess A , stored in disk , consists of pages . At the time of execution of the process A, the\noperating system finds six free frames and loads the six pages of the process A into six frames.\nThese six frames need not be contiguous frames in main memory. The operating system maintains a\npage table for each process. Within the program, each logical address consists of page number and a\nrelative address within the page.\nIn case of simple partitioning, a logical address is the location of a word relative to the beginning of\nthe program; the processor translates that into a physical address. With paging, a logical address is a\nlocation of the word relative to the beginning of the page of the program, because the whole\nprogram is divided into several pages of equal length and the length of a page is same with the\nlength of a page frame.\nA logical address consists of page number and relative address within the page, the process uses the\npage table to produce the physical address which consists of frame number and relative address\nwithin the frame.\nThe figure on next page shows the allocation of frames to a new process in the main memory. A\npage table is maintained for each process. This page table helps us to find the physical address in a\nframe which corresponds to a logical address within a process.\nThe conversion of logical address to physical address is shown in the figure for the Process A.\nThis approach solves the problems. Main memory is divided into many small equal size frames.\nEach process is divided into frame size pages. Smaller process requires fewer pages, larger process\nrequires more. When a process is brought in, its pages are loaded into available frames and a page\ntable is set up."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "Virtual Memory\nThe concept of paging helps us to develop truly effective multi-programming systems. Since a\nprocess need not be loaded into contiguous memory locations, it helps us to put a page of a process\nin any free page frame. On the other hand, it is not required to load the whole process to the main\nmemory,  because  the  execution  may  be  confined  to  a  small  section  of  the  program.  (eg.  a\nsubroutine).\nIt would clearly be wasteful to load in many pages for a process when only a few pages will be used\nbefore the program is suspended. Instead of loading all the pages of a process, each page of process\nis brought in only when it is needed, i.e on demand. This scheme is known as demand paging .\nDemand paging also allows us to accommodate more process in the main memory, since we are not\ngoing to load the whole process in the main memory, pages will be brought into the main memory\nas and when it is required.\nWith demand paging, it is not necessary to load an entire process into main memory.\nThis concept leads us to an important consequence \u2013 It is possible for a process to be larger than the\nsize of main memory. So, while developing a new process, it is not required to look for the main\nmemory available in the machine. Because, the process will be divided into pages and pages will be\nbrought to memory on demand.\nBecause a process executes only in main memory, so the main memory is referred to as real\nmemory or physical memory. A programmer or user perceives a much larger memory that is\nallocated on the disk. This memory is referred to as virtual memory. The program enjoys a huge\nvirtual memory space to develop his or her program or software.\nThe execution of a program is the job of operating system and the underlying hardware. To improve\nthe performance some special hardware is added to the system. This hardware unit is known as\nMemory Management Unit (MMU).\nIn paging system, we make a page table for the process. Page table helps us to find the physical\naddress from virtual address. The virtual address space is used to develop a process. The special\nhardware unit , called Memory Management Unit (MMU) translates virtual address to physical\naddress. When the desired data is in the main memory, the CPU can work with these data. If the\ndata are not in the main memory, the MMU causes the operating system to bring into the memory\nfrom the disk."
  },
  {
    "source": "Lecture 3 - Memory.pdf",
    "text": "Address Translation\nThe basic mechanism for reading a word from memory involves the translation of a virtual or\nlogical address, consisting of page number and offset, into a physical address, consisting of frame\nnumber and offset, using a page table.\nThere is one page table for each process. But each process can occupy huge amount of virtual\nmemory. But the virtual memory of a process cannot go beyond a certain limit which is restricted by\nthe underlying hardware of the MMU. One of such component may be the size of the virtual\naddress register.\nThe sizes of pages are relatively small and so the size of page table increases as the size of process\nincreases. Therefore, size of page table could be unacceptably high. To overcome this problem,\nmost virtual memory scheme store page table in virtual memory rather than in real memory. This\nmeans that the page table is subject to paging just as other pages are. When a process is running, at\nleast a part of its page table must be in main memory, including the page table entry of the currently\nexecuting page.\nEach virtual address generated by the processor is interpreted as virtual page number (high order\nlist) followed by an offset (lower order bits) that specifies the location of a particular word within a\npage. Information about the main memory location of each page kept in a page table. Some\nprocessors make use of a two level scheme to organize large page tables.\nIn this scheme, there is a page directory, in which each entry points to a page table. Thus, if the\nlength of the page directory is X, and if the maximum length of a page table is Y , then the process\ncan consist of up to X x Y pages. Typically, the maximum length of page table is restricted to the\nsize of one page frame."
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 4 - Instruction Set Architecture\nby \nE. Kondela\nVarious Addressing Modes\nWe  have  examined  the  types  of  operands  and  operations  that  may  be  specified  by  machine\ninstructions . Now we have to see how is the address of an operand specified, and how are the bits\nof an instruction organized to define the operand addresses and operation of that instruction.\nAddressing Modes:\nThe most common addressing techniques are:\n\uf0b7Immediate\n\uf0b7Direct\n\uf0b7Indirect\n\uf0b7Register\n\uf0b7Register Indirect\n\uf0b7Displacement\n\uf0b7Stack\nAll computer architectures provide more than one of these addressing modes . The question arises\nas to how the control unit can determine which addressing mode is being used in a particular\ninstruction. Several approaches are used. Often, different opcodes will use different addressing\nmodes. Also, one or more bits in the instruction format can be used as a mode field . The value of\nthe mode field determines which addressing mode is to be used.\nWhat is the interpretation of effective address . In a system without virtual memory, the effective\naddress will be either a main memory address or a register. In a virtual memory system, the\neffective address is a virtual address or a register. The actual mapping to a physical address is a\nfunction of the paging mechanism and is invisible to the programmer.\nTo explain the addressing modes, we use the following notation:\nA = contents of an address field in the instruction that refers to a memory\nR = contents of an address field in the instruction that refers to a register\nEA = actual (effective) address of the location containing the referenced operand\n(X) = contents of location X\nImmediate Addressing:\nThe simplest form of addressing is immediate addressing, in which the operand is actually present\nin the instruction:\nOPERAND = A\nThis mode can be used to define and use constants or set initial values of variables. The advantage\nof immediate addressing is that no memory reference other than the instruction fetch is required to\nobtain the operand. The disadvantage is that the size of the number is restricted to the size of the\naddress field, which, in most instruction sets, is small compared with the world length."
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "Direct Addressing:\nA very simple form of addressing is direct addressing, in which the address field contains the\neffective address of the operand:\nEA = A\nIt requires only one memory reference and no special calculation.\nIndirect Addressing:\nWith direct addressing, the length of the address field is usually less than the word length, thus\nlimiting the address range. One solution is to have the address field refer to the address of a word in\nmemory, which in turn contains a full-length address of the operand. This is know as indirect\naddressing:\nEA = (A)"
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "Register Addressing:\nRegister addressing is similar to direct addressing. The only difference is that the address field\nrefers to a register rather than a main memory address:\nEA = R\nThe advantages of register addressing are that only a small address field is needed in the instruction\nand no memory reference is required. The disadvantage of register addressing is that the address\nspace is very limited.\nRegister Indirect Addressing:\nRegister indirect addressing is similar to indirect addressing, except that the address field refers to a\nregister instead of a memory location.\nIt requires only one memory reference and no special calculation.\nEA = (R)\nRegister indirect addressing uses one less memory reference than indirect addressing. Because, the\nfirst information is available in a register which is nothing but a memory address. From that\nmemory location, we use to get the data or information. In general, register access is much more\nfaster than the memory access."
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "Displacement Addressing:\nA very powerful mode of addressing combines the capabilities of direct addressing and register\nindirect addressing, which is broadly categorized as displacement addressing:\nEA = A + (R)\nDisplacement addressing requires that the instruction have two address fields, at least one of which\nis explicit. The value contained in one address field (value = A) is used directly. The other address\nfield, or an implicit reference based on opcode, refers to a register whose contents are added to A to\nproduce the effective address.\nThree of the most common use of displacement addressing are:\n\uf0b7Relative addressing\n\uf0b7Base-register addressing\n\uf0b7Indexing\nRelative Addressing:\nFor relative addressing, the implicitly referenced register is the program counter (PC). That is, the\ncurrent instruction address is added to the address field to produce the EA. Thus, the effective\naddress is a displacement relative to the address of the instruction.\nBase-Register Addressing:\nThe reference register contains a memory address, and the address field contains a displacement\nfrom that address. The register reference may be explicit or implicit .\nIn some implimentation, a single segment/base register is employed and is used implicitly. In\nothers, the programmer may choose a register to hold the base address of a segment, and the\ninstruction must reference it explicitly."
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "Indexing:\nThe address field references a main memory address, and the reference register contains a positive\ndisplacement from that address. In this case also the register reference is sometimes explicit and\nsometimes implicit.\nGenerally index register are used for iterative tasks, it is typical that there is a need to increment or\ndecrement the index register after each reference to it. Because this is such a common operation,\nsome system will automatically do this as part of the same instruction cycle.\nThis is known as auto-indexing. We may get two types of auto-indexing:\n- - one is auto-incrementing and the other one is\n- - auto-decrementing.\nIf  certain  registers  are  devoted  exclusively  to  indexing,  then  auto-indexing  can  be  invoked\nimplicitly and automatically. If general purpose register are used, the autoindex operation may need\nto be signaled by a bit in the instruction.\nAuto-indexing using increment can be depicted as follows:\nEA = A + (R)\nR = (R) + 1\nAuto-indexing using decrement can be depicted as follows:\nEA = A + (R)\nR = (R) - 1\nIn some machines, both indirect addressing and indexing are provided, and it is possible to employ\nboth in the same instruction. There are two possibilities: The indexing is performed either before or\nafter the indirection.\nIf indexing is performed after the indirection, it is termed postindexing\nEA = (A) + (R)\nFirst, the contents of the address field are used to access a memory location containing an address.\nThis address is then indexed by the register value.\nWith preindexing, the indexing is performed before the indirection:\nEA = ( A + (R) )\nAn address is calculated, the calculated address contains not the operand, but the address of the\noperand.\nStack Addressing:\nA stack is a linear array or list of locations. It is sometimes referred to as a pushdown list or last-in-\nfirst-out queue . A stack is a reserved block of locations. Items are appended to the top of the stack\nso that, at any given time, the block is partially filled. Associated with the stack is a pointer whose\nvalue is the address of the top of the stack. The stack pointer is maintained in a register. Thus,\nreferences to stack locations in memory are in fact register indirect addresses. The stack mode of\naddressing is a form of implied addressing. The machine instructions need not include a memory\nreference but implicitly operate on the top of the stack.\nMachine Instruction\nThe  operation  of  a  CPU  is  determine  by  the  instruction  it  executes,  referred  to  as  machine\ninstructions or computer instructions. The collection of different instructions is referred as the\ninstruction set of the CPU.\nEach instruction must contain the information required by the CPU for execution. The elements of\nan instruction are as follows:\n\uf0b7Operation  Code:  Specifies  the  operation  to  be  performed  (e.g.,  add,  move  etc.).  The\noperation is specified by a binary code, know as the operation code or opcode.\n\uf0b7Source operand reference: The operation may involve one or more source operands; that is,\noperands that are inputs for the operation.\n\uf0b7Result operand reference: The operation may produce a result.\n\uf0b7Next instruction reference: This tells the CPU where to fetch the next instruction after the\nexecution of this instruction is complete.\nThe next instruction to be fetched is located in main memory. But in case of virtual memory system,\nit may be either in main memory or secondary memory (disk). In most cases, the next instruction to\nbe fetched immediately follow the current instruction. In those cases, there is no explicit reference\nto the next instruction. When an explicit reference is needed, then the main memory or virtual\nmemory address must be given.\nSource and result operands can be in one of the three areas:\n\uf0b7main or virtual memory,\n\uf0b7CPU register or\n\uf0b7I/O device.\nThe steps involved in instruction execution is shown in the figure-\nInstruction Representation\nWithin the computer, each instruction is represented by a sequence of bits. The instruction is\ndivided into fields, corresponding to the constituent elements of the instruction. The instruction\nformat is highly machine specific and it mainly depends on the machine architecture. A simple\nexample of an instruction format is shown in the figure. It is assume that it is a 16-bit CPU. 4 bits"
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "are  used  to  provide  the  operation  code.  So,  we  may  have  to  16  (2  4  =  16)  different  set  of\ninstructions. With each instruction, there are two operands. To specify each operands, 6 bits are\nused. It is possible to provide 64 ( 2 6 = 64 ) different operands for each operand reference.\nIt is difficult to deal with binary representation of machine instructions. Thus, it has become\ncommon practice to use a symbolic representation of machine instructions. Opcodes are represented\nby abbreviations, called mnemonics , that indicate the operations. Common examples include:\nA simple instruction format.\nADD     Add\nSUB      Subtract\nMULT   Multiply\nDIV       Division\nLOAD   Load data from memory to CPU\nSTORE Store data to memory from CPU.\nOperands are also represented symbolically. For example, the instruction\nMULT R, X : R \u2190R x X\nmay mean multiply the value contained in the data location X by the contents of register R and put\nthe result in register R\nIn this example, X refers to the address of a location in memory and R refers to a particular register.\nThus, it is possible to write a machine language program in symbolic form. Each symbolic opcode\nhas a fixed binary representation, and the programmer specifies the location of each symbolic\noperand.\nInstruction Types\nThe instruction set of a CPU can be categorized as follows:\n\uf0b7Data  Processing:  Arithmatic  and  Logic  instructions  Arithmatic  instructions  provide\ncomputational capabilities for processing numeric data. Logic (Boolean) instructions operate\non the  bits  of a word as  bits  rather  than  as  numbers. Logic instructions  thus  provide\ncapabilities for processing any other type of data. There operations are performed primarily\non data in CPU registers.\n\uf0b7Data Storage: Memory instructions Memory instructions are used for moving data between\nmemory and CPU registers.\n\uf0b7Data Movement: I/O instructions I/O instructions are needed to transfer program and data\ninto memory from storage device or input device and the results of computation back to the\nuser.\n\uf0b7Control: Test and branch instructions\nTest instructions are used to test the value of a data word or the status of a computation. Branch\ninstructions are then used to branch to a different set of instructions depending on the decision\nmade.\nNumber of Addresses"
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "What  is  the  maximum  number  of  addresses  one  might  need  in  an  instruction?  Most  of  the\narithmantic and logic operations are either unary (one operand) or binary (two operands). Thus we\nneed a maximum of two addresses to reference operands. The result of an operation must be stored,\nsuggesting a third address. Finally after completion of an instruction, the next instruction must be\nfetched, and its address is needed.\nThis reasoning suggests that an instruction may require to contain four address references: two\noperands, one result, and the address of the next instruction. In practice, four address instructions\nare rare. Most instructions have one, two or three operands addresses, with the address of the next\ninstruction being implicit (obtained from the program counter).\nInstruction Set Design\nOne of the most interesting, and most analyzed, aspects of computer design is instruction set design.\nThe  instruction  set  defines  the  functions  performed  by  the  CPU.  The  instruction  set  is  the\nprogrammer's means of controlling the CPU. Thus programmer requirements must be considered in\ndesigning the instruction set.\nMost important and fundamental design issues:\nOperation repertoire : How many and which operations to provide, and how complex operations\nshould be.\nData Types                : The various type of data upon which operations are performed.\nInstruction format     : Instruction length (in bits), number of addresses, size of various fields and so\non.\nRegisters                   : Number of CPU registers that can be referenced by instructions and their use.\nAddressing                : The mode or modes by which the address of an operand is specified.\nTypes of Operands\nMachine instructions operate on data. Data can be categorised as follows :\nAddresses: It basically indicates the address of a memory location. Addresses are nothing but the\nunsigned integer, but treated in a special way to indicate the address of a memory location. Address\narithmatic is somewhat different from normal arithmatic and it is related to machine architecture.\nNumbers: All machine languages include numeric data types. Numeric data are classified into two\nbroad categories: integer or fixed point and floating point.\nCharacters: A common form of data is text or character strings. Since computer works with bits, so\ncharacters are represented by a sequence of bits. The most commonly used coding scheme is ASCII\n(American Standard Code for Information Interchange) code.\nLogical Data: Normally each word or other addressable unit (byte, halfword, and so on) is treated as\na single unit of data. It is sometime useful to consider an n-bit unit as consisting of n 1-bit items of\ndata, each item having the value 0 or 1. When data are viewed this way, they are considered to be\nlogical data. Generally 1 is treated as true and 0 is treated as false.\nTypes of Opearations\nThe number of different opcodes and their types varies widely from machine to machine. However,\nsome general type of operations are found in most of the machine architecture. Those operations\ncan be categorized as follows:\n\u25a0 Data Transfer\n\u25a0 Arithmatic\n\u25a0 Logical\n\u25a0 Conversion\n\u25a0 Input Output (I/O)\n\u25a0 System Control\n\u25a0 Transfer Control\nData Transfer:\nThe most fundamental type of machine instruction is the data transfer instruction. The data transfer\ninstruction must specify several things. First, the location of the source and destination operands\nmust be specified. Each location could be memory, a register, or the top of the stack. Second, the\nlength of data to be transferred must be indicated. Third, as with all instructions with operands, the\nmode of addressing for each operand must be specified.\nThe CPU has to perform several task to accomplish a data transfer operation. If both source and\ndestination are registers, then the CPU simply causes data to be transferred from one register to\nanother; this is an operation internal to the CPU.\nIf one or both operands are in memory, then the CPU must perform some or all of the following\nactions:\na) Calculate the memory address, based on the addressing mode.\nb) If the address refers to virtual memory, translate from virtual to actual memory address.\nc) Determine whether the addressed item is in cache.\nd) If not, issue a command to the memory module.\nCommonly used data transfer operation:\nOperation Name                                 Description\nMove (Transfer)               Transfer word or block from source to destination\nStore                                 Transfer word or block from source to destination\nLoad (fetch)                      Transfer word from memory to processor\nExchange                          Transfer word from memory to processor\nClear (reset)                      Transfer word of 0s to destination\nSet                                     Transfer word of 0s to destination\nPush                                  Transfer word from source to top of stack\nPop                                    Transfer word from top of stack to destination\nArithmatic:\nMost machines provide the basic arithmatic operations like add, subtract, multiply, divide etc. These\nare  invariably  provided  for  signed  integer  (fixed-point)  numbers.  They  are  also  available  for\nfloating point number. The execution of an arithmatic operation may involve data transfer operation\nto provide the operands to the ALU input and to deliver the result of the ALU operation.\nCommonly used operation:\nOperation Name           Description\nAdd                      Compute sum of two operands\nSubtract               Compute sum of two operands\nMultiply               Compute product of two operands\nDivide                  Compute quotient of two operands\nAbsolute              Compute quotient of two operands\nNegate                 Change sign of operand\nIncrement            Change sign of operand\nDecrement           Subtract 1 from operand\nLogical:\nMost machines also provide a variety of operations for manipulating individual bits of a word or\nother addressable units.\nMost commonly available logical operations are:\nOperation Name           Description\nAND                              Performs the logical operation AND bitwise\nOR                                 Performs the logical operation AND bitwise\nNOT                              Performs the logical operation NOT bitwise\nExclusive OR                Performs the logical operation NOT bitwise\nTest                                Test specified condition; set flag(s) based on outcome\nCompare                        Make logical or arithmatic comparison Set flag(s) based on outcome\nSet  Control  Variables  Class  of  instructions  to  set  controls  for  protection  purposes,  interrupt\nhandling, timer control etc.\nShift                              Left (right) shift operand, introducing constant at end\nRotate                           Left (right) shift operation, with wraparound end\nConversion:\nConversion instructions are those that change the format or operate on the format of data. An\nexample is converting from decimal to binary.\nInput/Output:\nInput/Output instructions are used to transfer data between input/output devices and memory/CPU\nregister.\nCommonly available I/O operations are:\nOperation Name  Description\nInput (Read)        Transfer data from specified I/O port or device to destination (e.g., main memory\nor processor register)\nOutput (Write)      Transfer data from specified source to I/O port or device.\nStart I/O                Transfer instructions to I/O processor to initiate I/O operation.\nTest I/O                 Transfer status information from I/O system to specified destination\nSystem Control:\nSystem control instructions are those which are used for system setting and it can be used only in\nprivileged state. Typically, these instructions are reserved for the use of operating systems. For\nexample, a system control instruction may read or alter the content of a control register. Another\ninstruction may be to read or modify a storage protection key.\nTransfer of Control:\nIn most of the cases, the next instruction to be performed is the one that immediately follows the\ncurrent instruction in memory. Therefore, program counter helps us to get the next instruction. But\nsometimes it is required to change the sequence of instruction execution and for that instruction set\nshould  provide  instructions  to  accomplish  these  tasks.  For  these  instructions,  the  operation\nperformed by the CPU is to upload the program counter to contain the address of some instruction\nin memory. The most common transfer-of-control operations found in instruction set are: branch,\nskip and procedure call.\nBranch Instruction\nA branch instruction, also called a jump instruction, has one of its operands as the address of the\nnext instruction to be executed. Basically there are two types of branch instructions: Conditional\nBranch  instruction  and  unconditionla  branch  instruction.  In  case  of  unconditional  branch\ninstruction, the branch is made by updating the program counter to address specified in operand. In\ncase of conditional branch instruction, the branch is made only if a certain condition is met.\nOtherwise, the next instruction in sequence is executed.\nThere are two common ways of generating the condition to be tested in a conditional branch\ninstruction First most machines provide a 1-bit or multiple-bit condition code that is set as the result\nof some operations. As an example, an arithmetic operation could set a 2-bit condition code with\none of the following four values: zero, positive, negative and overflow. On such a machine, there\ncould be four different conditional branch instructions:\nBRP X Branch to location X   if result is positive\nBRN X Branch to location X  if result is negative\nBRZ X Branch to location X   is result is zero\nBRO X Branch to location X  if overflow occurs\nIn all of these cases, the result referred to is the result of the most recent operation that set the\ncondition code.\nAnother approach that can be used with three address instruction format is to perform a comparison\nand specify a\nbranch in the same instruction.\nFor example,\nBRE R1, R2, X Branch to X if contents of R1 = Contents of R2.\nSkip Instruction\nAnother common form of transfer-of-control instruction is the skip instruction. Generally, the skip\nimples that one instruction to be skipped; thus the implied address equals the address of the next\ninstruction plus one instruction length. A typical example is the increment-and-skip-if-zero (ISZ)\ninstruction. For example,\nISZ R1\nThis instruction will increment the value of the register R1. If the result of the increment is zero,\nthen it will skip the next instruction.\nProcedure Call Instruction\nA procedure is a self contained computer program that is incorporated into a large program. At any\npoint in the program the procedure may be invoked, or called. The processor is instructed to go and\nexecute the entire procedure and then return to the point from which the call took place.\nThe procedure mechanism involves two basic instructions: a call instruction that branches from the\npresent location to the procedure, and a return instruction that returns from the procedure to the\nplace from which it was called. Both of these are forms of branching instructions.\nSome important points regarding procedure call:\n\uf0b7A procedure can be called from more than one location.\n\uf0b7A procedure call can appear in a procedure. This allows the nesting of procedures to an\narbitrary depth.\n\uf0b7Each procedure call is matched by a return in the called program.\nSince we can call a procedure from a variety of points, the CPU must somehow save the return\naddress so that the\nreturn can take place appropriately. There are three common places for storing the return address:\n\uf0b7Register\n\uf0b7Start of procedure\n\uf0b7Top of stack\nConsider a machine language instruction CALL X, which stands for call procedure at location X. If\nthe register apprach is used, CALL X causes the following actions:\nRN \u2190 PC + IL\nPC \u2190 X \nwhere RN is a register that is always used for this purpose, PC is the program counter and IL is the\ninstruction length.\nThe called procedure can now save the contents of RN to be used for the later return. A second\npossibilities is to store the return address at the start of the procedure. In this case, CALL X causes\nX \u2190 PC + IL\nPC \u2190 X + 1\nBoth of these approaches have been used. The only limitation of these approaches is that they\nprevent the use of reentrant procedures. A reentrant procedure is one in which it is possible to have\nseveral calls open to it at the same time.\nA more general approach is to use stack. When the CPU executes a call, it places the return address\non the stack. When it executes a return, it uses the address on the stack.\nIt may happen that, the called procedure might have to use the processor registers. This will\noverwrite the contents of the registers and the calling environment will lose the information. So, it is\nnecessary to preserve the contents of processor register too along with the return address. The stack\nis used to store the contents of processor register. On return from the procedure call, the contents of\nthe stack will be popped out to appropriate registers.\nIn addition to provide a return address, it is also often necessary to pass parameters with a procedure\ncall. The most general approach to parameter passing is the stack. When the processor executes a\ncall, it not only stacks the return address, it stacks parameters to be passed to the called procedures.\nThe called procedure can access the parameters from the stack.Upon return, return parameters can\nalso be placed on the stack. The entire set of parameters, including return address, that is stored for\na procedure invocation is referred to as stack frame.\nMost commonly used transfer of control operation:\nOperation Name           Description\nJump (branch)           Unconditional transfer, load PC with specific address\nJump conditional    Test specific condition; either load PC with specific address or do nothing,\nbased on condition\nJump to subroutine   Place current program control information in known location; jump to specific\naddress\nReturn                       Replace contents of PC and other register from known location\nSkip                          Increment PC to skip next instruction\nSkip Conditional      Test specified condition; either skip or do nothing based on condition\nHalt                           Stop program execution\nInstruction Format:\nAn instruction format defines the layout of the bits of an instruction, in terms of its constituents\nparts. An instruction format must include an opcode and, implicitly or explicitly, zero or more\noperands. Each explit operand is referenced using one of the addressing mode that is available for\nthat  machine.  The  format  must,  implicitly  or  explictly,  indicate  the  addressing mode  of  each\noperand.  For  most  instruction  sets,  more  than  one  instruction  format  is  used.  Four  common\ninstruction format are shown in the figure on the next slide .\nInstruction Length:\nOn some machines, all instructions have the same length; on others there may be many different\nlengths. Instructions may be shorter than, the same length as, or more than the word length. Having\nall the instructions be the same length is simpler and make decoding easier but often wastes space,\nsince all instructions then have to be as long as the longest one. Possible relationship between\ninstruction length and word length is shown in the figure.\nGenerally there is a correlation between memory transfer length and the instruction length. Either\nthe instruction length should be equal to the memory transfer length or one should be a multiple of\nthe other. Also in most of the case there is a correlation between memory transfer length and word\nlength of the machine."
  },
  {
    "source": "Lecture 4 - Instruction Set.pdf",
    "text": "Allocation of Bits:\nFor a given instruction length, there is a clearly a trade-off between the number of opcodes and the\npower of the addressing capabilities. More opcodes obviously mean more bits in the opcode field.\nFor an instruction format of a given length, this reduces the number of bits available for addressing.\nThe following interrelated factors go into determining the use of the addressing bits:\nNumber of Addressing modes:\nSometimes as addressing mode can be indicated implicitly. In other cases, the addressing mode\nmust be explicit, and one or more bits will be needed.\nNumber of Operands:\nTypical instructions on today's machines provide for two operands. Each operand address in the\ninstruction might require its own mode indicator, or the use of a mode indicator could be limited to\njust one of the address field.\nRegister versus memory:\nA machine must have registers so that data can be brought into the CPU for processing. With a\nsingle user-visible register (usually called the accumulator), one operand address is implicit and\nconsumes no instruction bits. Even with multiple registers, only a few bits are needed to specify the\nregister. The more that registers can be used for operand references, the fewer bits are needed.\nNumber of register sets:\nA number of machines have one set of general purpose registers, with typically 8 or 16 registers in\nthe set. These registers can be used to store data and can be used to store addresses for displacement\naddressing. The trend recently has been away from one bank of general purpose registers and\ntoward a collection of two or more specialized sets (such as data and displacement).\nAddress range:\nFor addresses that reference memory, the range of addresses that can be referenced is related to the\nnumber of address bits. With displacement addressing, the range is opened up to the length of the\naddress register.\nAddress granularity:\nIn a system with 16- or 32-bit words, an address can reference a word or a byte at the designer's\nchoice. Byte addressing is convenient for character manipulation but requires, for a fixed size\nmemory, more address bits.\nVariable-Length Instructions:\nInstead of looking for fixed length instruction format, designer may choose to provide a variety of\ninstructions formats of different lengths. This tectic makes it easy to provide a large repertoire of\nopcodes, with different opcode lengths. Addressing can be more flexible, with various combinations\nof register and memory references plus addressing modes. With variable length instructions, many\nvariations can be provided efficiently and compactly. The principal price to pay for variable length\ninstructions is an increase in the complexity of the CPU.\nNumber of addresses :\nThe processor architecture is described in terms of the number of addresses contained in each\ninstruction. Most of the arithmatic and logic instructions will require more operands. All arithmatic\nand  logic  operations  are  either  unary  (one  source  operand,  e.g.  NOT)  or  binary  (two  source\noperands, e.g. ADD).\nThus, we  need  a maximum  of two  addresses  to  reference  source  operands.  The  result  of an\noperation must be stored, suggesting a third reference.\nThree address instruction formats are not common because they require a relatively long instruction\nformat to hold the three address reference.\nWith two address instructions, and for binary operations, one address must do double duty as both\nan operand and a result.\nIn one address instruction format, a second address must be implicit for a binary operation. For\nimplicit reference, a processor register is used and it is termed as accumulator(AC). the accumulator\ncontains one of the operands and is used to store the result.\nConsider a simple arithmatic expression to evalute:\nY= (A + B) / (C * D)"
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 5 - Introduction to CPU Design\nby \nE. Kondela \nThe operation or task that must perform by CPU are:\n\uf0b7Fetch Instruction: The CPU reads an instruction from memory.\n\uf0b7Interprete Instruction: The instruction is decoded to determine what action is required.\n\uf0b7Fetch Data: The execution of an instruction may require reading data from memory or I/O\nmodule.\n\uf0b7Process data: The execution of an instruction may require performing some arithmatic or\nlogical operation on data.\n\uf0b7Write data: The result of an execution may require writing data to memory or an I/O\nmodule.\nTo do these tasks, it should be clear that the CPU needs to store some data temporarily. It must\nremember the location of the last instruction so that it can know where to get the next instruction. It\nneeds to store instructions and data temporarily while an instruction is beign executed. In other\nwords, the CPU needs a small internal memory.\nThese storage location are generally referred as registers.\nThe major components of the CPU are an arithmatic and logic unit (ALU) and a control unit (CU).\nThe ALU does the actual computation or processing of data. The CU controls the movement of data\nand instruction into and out of the CPU and controls the operation of the ALU.\nThe CPU is connected to the rest of the system through system bus. Through system bus, data or\ninformation gets transferred between the CPU and the other component of the system. The system\nbus may have three components:\nData Bus:\nData bus is used to transfer the data between main memory and CPU.\nAddress Bus:\nAddress bus is used to access a particular memory location by putting the address of the memory\nlocation.\nControl Bus:\nControl bus is used to provide the different control signal generated by CPU to different part of the\nsystem. As for example, memory read is a signal generated by CPU to indicate that a memory read\noperation has to be performed. Through control bus this signal is transferred to memory module to\nindicate the required operation.\nThere are three basic components of CPU: register bank, ALU and Control Unit. There are several\ndata movements between these units and for that an internal CPU bus is used. Internal CPU bus is\nneeded to transfer data between the various registers and the ALU, because the ALU in fact operates\nonly on data in the internal CPU memory.\nRegister Organization\nA computer system employs a memory hierarchy. At the highest level of hierarchy, memory is\nfaster, smaller and\nmore expensive. Within the CPU, there is a set of registers which can be treated as a memory in the\nhighest level of\nhierarchy. The registers in the CPU can be categorized into two groups:\n\uf0b7User-visible registers: These enables the machine - or assembly-language programmer to\nminimize main memory reference by optimizing use of registers.\n\uf0b7Control and status registers: These are used by the control unit to control the operation of the\nCPU.\nOperating system programs may also use these in privileged mode to control the execution of\nprogram."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "User-visible Registers:\nThe user-visible registers can be categorized as follows:\n\uf0b7General Purpose Registers\n\uf0b7Data Registers\n\uf0b7Address Registers\n\uf0b7Condition Codes\nGeneral-purpose registers  can be assigned to a variety of functions by the programmer. In some\ncases,  general-purpose  registers  can  be  used  for  addressing  functions  (e.g.,  register  indirect,\ndisplacement). In other cases, there is a partial or clean separation between data registers and\naddress registers. \nData registers may be used to hold only data and cannot be employed in the calculation of an\noperand address.\nAddress registers  may be somewhat general purpose, or they may be devoted to a particular\naddressing mode.\nExamples include the following:\n\uf0b7Segment  pointer: In a  machine  with segment  addressing, a segment  register  holds  the\naddress of the base of the segment. There may be multiple registers, one for the code\nsegment and one for the data segment.\n\uf0b7Index registers: These are used for indexed addressing and may be autoindexed.\n\uf0b7Stack pointer: If there is user visible stack addressing, then typically the stack is in memory\nand there is a dedicated register that points to the top of the stack.\nCondition Codes (also referred to as flags) are bits set by the CPU hardware as the result of the\noperations. For example, an arithmatic operation may produce a positive, negative, zero or overflow\nresult. In addition to the result itself beign stored in a register or memory, a condition code is also\nset. The code may be subsequently be tested as part of a condition branch operation. Condition code\nbits are collected into one or more registers.\nThere are a variety of CPU registers that are employed to control the operation of the CPU. Most of\nthese, on most machines, are not visible to the user.\nDifferent machines will have different register organizations and use different terminology. We will\ndiscuss here the most commonly used registers which are part of most of the machines.\nFour registers are essential to instruction execution:\nProgram Counter (PC):  Contains the address of an instruction to be fetched. Typically, the PC is\nupdated by the CPU after each instruction fetched so that it always points to the next instruction to\nbe executed. A branch or skip instruction will also modify the contents of the PC.\nInstruction Register (IR):  Contains the instruction most recently fetched. The fetched instruction\nis loaded into an IR, where the opcode and operand specifiers are analyzed.\nMemory Address Register (MAR):  Containts the address of a location of main memory from\nwhere information has to be fetched or information has to be stored. Contents of MAR is directly\nconnected to the address bus.\nMemory Buffer Register (MBR):  Contains a word of data to be written to memory or the word\nmost recently read. Contents of MBR is directly connected to the data bus. It is also known as\nMemory Data Register(MDR).\nApart from these specific register, we may have some temporary registers which are not visible to\nthe user. As such, there may be temporary buffering registers at the boundary to the ALU; these\nregisters serve as input and output registers for the ALU and exchange data with the MBR and user\nvisible registers.\nProcessor Status Word\nAll CPU designs include a register or set of registers, often known as the processor status word\n(PSW), that contains status information. The PSW typically contains condition codes plus other\nstatus information. Common fields or flags include the following:\n\u25cf Sign : Contains the sign bit of the result of the last arithmatic operation.\n\u25cf Zero : Set when the result is zero.\n\u25cf Carry : Set if an operation resulted in a carry (addition) into or borrow (subtraction) out of a high\norder bit.\n\u25cf Equal : Set if a logical campare result is equal.\n\u25cf Overflow : Used to indicate arithmatic overflow.\n\u25cf Interrupt enable/disable : Used to enable or disable interrupts.\n\u25cf Supervisor : Indicate whether the CPU is executing in supervisor or user mode.\nCertain privileged instructions can be executed only in supervisor mode, and certain areas of\nmemory can be accessed only in supervisor mode.\nApart from these, a number of other registers related to status and control might be found in a\nparticular CPU design. In addition to the PSW, there may be a pointer to a block of memory\ncontaining additional status information (e.g. process control blocks).\nConcept of Program Execution\nThe instructions constituting a program to be executed by a computer are loaded in sequential\nlocations in its main memory. To execute this program, the CPU fetches one instruction at a time\nand performs the functions specified. Instructions are fetched from successive memory locations\nuntil the execution of a branch or a jump instruction.\nThe CPU keeps track of the address of the memory location where the next instruction is located\nthrough the use of a dedicated CPU register, referred to as the program counter (PC). After fetching\nan instruction, the contents of the PC are updated to point at the next instruction in sequence.\nFor simplicity, let us assume that each instruction occupies one memory word. Therefore, execution\nof one instruction requires the following three steps to be performed by the CPU:\n1. Fetch the contents of the memory location pointed at by the PC. The contents of this location are\ninterpreted as an instruction to be executed. Hence, they are stored in the instruction register (IR).\nSymbolically this can be written as:\nIR = [ [PC] ]\n2. Increment the contents of the PC by 1.\nPC = [PC] + 1\n3. Carry out the actions specified by the instruction stored in the IR.\nThe first two steps are usually referred to as the fetch phase and the step 3 is known as the execution\nphase. Fetch cycle basically involves read the next instruction from the memory into the CPU and\nalong with that update the contents of the program counter. In the execution phase, it interpretes the\nopcode and perform the indicated operation. The instruction fetch and execution phase together\nknown as instruction cycle. The basic instruction cycle is shown in the figure.\nIn cases, where an instruction occupies more than one word, step 1 and step 2 can be repeated as\nmany times as necessary to fetch the complete instruction. In these cases, the execution of a\ninstruction may involve one or more operands in memory, each of which requires a memory access.\nFurther, if indirect addressing is used, then additional memory access are required.\nThe fetched instruction is loaded into the instruction register. The instruction contains bits that\nspecify the action to be\nperformed by the processor. The processor interpretes the instruction and performs the required\naction. In general,\nthe actions fall into four categories:\n\uf0b7Processor-memory: Data may be transfrred from processor to memory or from memory to\nprocessor.\n\uf0b7Processor-I/O:  Data  may  be  transferred  to  or from a  peripheral  device  by transferring\nbetween the processor and an I/O module.\n\uf0b7Data processing: The processor may perform some arithmatic or logic operation on data.\n\uf0b7Control: An instruction may specify that the sequence of execution be altered.\nThe main line of activity consists of alternating instruction fetch and instruction execution activities.\nAfter an instruction is fetched, it is examined to determine if any indirect addressing is involved. If\nso, the required operands are fetched using indirect addressing.\nThe execution cycle of a perticular instruction may involve more than one reference to memory.\nAlso, instead of memory references, an instruction  may specify  an I/O  operation. With these\nadditional considerations the basic instruction cycle can be expanded with more details view in this\nfigure. The figure is in the form of a state diagram."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "Processor Organization\nThere are several components inside a CPU, namely, ALU, control unit, general purpose register,\nInstruction registers etc. Now we will see how these components are organized inside CPU. There\nare several ways to place these components and inteconnect them. One such organization is shown\nin the figure A.\nIn this case, the arithmatic and logic unit (ALU), and all CPU registers are connected via a single\ncommon bus. This bus is internal to CPU and this internal bus is used to transfer the information\nbetween different components of the CPU. This organization is termed as single bus organization,\nsince only one internal bus is used for transferring of information between different components of\nCPU. We have external bus or buses to CPU also to connect the CPU with the memory module and\nI/O devices. The external memory bus is also shown in the figure A connected to the CPU via the\nmemory data and address register MDR and MAR .\nThe number and function of registers R0 to R(n-1) vary considerably from one machine to another.\nThey may be given for general-purpose for the use of the programmer. Alternatively, some of them\nmay be dedicated as special-purpose registers, such as index register or stack pointers .\nIn this organization, two registers, namely Y and Z are used which are transperant to the user.\nProgrammer can not directly access these two registers. These are used as input and output buffer to\nthe ALU which will be used in ALU operations. They will be used by CPU as temporary storage for\nsome instructions."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "For the execution of an instruction, we need to perform an instruction cycle. An instruction cycle\nconsists of two\nphase,\n\uf0b7Fetch cycle and\n\uf0b7Execution cycle.\nMost of the operation of a CPU can be carried out by performing one or more of the following\nfunctions in some prespecified sequence:\n1.Fetch the contents of a given memory location and load them into a CPU register.\n2.Store a word of data from a CPU register into a given memory location.\n3.Transfer a word of data from one CPU register to another or to the ALU.\n4.Perform an arithmatic or logic operation, and store the result in a CPU register.\nNow we will examine the way in which each of the above functions is implemented in a computer.\nFetching a Word from Memory:\nInformation is  stored in memory  location  indentified  by their  address. To fetch  a word from\nmemory, the CPU has to specify the address of the memory location where this information is\nstored and request a Read operation. The information may include both, the data for an operation or\nthe instruction of a program which is available in main memory.\nTo perform a memory fetch operation, we need to complete the following tasks:\nThe CPU transfers the address of the required memory location to the Memory Address Register\n(MAR).\nThe MAR is connected to the memory address line of the memory bus, hence the address of the\nrequired word is transfered to the main memory.\nNext, CPU uses the control lines of the memory bus to indicate that a Read operation is initiated.\nAfter issuing this request, the CPU waits until it receives an answer from the memory, indicating\nthat the requested operation has been completed.\nThis  is  accomplished  by  another  control  signal  of  memory  bus  known  as  Memory-Function-\nComplete (MFC).\nThe memory set this signal to 1 to indicate that the contents of the specified memory location are\navailable in memory data bus.\nAs soon as MFC signal is set to 1, the information available in the data bus is loaded into the\nMemory Data Register (MDR) and this is available for use inside the CPU.\nAs an example, assume that the address of the memory location to be accessed is kept in register R2\nand that the\nmemory  contents  to  be  loaded  into  register  R1.  This  is  done  by  the  following  sequence  of\noperations:\n1. MAR\u2190 [R2]\n2. Read\n3. Wait for MFC signal\n4. R1\u2190 [MDR]\nThe time required for step 3 depends on the speed of the memory unit. In general, the time required\nto access a word from the memory is longer than the time required to perform any operation within\nthe CPU.\nThe scheme that is used here to transfer data from one device (memory) to another device (CPU) is\nreferred to as an asynchronous transfer.\nThis asynchronous transfer enables transfer of data between two independent devices that have\ndifferent speeds of operation. The data transfer is synchronised with the help of some control\nsignals. In this example, Read request and MFC signal are doing the synchronization task.\nAn alternative scheme is synchronous transfer. In this case all the devices are controlled by a\ncommon  clock  pulse  (continously  running  clock  of  a  fixed  frequency).  These  pulses  provide\ncommon timing signal to the CPU and the main memory. A memory operation is completed during\nevery  clock  period.  Though  the  synchronous  data  transfer  scheme  leads  to  a  simpler\nimplementation, it is difficult to accommodate devices with widely varying speed. In such cases, the\nduration of the clock pulse will be synchronized to the slowest device. It reduces the speed of all the\ndevices to the slowest one.\nStoring a word into memory\nThe procedure of writing a word into memory location is similar to that for reading one from\nmemory. The only difference is that the data word to be written is first loaded into the MDR, the\nwrite command is issued. As an example, assumes that the data word to be stored in the memory is\nin register R1 and that the memory address is in register R2. The memory write operation requires\nthe following sequence:\n1. MAR\u2190 [R2]\n2. MDR\u2192 [R1]\n3. Write\n4. Wait for MFC\n- In this case step 1 and step 2 are independent and so they can be carried out in any order. In fact,\nstep 1 and 2 can be carried out simultaneously, if this is allowed by the architecture, that is, if these\ntwo data transfers (memory address and data) do not use the same data path.\nIn case of both memory read and memory write operation, the total time duration depends on wait\nfor the MFC signal, which depends on the speed of the memory module.\nThere is a scope to improve the performance of the CPU , if CPU is allowed to perform some other\noperation  while  waiting  for  MFC  signal.  D  uring  the  period,  CPU  can  perform  some  other\ninstructions which do not require the use of MAR and MDR.\nRegister Transfer Operation\nRegister transfer operations enable data transfer between various blocks connected to the common\nbus of CPU . We have several registers inside CPU and it is needed to transfer information from one\nregister another. As for example during memory write operation data from appropriate register must\nbe moved to MDR. Since the input output lines of all the register are connected to the common\ninternal bus, we need appropriate input output gating. The input and output gates for register R i are\ncontrolled by the signal R i in and R i out respectively.\nThus, when Riin set to 1 the data available in the common bus is loaded into Ri. Similarly when,\nRiout is set to 1, the contents of the register Ri are placed on the bus. To transfer data from one\nregister to other register, we need to generate the appropriate register gating signal.\nFor example, to transfer the contents of register R1 to register R2 , the following actions are needed:\n\uf0b7Enable the output gate of register R1 by setting R1out to 1.\n-- This places the contents of R1 on the CPU bus.\n\uf0b7Enable the input gate of register R2 by setting R2in to 1.\n-- This loads data from the CPU bus into the register R2 .\nPerforming the arithmetic or logic operation:\nGenerally,  ALU  is  used  inside  CPU  to  perform  arithmetic  and  logic  operation.  ALU  is  a\ncombinational logic circuit which does not have any internal storage.\nTherefore, to perform any arithmetic or logic operation (say binary operation) both the input should\nbe made available at the two inputs of the ALU simultaneously. Once both the inputs are available\nthen appropriate signal is generated to perform the required operation.\nWe may have to use temporary storage (register) to carry out the operation in ALU .\nThe sequence of operations that have to carried out to perform one ALU operation depends on the\norganization of the CPU. Consider an organization in which one of the operand of ALU is stored in\nsome temporary register Y and other operand is directly taken from CPU internal bus. The result of\nthe ALU operation is stored in another temporary register Z.\nMultiple Bus Organization\nTill now we have considered only one internal bus of CPU. The single-bus organization, which is\nonly one of the possibilities for interconnecting different building blocks of CPU.\nAn alternative structure is the two bus structure, where two different internal buses are used in CPU.\nAll register outputs are connected to bus A, add all registered inputs are connected to bus B.\nThere is a special arrangement to transfer the data from one bus to the other bus. The buses are\nconnected through the bus tie G. When this tie is enabled data on bus A is transfer to bus B. When G\nis disabled, the two buses are electrically isolated.\nSince two buses are used here the temporary register Z is not required here which is used in single\nbus organization to store the result of ALU. Now result can be directly transferred to bus B, since\none of the inputs is in bus A. With the bus tie disabled, the result can directly be transferred to\ndestination register.\nFor example, for the operation, [R3] \u2190 [R1] + [R2] can now be performed as\n1. R1 out , G enable , Y in\n2. R2 out , Add, ALU out , R3 in"
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "In this case source register R2 and destination register R3 has to be different, because the two\noperations R2 in and R2 out can not be performed together. If the registers are made of simple\nlatches then only we have the restriction.\nWe  may  have  another  CPU  organization,  where  three  internal  CPU  buses  are  used.  In  this\norganization each bus connected to only one output and number of inputs. The elimination of the\nneed for connecting more than one output to the same bus leads to faster bus transfer and simple\ncontrol. A simple three-bus organization is shown in the figure D.\nA multiplexer is provided at the input to each of the two work registers A and B, which allow them\nto be loaded from either the input data bus or the register data bus. In the diagram, a possible\ninterconnection of three-bus organization is  presented, there may be different interconnections\npossible. In this three bus organization, we are keeping two input data buses instead of one that is\nused in two bus organization.\nTwo separate input data buses are present \u2013 one is for external data transfer, i.e. retrieving from\nmemory and the second one is for internal data transfer that is transferring data from general\npurpose register to other building block inside the CPU."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "Execution of a Complete Instructions:\nWe have discussed about four different types of basic operations:\n\uf0b7Fetch information from memory to CPU\n\uf0b7Store information to CPU register to memory\n\uf0b7Transfer of data between CPU registers.\n\uf0b7Perform arithmetic or logic operation and store the result in CPU registers.\nTo execute a complete instruction we need to take help of these basic operations and we need to\nexecute these operation in some particular order to execute an instruction.\nAs for example, consider the instruction : \"Add contents of memory location NUM to the contents\nof register R1 and store the result in register R1.\" For simplicity, assume that the address NUM is\ngiven explicitly in the address field of the instruction .That is, in this instruction, direct addressing\nmode is used.\nExecution of this instruction requires the following action :\n1.Fetch instruction\n2.Fetch first operand (Contents of memory location pointed at by the address field of the\ninstruction)\n3.Perform addition\n4.Load the result into R1.\nInstruction execution proceeds as follows:\nIn Step1:\nThe instruction fetch operation is initiated by loading the contents of the PC into the MAR and\nsending a read request to memory.\nTo perform this task first of all the contents of PC have to be brought to internal bus and then it is\nloaded to MAR.To perform this task control circuit has to generate the PC out signal and MAR in\nsignal. After issuing the read signal, CPU has to wait for some time to get the MFC signal. During\nthat time PC is updated by 1 through the use of the ALU. This is accomplished by setting one of the\ninputs to the ALU (Register Y) to 0 and the other input is available in bus which is current value of\nPC. At the same time, the carry-in to the ALU is set to 1 and an add operation is specified.\nIn Step 2:\nThe updated value is moved from register Z back into the PC. Step 2 is initiated immediately after\nissuing the memory Read request without waiting for completion of memory function. This is\npossible, because step 2 does not use the memory bus and its execution does not depend on the\nmemory read operation.\nIn Step 3:\nStep3 has been delayed until the MFC is received. Once MFC is received, the word fetched from\nthe memory is transfered to IR (Instruction Register), Because it is an instruction. Step 1 through 3\nconstitute the instruction fetch phase of the control sequence.\nThe instruction fetch portion is same for all instructions. Next step onwards, instruction execution\nphase takes place.\nAs soon as the IR is loaded with instruction, the instruction decoding circuits interprets its contents.\nThis\nenables the control circuitry to choose the appropriate signals for the remainder of the control\nsequence, step 4 to 8,\nwhich we referred to as the execution phase. To design the control sequence of execution phase, it is\nneeded to\nhave the knowledge of the internal structure and instruction format of the PU. Secondly , the length\nof instruction\nphase is different for different instruction.\nIn this example , we have assumed the following instruction format :\nopcode M R\ni.e., opcode: Operation Code\nM: Memory address for source\nR: Register address for source/destination\nIn Step 5 :\nThe destination field of IR, which contains the address of the register R1, is used to transfer the\ncontents of register R1 to register Y and wait for Memory function Complete. When the read\noperation is completed, the memory operand is available in MDR.\nIn Step 6 :\nThe result of addition operation is performed in this step.\nIn Step 7:\nThe result of addition operation is transferred from temporary register Z to the destination register\nR1 in this step.\nIn step 8 :\nIt indicates the end of the execution of the instruction by generating End signal. This indicates\ncompletion of execution of the current instruction and causes a new fetch cycle to be started by\ngoing back to step 1.\nBranching\nWith the help of branching instruction, the control of the execution of the program is transfered\nfrom one particular position to some other position, due to which the sequence flow of control is\nbroken. Branching is accomplished by replacing the current contents of the PC by the branch\naddress, that is, the address of the instruction to which branching is required.\nConsider a branch instruction in which branch address is obtained by adding an offset X, which is\ngiven in the address field of the branch instruction, to the current value of PC.\nConsider the following unconditional branch instruction\n JUMP X\ni.e., the format is\nop-code Offset of jump\nDesign of Control Unit\nTo execute an instruction, the control unit of the CPU must generate the required control signal in\nthe proper sequence. As for example, during the fetch phase, CPU has to generate PC out signal\nalong with other required signal in the first clock pulse. In the second clock pulse CPU has to\ngenerate PC in signal along with other required signals.\nSo, during fetch phase, the proper sequence for generating the signal to retrieve from and store to\nPC is PCout and PCin .\nTo generate the control signal in proper sequence, a wide variety of techniques exist. Most of these\ntechniques, howeve, fall into one of the two categories,\n1. Hardwired Control\n2. Microprogrammed Control.\nHardwired Control\nIn this hardwired control techniques, the control signals are generated by means of hardwired\ncircuit. The main objective of control unit is to generate the control signal in proper sequence.\nConsider the sequence of control signal required to execute the ADD instruction that is explained in\nprevious  lecture.  It  is  obvious  that  eight  non-overlapping  time  slots  are  required  for  proper\nexecution of the instruction represented by this sequence. Each time slot must be at least long\nenough for the function specified in the corresponding step to be completed.\nSince, the control unit is implemented by hardwire device and every device is having a propagation\ndelay, due to which it requires some time to get the stable output signal at the output port after\ngiving the input signal. So, to find out the time slot is a complicated design task.\nFor the moment, for simplicity, let us assume that all slots are equal in time duration. Therefore the\nrequired controller may be implemented based upon the use of a counter driven by a clock.\nEach state, or count, of this counter corresponds to one of the steps of the control sequence of the\ninstructions of the CPU.\nIn the previous lecture, we have mentioned control sequence for execution of two instructions only\n(one is for add and other one is for branch). Like that we need to design the control sequence of all\nthe instructions.\nBy looking into the design of the CPU, we may say that there are various instruction for add\noperation. As for example,\nADD NUM R1 Add the contents of memory location specified by NUM to the contents of register\nR1 .\nADD R2 R1 Add the contents of register R 2 to the contents of register R 1 .\nThe control sequence for execution of these two ADD instructions are different. Of course, the fetch\nphase of all the instructions remain same.\nIt is clear that control signals depend on the instruction, i.e., the contents of the instruction register.\nIt is also observed that execution of some of the instructions depend on the contents of condition\ncode or status flag register, where the control sequence depends in conditional branch instruction.\nHence, the required control signals are uniquely determined by the following information:\n\uf0b7Contents of the control counter.\n\uf0b7Contents of the instruction register.\n\uf0b7Contents of the condition code and other status flags.\nThe external inputs represent the state of the CPU and various control lines connected to it, such as\nMFC status signal. The condition codes/ status flags indicates the state of the CPU. These includes\nthe status flags like carry, overflow, zero, etc.\nThe structure of control unit can be represented in a simplified view by putting it in block diagram.\nThe detailed hardware involved may be explored step by step. The simplified view of the control\nunit is given in the figure above (A) The decoder/encoder block is simply a combinational circuit\nthat generates the required control outputs depending on the state of all its input.\nThe decoder part of decoder/encoder part provide a separate signal line for each control step, or\ntime slot in the control sequence. Similarly, the output of the instructor decoder consists of a\nseparate line for each machine instruction loaded in the IR, one of the output line INS 1 to INS m is\nset to 1 and all other lines are set to 0."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "All input signals to the encoder block should be combined to generate the individual control signals.\nIn the previous section, we have mentioned the control sequence of the instruction,\n\"Add contents of memory location address in memory direct made to register R 1 ( ADD_MD)\",\n\"Control sequence for an unconditional branch instruction (BR)\",\nalso, we have mentioned about Branch on negative (BRN).\nConsder those three CPU instruction ADD_MD, BR, BRN.\nIt is required to generate many control signals by the control unit. These are basically coming out\nfrom the encoder circuit of the control signal generator. The control signals are: PC in , PC out , Z\nin , Z out , MAR in , ADD, END, etc.\nBy looking into the above three instructions, we can write the logic function for Z in as :\nZ in = T 1 + T 6 . ADD_MD + T 5 . BR + T 5 . BRN + . . . . . . . . . . . . . .\nFor all instructions, in time step1 we need the control signal Z in to enable the input to register Z in\ntime cycle T 6 of ADD_MD instruction, in time cycle T 5 of BR instruction and so on.\nSimilarly, the Boolean logic function for ADD signal is\nADD = T 1 + T 6 . ADD_MD + T 5 . BR + . . . . . . . . . . . . . .\nThese logic functions can be implemented by a two level combinational circuit of AND and OR\ngates. Similarly, the END control signal is generated by the logic function :\nEND = T 8 . ADD_MD + T 7 . BR + ( T 7 . N + T 4 . N^) . BRN + . . . . . . . . . . . . . .\nThis END signal indicates the end of the execution of an instruction, so this END signal can be used\nto start a new instruction fetch cycle by resetting the control step counter to its starting value."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "The signal ADD_MD, BR, BRN etc. are coming from instruction decoder circuits which depends\non the contents of IR.\nThe signal T 1 , T 2 , T 3 etc are coming out from step decoder depends on control step counter.\nThe signal N (Negative) is coming from condition code register.\nWhen wait for MFC (WMFC) signal is generated, then CPU does not do any works and it waits for\nan MFC signal from memory unit. In this case, the desired effect is to delay the initiation of the next\ncontrol step until the MFC signal is received from the main memory. This can be incorporated by\ninhibiting the advancement of the control step counter for the required period.\nLet us assume that the control step counter is controlled by a signal called RUN.\nBy looking at the control sequence of all the instructions, the WMFC signal is generated as:\nWMFC = T 2 + T 5 . ADD_MD + . . . . . . . . . . . . . .\nThe RUN signal is generated with the help of WMFC signal and MFC signal. The arrangement is\nshown in the figure."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "The MFC signal is generated by the main memory whose operation is independent of CPU clock.\nHence MFC is an asynchronous signal that may arrive at any time relative to the CPU clock. It is\npossible to synchronized with CPU clock with the help of a D flip-flop.\nWhen WMFC signal is high, then RUN signal is low. This run signal is used with the master clock\npulse through an AND gate. When RUN is low, then the CLK signal remains low, and it does not\nallow to progress the control step counter.\nWhen the MFC signal is received, the run signal becomes high and the CLK signal becomes same\nwith the MCLK signal and due to which the control step counter progresses. Therefore, in the next\ncontrol step, the WMFC signal goes low and control unit operates normally till the next memory\naccess signal is generated.\nMicroprogrammed Control\nIn hardwired control, we saw how all the control signals required inside the CPU can be generated\nusing a state counter and a PLA circuit.\nThere is an alternative approach by which the control signals required inside the CPU can be\ngenerated  .  This  alternative  approach  is  known  as  microprogrammed  control  unit.  In\nmicroprogrammed control unit, the logic of the control unit is specified by a microprogram. A\nmicroprogram consists of a sequence of instructions in a microprogramming language. These are\ninstructions that specify microoperations.\nA microprogrammed control unit is a relatively simple logic circuit that is capable of (1) sequencing\nthrough microinstructions and (2) generating control signals to execute each microinstruction.\nThe concept of microprogram is similar to computer program. In computer program the complete\ninstructions  of  the  program  is  stored  in  main  memory  and  during  execution  it  fetches  the\ninstructions from main memory one after another. The sequence of instruction fetch is controlled by\nprogram counter (PC) . Microprogram are stored in microprogram memory and the execution is\ncontrolled by microprogram counter (uPC).\nMicroprogram consists of microinstructions which are nothing but the strings of 0's and 1's. In a\nparticular instance, we read the contents of one location of microprogram memory, which is nothing\nbut a microinstruction. Each output line ( data line ) of microprogram memory corresponds to one\ncontrol signal. If the contents of the memory cell is 0, it indicates that the signal is not generated\nand if the contents of memory cell is 1, it indicates to generate that control signal at that instant of\ntime.\nControl Word (CW) :\nControl word is defined as a word whose individual bits represent the various control signal.\nTherefore each of the control steps in the control sequence of an instruction defines a unique\ncombination of 0s and 1s in the CW. A sequence of control words (CWs) corresponding to the\ncontrol sequence of a machine instruction constitutes the microprogram for that instruction.\nThe  individual  control  words  in  this  microprogram  are  referred  to  as  microinstructions.  The\nmicroprograms corresponding to the instruction set of a computer are stored ina aspecial memory\nwhich will be referred to as the microprogram memory. The control words related to an instructions\nare stored in microprogram memory.\nThe control unit can generate the control signals for any instruction by sequencially reading the\nCWs of the corresponding microprogram from the microprogram memory. To read the control word\nsequentially from the microprogram memory a microprogram counter ( PC) is needed.\nThe basic organization of a microprogrammed control unit is shown in the figure. The \"starting\naddress generator\" block is responsible for loading the starting address of the microprogram into the\nPC everytime a new instruction is loaded in the IR. The PC is then automatically incremented by\nthe clock, and it reads the successive microinstruction from memory.\nEach\nmicroinstruction basically provides the required control signal at that time step. The microprogram\ncounter ensures that the control signal will be delivered to the various parts of the CPU in correct\nsequence.\nWe have some instructions whose execution depends on the status of condition codes and status\nflag, as for example, the branch instruction. During branch instruction execution, it is required to\ntake  the  decision  between  the  alternative  action.  To  handle  such  type  of  instructions  with\nmicroprogrammed  control,  the  design  of  control  unit  is  based  on  the  concept  of  conditional\nbranching  in  the  microprogram.  For  that  it  is  required  to  include  some  conditional  branch"
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "microinstructions. In conditional microinstructions, it is required to specify the address of the\nmicroprogram memory to which the control must direct. It is known as branch address. Apart from\nbranch address, these microinstructions can specify which of the states flags, condition codes, or\npossibly, bits of the instruction register should be checked as a condition for branching to take\nplace.\nTo  support  microprogram  branching,  the  organization  of  control  unit  should  be  modified  to\naccommodate the branching decision.\nTo generate the branch address, it is required to know the status of the condition codes and status\nflag. \nTo generate the starting address, we need the instruction which is present in IR. But for branch\naddress generation we have to check the content of condition codes and status flag.\nThe organization of control unit to enable conditional branching in the microprogram is shown in\nthe figure.\nThe control bits of the microinstructions word which specify the branch conditions and address are\nfed to the \"Starting and branch address generator\" block.\nThis block performs the function of loading a new address into the satisfied. PC when the condition\nof branch instruction is In a computer program we have seen that execution of every instruction\nconsists of two part - fetch phase and execution phase of the instruction. It is also observed that the\nfetch phase of all instruction is same.\nIn  microprogrammed  controlled  control  unit,  a  common  microprogram  is  used  to  fetch  the\ninstruction. This microprogram is stored in a specific location and execution of each instruction\nstart from that memory location."
  },
  {
    "source": "Lecture 5 - CPU Design.pdf",
    "text": "At the end of fetch microprogram, the starting address generator unit calculate the appropriate\nstarting address of the microprogram for the instruction which is currently present in IR. After the\nPC controls the execution of microprogram which generates the appropriate control signal in proper\nsequence. \nDuring  the  execution  of  a  microprogram,  the  PC  is  always  incremented  everytime  a  new\nmicroinstruction is fetched from the microprogram memory, except in the following situations :\n1. When an End instruction is encountered, uPC is loaded with the address of the first CW in the\nmicroprogram for the instruction fetch cycle.\n2. When a new instruction is loaded into the IR, the PC is loaded with the starting address of the\nmicroprogram for that instruction.\n3. When a branch microinstruction is encountered, and the branch condition is satisfied, the PC is\nloaded with the branch address."
  },
  {
    "source": "Lecture 6 - RISC.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 1 - Introduction to RISC\nby \nE. Kondela \nIntroduction \nSince the development of the stored program computer around 1950, there are few innovations in\nthe area of computer organization and architecture. Some of the major developments are:\n\uf0b7The Family Concept : Introduced by IBM with its system/360 in 1964 followed by DEC,\nwith  its  PDP-8.  The  family  concept  decouples  the  architecture  of  a  machine  from  its\nimplementation.  A  set  of  computers  are  offered,  with  different  price/performance\ncharacteristics, that present the same architecture to the user.\n\uf0b7Microprogrammed Control Unit : Suggested by Wilkes in 1951, and introduced by IBM\non the S/360 line in 1964. Microprogramming eases the task of designing and implementing\nthe control unit and provide support for the family concept.\n\uf0b7Cache Memory: First introduced commercially on IBM S/360 Model 85 in 1968. The\ninsertion of this element into the memory hierarchy dramatically improves performance.\n\uf0b7Pipelining: A means of introducing parallelism into the essentially sequential nature of a\nmachine instruction program. Examples are instruction pipelining and vector processing.\n\uf0b7Multiple  Processor :  This  category  covers  a  number  of  different  organizations  and\nobjectives.\nWhen it appeared, RISC architecture was a dramatic departure from the historical trend in processor\narchitecture. An analysis of the RISC architecture brings into focus many of the important issues in\ncomputer organization and architecture.\nAlthough RISC architectures have been defined and designed in a variety of ways by different\ngroups, the key elements shared by most designs are these:\n\uf0b7A large number of  general-purpose registers, and/or the use of compiler technology to\noptimize register usage.\n\uf0b7A limited and simple instruction set.\n\uf0b7An emphasis on optimizing the instruction pipeline.\nTable 1 compares several RISC and non-RISC systems. We begin this chapter with a brief survey of\nsome results on instruction sets, and then examine each of the three topics just listed. This is\nfollowed by a description of two of the best-documented RISC designs"
  },
  {
    "source": "Lecture 6 - RISC.pdf",
    "text": "Instruction Execution Characteristics\nOne of the most visual forms of evolution associated with computers is that of programming\nlanguages.  Even  more  powerful  and  complex  high  level  programming  languages  has  been\ndeveloped by the researcher and industry people.\nThe development of powerful high level programming languages give rise to another problem\nknown as the semantic gap, the difference between the operations provided in HLLs and those\nprovided in computer architecture.\nThe computer designers intend to reduce this gap and include large instruction set, more addressing\nmode and various HLL statements implemented in hardware. As a result the instruction set becomes\ncomplex. Such complex instruction sets are intended to-\n\uf0b7Ease the task of the compiler writer.\n\uf0b7Improve execution efficiency, because complex sequences of operations can be implemented\nin microcode.\n\uf0b7Provide support for even more complex and sophisticated HLLs.\nTo reduce the gap between HLL and the instruction set of computer architecture, the system\nbecomes more and more complex and the resulted system is termed as Complex Instruction Set\nComputer (CISC).\nA number of studies have been done over the years to determine the characteristics and patterns of\nexecution  of  machine  instructions  generated  from  HLL  programs.  The  instruction  execution\ncharacteristics involves the following aspects of computation:\n\uf0b7Operation Performed : These determine the functions to be performed by the processor and\nits interaction with memory.\n\uf0b7Operand Used: The types of operands and the frequency of their use determine the memory\norganization for storing them and the addressing modes for accessing them.\n\uf0b7Execution sequencing : This determines the control and pipeline organization.\nThese results are instructive to the machine instruction set designers, indicating which type of\nstatements occur most often and therefore should be supported in an \u201coptimal\u201d fashion.\nFrom these studies one can observe that though a complex and sophisticated instruction set is\navailable in a machine architecture, common programmer may not use those instructions frequently.\nOperands :\nResearches also studied the dynamic frequency of occurrence of classes of variables. The results\nshowed  that  majority  of  references  are  single  scalar  variables.  In  addition  references  to\narrays/structures required a previous reference to their index or pointer, which again is usually a\nlocal scalar. Thus there is a predominance of references to scalars, and these are highly localized.\nIt is also observed that operation on local variables is performed frequently and it requires a fast\naccessing  of  these  operands.  So,  it  suggests  that  a  prime  candidate  for  optimization  is  the\nmechanism for storing and accessing local scalar variables.\nProcedure Call :\nThe procedure calls and returns are an important aspects of HLL programs. Due to the concept of\nmodular and functional programming, the call/return statements are becoming a predominate factor\nin HLL program.\nIt is known fact that call/return is a most time consuming and expensive statements. Because during\ncall we have to restore the current state of the program which includes the contents of local\nvariables that are present in general purpose registers. During return, we have to restore the original\nstate of the program from where we start the procedure call.\nThus, it will be profitable to consider ways of implementing these operations efficiently. Two\naspects are significant, the number of parameters and variables that a procedure deals with, and the\ndepth of nesting.\nImplications :\nA number of groups have looked at these results and have concluded that the attempt to make the\ninstruction set architecture close to HLL is not the most effective design strategy. Generalizing from\nthe work of a number of researchers three element emerge in the computer architecture.\n\uf0b7First, use a large number of registers or use a compiler to optimize register usage. This is\nintended to optimize operand referencing.\n\uf0b7Second, careful attention needs to be paid to the design of instruction pipelines. Because of\nthe high proportion of conditional branch and procedure call instructions, a straight forward\ninstruction  pipeline  will  be  inefficient.  This  manifests  itself  as  a  high  proportion  of\ninstructions that are prefetched but never executed.\n\uf0b7Third, a simplified (reduced) instruction set is indicated. It is observed that there is no point\nto design a complex instruction set which will lead to a complex architecture. Due to the\nfact, a most interesting and important processor architecture evolves which is termed as\nReduced Instruction Set Computer (RISC) architecture.\nAlthough RISC system have been defined and designed in a variety of ways by different groups, the\nkey element shared by most design are these:\n\uf0b7A large number of general purpose registers, or the use of compiler technology to optimize\nregister usage.\n\uf0b7A limited and simple instruction set.\n\uf0b7An emphasis on optimizing the instruction pipelin\nAn analysis of the RSIC architecture begins into focus many of the important issues in computer\norganization and architecture. The table in the next page compares several RISC and non-RISC\nsystem. . . . . .\nCharacteristics of Reduced Instruction Set Architecture :\nAlthough a variety of different approaches to reduce Instruction set architecture have been taken,\ncertain characteristics are common to all of them:\n1.One instruction per cycle.\n2.Register\u2013to\u2013register operations.\n3.Simple addressing modes.\n4.Simple instruction formats.\n1. One machine instruction per machine cycle :\nA machine cycle is defined to be the time it takes to fetch two operands from registers, perform an\nALU operation, and store the result in a register.\nWith simple, one-cycle instructions there is little or no need of microcode, the machine instructions\ncan  be  hardwired.  Hardware  implementation  of  control  unit  executes  faster  than  the\nmicroprogrammed control, because it is not necessary to access a microprogram control store\nduring instruction execution.\n2. Register \u2013to\u2013 register operations\nWith register\u2013to\u2013register operation, a simple LOAD and STORE operation is required to access the\nmemory, because most of the operation are register\u2013to-register. Generally we do not have memory\u2013\nto\u2013memory and mixed register/memory operation.\n3. Simple Addressing Modes\nAlmost all RISC instructions use simple register addressing. For memory access only, we may\ninclude some other addressing, such as displacement and PC-relative. Once the data are fetched\ninside the CPU, all instruction can be performed with simple register addressing.\n4. Simple Instruction Format\nGenerally in most of the RISC machine, only one or few formats are used. Instruction length is\nfixed and aligned on word boundaries. Field locations, especially the opcode, are fixed.\nWith  fixed  fields,  opcode  decoding  and  register  operand  accessing  can  occur  simultaneously.\nSimplified formats simplify the control unit.\nThe use of a large register file:\nThe reason that register storage is indicated is that it is the fastest available storage device, faster\nthan both main memory and cache. The register file will allow  the most frequently accessed\noperands to be kept in registers and to minimize register-memory operations.\nTwo basic approaches are possible, one based on software and the other on hardware.\nThe software approach is to rely on the compiler to maximize register usage. The compiler will\nattempt to allocate registers to those variables that will be used the most in a given time period. This\napproach requires the use of sophisticated program-analysis algorithms.\nThe hardware approach is simply to use more registers so that more variables can be held in\nregisters for longer periods of time. Here we will discuss the hardware approach.\nFor fast execution of instructions, it is desirable of quick access to operands.\nThere is large proportion of assignment statements in HLL programs, and many of these are of the\nsimple form A\u2190B. Also there are significant number of operand accesses per HLL Statement.\nAlso it is observed that most of the accesses are local scalars. To get a fast response, we must have\nan easy excess to these local scalars, and so the use of register storage is suggested. Since registers\nare the fastest available storage devices, faster than both main memory and cache, so the uses of\nregisters are preferable. The register file is physically small, and on the same chip as the ALU and\nControl Unit. A strategy is needed that will allow the most frequently accessed operands to be kept\nin registers and to minimize register-memory operations.\nTwo basic approaches are possible, one is based on software and the other on hardware.\n\uf0b7The software approach is to rely on the compiler to maximize register uses. The compiler\nwill attempt to allocate registers to those variables that will be used the most in a given time\nperiod.\n\uf0b7The hardware approach is simply to use more registers so that more variables can be held in\nregisters for longer period of time.\nRegister Window :\nThe use of a large set of registers should decrease the need to access memory. The design task is to\norganize the registers in such a way that this goal is realized.\nDue to the use of the concept of modular programming, the present day programs are dominated by\ncall/return statements. There are some local variables present in each function or procedure.\n1.On every call, local variables must be saved from the registers into memory, so that the\nregisters can be reused by the called program. Furthermore, the parameters must be passed.\n2.On return, the variables of the parent program must be restored (loaded back into registers)\nand results must be passed back to the parent program.\n3.There are also some global variables which are used by the module or procedure.\nThus the variables that are used in a program can be categorized as follows :\n\uf0b7Global variables : which is visible to all the procedures.\n\uf0b7Local variables : which is local to a procedure and it can be accessed inside the procedure\nonly.\n\uf0b7Passed parameters : which are passed to a subroutine from the calling program. So, these are\nvisible to both called and calling program.\n\uf0b7Returned  variable  :  variable  to  transfer  the  results  from  called  program  to  the  calling\nprogram. These are also visible to both called and calling program.\nWhy CISC\nCISC has richer instruction sets, which include a larger number of instructions and more complex\ninstructions. Two principal reasons have motivated this trend: a desire to simplify compilers and a\ndesire to improve performance.\nThe first of the reasons cited, compiler simplification, seems obvious. The task of the compiler\nwriter is to generate a sequence of machine instructions for each HLL statement. If there are\nmachine instructions that resemble HLL statements, this task is simplified.\nThis reasoning has been disputed by the RISC researchers. They have found that complex machine\ninstructions are often hard to exploit because the compiler must find those cases that exactly fit the\nconstruct. The task of optimizing the generated code to minimize code size, reduce instruction\nexecution count, and enhance pipelining is much more difficult with a complex instruction set.\nThe other major reason cited is the expectation that a CISC will yield smaller, faster programs. Let\nus examine both aspects of this assertion: that program will be smaller and that they will execute\nfaster. There are two advantages to smaller programs. \n\uf0b7First, because the program takes up less memory, there is a savings in that resource. \n\uf0b7Second, in a paging environment, smaller programs occupy fewer pages, reducing page\nfaults.\nThe problem with this line of reasoning is that it is far from certain that a CISC program will be\nsmaller than a corresponding RISC program. Thus it is far from clear that a trend to increasingly\ncomplex instruction sets is appropriate. This has led a number of groups to pursue the opposite path.\nCISC versus RISC Characteristics\nAfter the initial enthusiasm for RISC machines, there has been a growing realization that (1) RISC\ndesigns may benefit from the inclusion of some CISC features and that (2) CISC designs may\nbenefit from the inclusion of some RISC features. The result is that the more recent RISC designs,\nnotably the PowerPC, are no longer \u201cpure\u201d RISC and the more recent CISC designs, notably the\nPentium II and later Pentium models, do incorporate some RISC characteristics.\nFor purposes of this comparison, the following are considered typical of a classic RISC:\n1.A single instruction size.\n2.That size is typically 4 bytes.\n3.A small number of data addressing modes,typically less than five.This parameter is difficult\nto pin down. In the table, register and literal modes are not counted and different formats\nwith different offset sizes are counted separately.\n4.No indirect addressing that requires you to make one memory access to get the address of\nanother operand in memory.\n5.No operations  that combine load/store with arithmetic (e.g., add from memory, add to\nmemory).\n6.No more than one memory-addressed operand per instruction.\n7.Does not support arbitrary alignment of data for load/store operations.\n8.Maximum number of uses of the memory management unit (MMU) for a data address in an\ninstruction.\n9.Number of bits for integer register specifier equal to five or more. This means that at least\n32 integer registers can be explicitly referenced at a time.\n10.Number of bits for floating-point register specifier equal to four or more.This means that at\nleast 16 floating-point registers can be explicitly referenced at a time. \nItems 1 through 3 are an indication of instruction decode complexity. Items 4 through 8 suggest the\nease or difficulty of pipelining, especially in the presence of virtual memory requirements. Items 9\nand 10 are related to the ability to take good advantage of compilers.\nIn the table, the first eight processors are clearly RISC architectures, the next five are clearly CISC,\nand  the  last  two  are  processors  often  thought  of  as  RISC  that  in  fact  have  many  CISC\ncharacteristics."
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "Display and Visualization \nSystems\nLecture SIX\n2Use video data format \nto describe display and \nvisualization systemsa)Identify display and \nvisualization systems\nb)Describe basic parts of a \ndisplay and visualization \nsystem\nc)Demonstrate the operation of \nvarious display and \nvisualization systems  \nd)Describe various applications \nof display and visualization \nsystemsVideo data format is \ncorrectly used  to \ndescribe display and \nvisualization systems\n\u2022Data visualization is the graphical representation of information and data. \n\u2022By using visual elements like charts , graphs , and maps , data visualization \ntools provide an accessible way to see and understand trends, outliers, and \npatterns in data. \n\u2022Additionally , it provides an excellent way to present data to non -technical \naudiences without confusion.\n3Data Visualization \n\u2022The term is often used interchangeably with others, including information \ngraphics , information visualization and statistical graphics .\n\u2022Data visualization provides a quick and effective way to communicate \ninformation in a universal manner using visual information .\n\u2022It is the practice of translating information into a visual context, such as a map \nor graph, to make data easier for the human brain to understand and pull \ninsights from . \n\u2022The main goal of data visualization is to make it easier to identify patterns , \ntrends and outliers in large data sets . \n4Data Visualization \u2026 \n\u2022Data visualization is an element of the broader Data Presentation Architecture \n(DPA) discipline, which aims to identify , locate , manipulate , format and deliver \ndata in the most efficient way po ssible.\n\u2022Data visualization is important for almost every career. \n\u2022It can be used by teachers to display student test results, by computer \nscientists exploring advancements in artificial intelligence (AI) or by executives \nlooking to share information with stakeholders. \n\u2022It also plays an important role in big data projects. \n\u2022As businesses accumulated massive collections of data during the early years \nof the big data trend , they needed a way to get an overview of their data \nquickly and easily. \n5Data Visualization \u2026 \n\u2022Examples of data visualization\n\u2022The most common visualization technique used including the following:\n\u2022infographics\n\u2022bubble clouds\n\u2022bullet graphs\n\u2022heat maps\n\u2022fever charts\n\u2022time series charts , etc.\n6Data Visualization \u2026 \n\u2022An infographic is defined as a visualization of data that tries to convey \ncomplex information to an audience in a manner than can be quickly \nconsumed and easily understood.\n\u2022An infographic is a collection of imagery , data visualizations like pie charts and \nbar graphs , and minimal text that gives an easy -to-understand overview of a \ntopic.\n7Data Visualization \u2026 \n\u2022Bubble cloud charts are useful for illustrating the relationship between \nelements in your data set and the set as a whole. \n\u2022For example, you can visualize data collected from different cities , and \nrepresent each city as a bubble whose size is proportional to the value for \nthat city. \n8Mislabeled State Manufacturing Plant\n__________ _____ ___________________ \n58 \"NJ\" \"Plant A\" \n115 \"NY\" \"Plant A\" \n81 \"MA\" \"Plant A\" \n252 \"OH\" \"Plant A\" \n180 \"NH\" \"Plant A\" \n124 \"ME\" \"Plant A\" \n40 \"CT\" \"Plant A\" \n80 \"PA\" \"Plant B\" \n50 \"RI\" \"Plant B\" \n20 \"VT\" \"Plant B\" Data Visualization \u2026"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "9\n\u2022The difference being the size of the bubble \nalso determines a value . \nData Visualization \u2026 \n\u2022A bullet graph , or a bullet chart , is a variation of a bar chart, typically consisting of a \nprimary bar layered on top of a secondary stack of less -prominent bars. \n\u2022Bullet graphs are best used for making comparisons , such as showing progress \nagainst a target or series of thresholds. \n10\nData Visualization \u2026 \n\u2022A heat map is a visual \nrepresentation of data, which \ndisplays numeric values as color \nschemes .\n11\nData Visualization \u2026 \n12\n\u2022A fever chart is a graphical \nrepresentation of changing data over \ntime.\n\u2022When the values of variables are \nrecorded and viewed over a long \nperiod of time, it is difficult to derive \npatterns or trends from plain data. \n\u2022However , when the same data is \nrepresented in a fever chart, it \nbecomes easier to spot trends or \npatterns .Data Visualization \u2026 \n13\nData Visualization \u2026 \n\u2022Time series charts present a series of data \npoints collected over a specified reporting \nperiod .\n14\u2022IPCC_A1FI_CO2is the climate change \nexperiment where the greenhouse gas \nquantities increase gradually as the \nmodel runs .\u2022Modern_PredictedSST is a simulation \nthat demonstrates what future climate \nwould be like with no increase in \ngreenhouse gases or any other \nvariables, such as solar luminosity. \n\u2022This is the baseline for any climate \nchange experiments .\nData Visualization \u2026 \n\u2022Some other popular techniques are as follows:\n\u2022Line charts. Line charts display how variables can change over time.\n\u2022Area charts. This displays multiple values in a time series or a sequence of data \ncollected at consecutive, equally spaced points in time.\n\u2022Scatter plots. This technique displays the relationship between two variables.\n\u2022Treemaps . This method shows hierarchical data in a nested format. The size of \nthe rectangles used for each category is proportional to its percentage of the \nwhole. Treemaps are best used when multiple categories are present , and the \ngoal is to compare different parts of a whole.\n15Data Visualization \u2026 \n16VISUALIZATION SYSTEMS\n\u2022System visualization is the process of mapping the flow and/or function of a \nsystem or data flow . \n\u2022Visualizations are beneficial to help a team more quickly diagnose issues, \ncommunicate across departments, and efficiently build or update a system.\n\u2022A display is a core ingredient in a human -machine interface (HMI), the \ndynamic device through which information is entered and presented, as text \nor images. \n\u2022Not too many years ago, the CRT (cathode ray tube) monitor large , heavy, \nand bulky was the common display device. \nDISPLAYS AND DATA TYPES\n\u2022Two main technologies, liquid crystals and organic light -emitting diodes , \ncurrently dominate the market for visual displays. \n\u2022An older technology, the cathode ray tube, has all but vanished from the \nscene, and plasma monitors also see use in some applications .\n17\u2022A display is an electronic device whose main purpose is an interface to \nshow the reading (information) as the result of a certain process of a \ncomputer or electronic circuit . \n\u2022The information (data types) could be a text, picture (images) , or video\nwhich is a sequence of pictures.\nCommon components\n\u2022A display consists of three primary components: \n1.display assembly, \n2.controller , and \n3.backlight .\n18DISPLAYS AND DATA TYPES \u2026\n\u2022In liquid crystal technology, as an \nexample,  display assembly is a \nliquid crystal structure, sandwiched \nbetween layers of glass . \n\u2022Individual display elements are \ncalled pixels , each one \naddressable in a row -and-column \nformat. \n19DISPLAYS AND DATA TYPES \u2026\n\u2022In simple terms, pixel response provides flexibility the amount depending \non display type to create graphics and text in combination, in monochrome \nor color.\n20\n\u2022Active matrix driving technology \nintegrating red-green -blue (RGB) \nmicroscale light emitting diodes (micro \nLEDs) and a microscale integrated circuit \n(micro IC) in each pixel for the Crystal \nLED display system. DISPLAYS AND DATA TYPES \u2026\n\u2022A controller , either built -in to the \nassembly or external, guides the \noperation of the display assembly . \n\u2022In most cases, the controller is \nintegrated into the display. \n\u2022When external to the device, it may \ntake the form of a separate card or \nbe a built -in function of a Single \nBoard Computer (SBC). \n21DISPLAYS AND DATA TYPES \u2026"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "\u2022A backlight is a form of illumination used in \nliquid crystal displays (LCDs). \n\u2022As LCDs do not produce light by themselves \nunlike , for example, cathode ray tube (CRT), \nplasma (PDP) or OLED displays.\n\u2022They need illumination (ambient light or a \nspecial light source) to produce a visible image. \n\u2022Backlights illuminate the LCD from the side or \nback of the display panel , unlike front-lights , \nwhich are placed in front of the LCD. \n22DISPLAYS AND DATA TYPES \u2026"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "\u2022Backlights are used in small \ndisplays to increase readability \nin low light conditions such as in \nwristwatches, and are used in \nsmart phones , computer \ndisplays and LCD televisions to \nproduce light in a manner similar \nto a CRT display.\n23DISPLAYS AND DATA TYPES \u2026"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "THE DISPLAY TYPES\nCathode Ray Tubes (CRT ):\n\u2022This type of display has a big size, heavy, and bulky \ndimension. \n\u2022Not the first choice in the 21stera. \n24"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "25LED Displays:\nLight emitting diode (LED) is the most \ncommonly used device for displaying the \nstatus of microcontroller pins ."
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "7-Segment LED Display:\n\u20227-Segment LED display can be used for displaying digits and \nfew characters . \n\u2022A seven segment display consists of 7 LEDs arranged in the \nform of Square \u20188\u2019 and a single LED as dot character. \n\u2022Different characters can be displayed by selecting the \nrequired LED segments. \n26"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "Dot Matrix LED Display:\n\u2022Dot matrix LED display contains the \ngroup of LEDs as a two dimensional \narray . \n\u2022They can display different types of \ncharacters or a group of characters . \n\u2022Dot matrix display is manufactured in \nvarious dimensions. \n\u2022By using this dot matrix display we can \nreduce the number of pins required for \ncontrolling all the LEDs.\n27"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "Organic Light -Emitting Diode (OLED)\n\u2022Although the OLED screen is lightly similar to \nLCD technology, OLEDs require no backlight ; \nthey produce their own light . \n\u2022Because of this advantage, OLED displays can \nbe much thinner than an LCD equivalent . \n\u2022And because a backlight consumes significant \namounts of energy, OLEDs help improve battery \nlife in mobile devices . \n\u2022Although the image quality of OLED displays is \nvery good, their working lifetime is currently not \nas good as LCDs .\n28"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "Liquid Crystal Displays (LCD)\n\u2022Liquid crystals are liquid materials that have \nsome of the optical properties of crystals . \n\u2022A display made of liquid crystals acts like an \narray of tiny shutters that transmit or block \nlight. \n\u2022A bright light source called a backlight , \nsituated behind the LCD screen , shines \nthrough the LCD, creating thousands of tiny \ndots of red, blue and green that form a color \nimage. \n\u2022Because the backlight is sealed inside the \ndisplay, you normally never see it directly , only \nits light filtered through the LCD panel.\n29"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "\u2022The LCD is much more informative output device than \na single LED. \n\u2022The LCD is a display that can easily show characters \non its screen . \n\u2022Some LCDs are specially designed for specific \napplications to display graphic images such as 16\u00d72 \nLCD (HD44780) module is commonly used. \n30\n\u2022The graphical LCDs can be used to \ndisplay customized characters and \nimages . \n\u2022The graphical LCDs find use in many \napplications like video games , mobile \nphones , and lifts as display units. \n\u2022The most commonly used GLCD is \nJHD12864E. \n\u2022This LCD has a display format of 128 \u00d764 \ndots. \n31"
  },
  {
    "source": "LECTURE 6.pdf",
    "text": "\u2022The most common add -on to an LCD is a touch -panel . \n\u2022In this four-wire resistive technology example, a glass panel is uniformly \ncoated with electrically conductive and resistive layers . \n\u2022A polyester cover sheet is suspended over the top of the glass and \nseparated from it by small, transparent insulating separators . \n\u2022During operation, an electrical current moves through the touchscreen . \n\u2022When activated by a touch, the conductive coating makes contact with the \ncoating on the glass and a touch point is registered.\n32\n\u2022Images are brought to the display surface by a backlight , typically CCFL (cold \ncathode fluorescent lamps) powered by a high-voltage power supply called an \ninverter . \n33\nAn inverter\n\u2022As light -emitting diode (LED) and organic light -emitting diode (OLED) \ntechnologies advance in power consumption and lumen output, they are \nincreasingly popular. \n\u2022A logic -powered light source, the LED requires no inverter . \n\u2022While CCFL is still the backlight of choice in large displays and laptops, LEDs \nare found in smaller displays (5-in. diagonal and less), cellphones, and a \nvariety of other devices, such as intelligent thermostats. \n\u2022OLED displays embody low voltage, 3x self -illumination, and high brightness. \n\u2022With improving life, future OLED use will escalate.\n34\nPlasma \n\u2022A plasma display panel (PDP) is a type of flat panel display that \nuses small cells containing plasma: ionized gas that responds to \nelectric fields. \n\u2022Plasma televisions were the first large (over 32 inches diagonal) \nflat panel displays to be released to the public.\n\u2022A plasma display screen consists of tiny gas capsules arranged in \na grid; when stimulated by electricity, the gas glows much in the \nsame manner as a neon sign. \n35\n\u2022Plasma TVs are actually very similar to CRTs . \n\u2022Instead of having a beam that scans, a plasma produces light from its \npixels when an electric charge is applied to a cell containing a noble gas , \nor \"plasma\". \n\u2022These plasma chambers are sealed units, the gas will never escape.\n36Plasma \u2026\n\u2022Plasma displays have two glass \nplates , containing electrodes, sealed \nto form an envelope filled with a \nneon and xenon gas mixture . \n\u2022A gas discharge plasma is created \nby applying an electric field between \nthe electrodes . \n\u2022The plasma generates ultraviolet \nlight which in turn excites the \nphosphor coating inside the glass \nenvelope. \n37\nPlasma \u2026\n\u2022The phosphor emits a single color of visible light . \n\u2022Each pixel consists of three sub -pixels, one each of red, green and blue . \n\u2022By combining these primary colors at varying intensities, all colors can be \nformed.\n\u2022Some aspects of image quality, such as the darkness of blacks and the \nvividness of colors, can be better in plasma screens than LCDs. \n\u2022However , LCDs are more energy -efficient than plasmas ; due to battery life \nconcerns, virtually all laptop computers have LCD screens and not plasma \ntechnology. \n\u2022Most plasma screens currently sold tend to be in the 40 -inch to 60 -inch size \nrange where image quality helps justify the greater energy consumption.\n38Plasma \u2026\nThe Display Specifications\n1. Resolution\n\u2022It determines the total number of pixels on the screen . \n\u2022The most common is 1920 \u00d71080 that is Full-HD or 1080p. \n\u2022On its development, more resolution is introduced such as 2K, \n1440p, 4K, even 5K and 8Kfor professionals .\n2. Size\n\u2022This is often explained in inch size , e.g. 22\u201d, 55\u201d etc. \n\u2022The size is measured diagonally just for your information. \n\u2022To keep the image quality sharp and clear , a bigger size display \nmay need a bigger resolution .\n39\n40STANDARD DESCRIPTION RESOLUTION\nCGA Color Graphics Adapter 640 x 200  ( monochrome )\n320 x 200 ( 4-color)\nEGA Enhanced Graphics Adapter 640 x 350 (4 -color)\nVGA Video Graphics Array 640 x 480\nXGA Extended Graphics Array 1024 x 768\nSXGA Super Extended Graphics 1280 x 1024\nUXGA Ultra Extended Graphics 1600 x 1200\nWXGA Wide Extended Graphics 1366 x 768\nWSXGA Wide Super Extended Graphics 1680 x 1050\nWUXGA Wide Ultra Extended Graphics 1920 x 1200\n3. Aspect Ratio\n\u2022This is to represent the width to the height. \n\u2022It can be measured from the resolution too. \n\u2022The common aspect ratios are:\n1.Wide \u201316:9\n2.Ultra -Wide \u201321:9\n3.Super -Wide \u201332:9\n414. Refresh Rate\n\u2022It represents how often a display can update per second (Hertz or Hz). \n\u2022The common refresh rates are 60Hz, 75Hz, 90Hz, 120Hz -Gaming, and 144Hz -\nGaming.\n5. Viewing Angles\n\u2022It is measured by two numbers such as 160/120 where 160 is the horizontal \nviewing angle and 120 is the vertical viewing angle . \n\u2022As long as you are in 160 out of 180 degrees horizontally and 120 out 180 \ndegrees, you will still get a clear view.\n426. Brightness\n\u2022The units used are nits or cd/m2 . \n\u2022The adequate standard is about 200-300 nits ."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 1 - Pipelining\nby \nE. Kondela \nIntroduction\nIt is observed that organization enhancements to the CPU can improve performance. We have\nalready seen that use of multiple registers rather than a single a accumulator, and use of cache\nmemory improves the performance considerably. Another organizational approach, which is quite\ncommon, is instruction pipelining.\nPipelining is a particularly effective way of organizing parallel activity in a computer system. The\nbasic idea is very simple. It is frequently encountered in manufacturing plants, where pipelining is\ncommonly known as an assembly line operation.\nBy laying the production process out in an assembly line, product at various stages can be worked\non simultaneously. This process is also referred to as pipelining, because, as in a pipeline, new\ninputs are accepted at one end before previously accepted inputs appear as outputs at the other end.\nTo apply the concept of instruction execution in pipeline, it is required to break the instruction in\ndifferent task. Each task will be executed in different processing elements of the CPU.\nAs we know that there are two distinct phases of instruction execution: one is instruction fetch and\nthe other one is instruction execution. Therefore, the processor executes a program by fetching and\nexecuting instructions, one after another.\nLet Fi and Ei refer to the fetch and execute steps for instruction Ii. Execution of a program consists\nof a sequence of fetch and execute steps is shown in the figure on the next slide.\nNow consider a CPU that has two separate hardware units, one for fetching instructions and another for executing\nthem.\nThe instruction fetch by the fetch unit is stored in an intermediate storage buffer B1. The results of execution are\nstored in the destination location specified by the instruction.\nFor simplicity it is assumed that fetch and execute steps of any instruction can be completed in one clock cycle.\nThe operation of the computer proceeds as follows:\n\uf0b7In the first clock cycle, the fetch unit fetches an instruction (instruction I1, step F1) and stored it in buffer\nB1 at the end of the clock cycle.\n\uf0b7In the second clock cycle, the instruction fetch unit proceeds with the fetch operation for instruction I2\n(step F2).\n\uf0b7Meanwhile, the execution unit performs the operation specified by instruction  I1which is already fetched\nand available in the buffer B1 (step E1).\n\uf0b7By the end of the second clock cycle, the execution of the instruction I1 is completed and instruction  I2 is\navailable.\n\uf0b7Instruction I2 is stored in buffer B1 replacing I1 which is no longer needed.\n\uf0b7Step  E2 is performed by the execution unit during the third clock cycle, while instruction  I3 is being\nfetched by the fetch unit."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "\uf0b7Both the fetch and execute units are kept busy all the time and one instruction is completed after each\nclock cycle except the first clock cycle.\n\uf0b7If a long sequence of instructions is executed, the completion rate of instruction execution will be twice\nthat achievable by the sequential operation with only one unit that performs both fetch and execute.\nBasic idea of instruction pipelining with hardware organization is shown in the figure on the next slide.\nThe processing of an instruction need not be divided into only two steps. To gain further speed up, the pipeline\nmust have more stages.\nLet us consider the following decomposition of the instruction execution:\n\uf0b7Fetch Instruction (FI): Read the next expected instruction into a buffer.\n\uf0b7Decode Instruction ((DI): Determine the opcode and the operand specifiers.\n\uf0b7Calculate Operand (CO): calculate the effective address of each source operand.\n\uf0b7Fetch Operands(FO): Fetch each operand from memory.\n\uf0b7Execute Instruction (EI): Perform the indicated operation.\n\uf0b7Write Operand(WO): Store the result in memory.\nThere will be six different stages for these six subtasks. For the sake of simplicity, let us assume the equal duration\nto perform all the subtasks. It the six stages are not of equal duration, there will be some waiting involved at\nvarious pipeline stages.\nThe timing diagram for the execution of instruction in pipeline fashion is shown in the figure on the next slide."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "From this timing diagram it is clear that the total execution time of 8 instructions in this 6 stages\npipeline is 13-time unit. The first instruction gets completed after 6 time unit, and there after in each\ntime unit it completes one instruction. Without pipeline, the total time required to complete 8\ninstructions would have been 48 (6 X 8) time unit. Therefore, there is a speed up in pipeline\nprocessing and the speed up is related to the number of stages."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "i.e. We have a k fold speed up, the speed up factor is a function of the number of stages in the\ninstruction pipeline.\nThough, it has been seen that the speed up is proportional to number of stages in the pipeline, but in\npractice the\nspeed up is less due to some practical reason. The factors that affect the pipeline performance is\ndiscussed next.\nEffect of Intermediate storage buffer:\nConsider a pipeline processor, which process each instruction in four steps;\nF: Fetch, Read the instruction from the memory\nD: Decode, decode the instruction and fetch the source operand (S)\nO: Operate, perform the operation\nW: Write, store the result in the destination location.\nThe hardware organization of this four-stage pipeline processor is shown next."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "In the preceding section we have seen that the speed up of pipeline processor is related to number of\nstages in the pipeline, i.e, the greater the number of stages in the pipeline, the faster the execution\nrate. But the organization of the stages of a pipeline is a complex task and if affects the performance\nof the pipeline.\nThe problem related to more number of stages:\nAt each stage of the pipeline, there is some overhead involved in moving data from buffer to buffer\nand  in  performing  various  preparation  and  delivery  functions.  This  overhead  can  appreciably\nlengthen the total execution time of a single instruction.\nThe amount of control logic required to handle memory and register dependencies and to optimize\nthe use of the pipeline increases enormously with the number of stages.\nApart from hardware organization, there are some other reasons which may effect the performance\nof the pipeline. \n(A) Unequal time requirement to complete a subtask:\nConsider the four-stage pipeline with processing step Fetch, Decode, Operand and write.\nThe stage-3 of the pipeline is responsible for arithmetic and logic operation, and in general one\nclock cycle is assigned for this task\nAlthough this may be sufficient for most operations, but some operations like divide may require\nmore time to complete. Following figure shows the effect of an operation that takes more than one\nclock cycle to complete an operation in operate stage."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "The operate stage for instruction I2 takes 3 clock cycle to perform the specified operation. Clock\ncycle 4 to 6 required to perform this operation and so write stage is doing nothing during the clock\ncycle 5 and 6, because no data is available to write.\nMeanwhile, the information in buffer B2 must remain intake until the operate stage has completed\nits operation. This means that stage 2 and stage 1 are blocked from accepting new instructions\nbecause the information in B1 cannot be overwritten by a new fetch instruction.\nThe contents of B1, B2 and B3 must always change at the same clock edge.\nDue to that reason, pipeline operation is said to have been stalled for two clock cycle. Normal\npipeline operation resumes in clock cycle 7. Whenever the pipeline stalled, some degradation in\nperformance occurs.\nRole of cache memory:\nThe use of cache memory solves the memory access problem. Occasionally, a memory request\nresults in a cache miss. This causes the pipeline stage that issued the memory request to take much\nlonger time to complete its task and in this case the pipeline stalls. The effect of cache miss in\npipeline processing is shown in the figure."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "Function performed by each stage as a function of time\nIn this example, instruction  I1  is fetched from the cache in cycle 1 and its execution proceeds\nnormally. The fetch operation for instruction I2 which starts in cycle 2, results in a cache miss. The\ninstruction fetch unit must now suspend any further fetch requests and wait for I2 to arrive.\nWe assume that instruction I2 is received and loaded into buffer B1 at the end of cycle 5, It appears\nthat cache memory used here is four time faster than the main memory.\nThe pipeline resumes its normal operation at that point and it will remain in normal operation mode\nfor some times, because a cache miss generally transfer a block from main memory to cache.\nFrom the figure, it is clear that Decode unit, Operate unit and Write unit remain idle for three clock\ncycle. Such idle periods are sometimes referred to as bubbles in the pipeline. Once created as a\nresult of a delay in one of the pipeline stages, a bubble moves downstream until it reaches the last\nunit. A pipeline can not stall as long as the instructions and data being accessed reside in the cache.\nThis is facilitated by providing separate on chip instruction and data caches.\nDependency Constraints:\nConsider the following program that contains two instructions, I1 followed by I2\nI1 : A\u2190 A + 5\nI2 : B\u2190 3 * A"
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "When this program is executed in a pipeline, the execution of I2 can begin before the execution of\nI1 completes.\nThe pipeline execution is shown below.\nIn clock cycle 3, the specific operation of instruction I1 i.e. addition takes place and at that time\nonly the new updated value of A is available. But in the clock cycle 3, the instruction I2 is fetching\nthe operand that is required for the operation of  I2. Since in clock cycle 3 only, operation of\ninstruction I1 is taking place, so the instruction will get operation of the old value of A , it will not\nget the updated value of A , and will produce a wrong result. Consider that the initial value of A is 4.\nThe proper execution will produce the result as\nB=27\nI1: A\u2190 A + 5 = 4 + 5 = 9\nI2: B\u2190 3 x A = 3 x 9 = 27\nBut due to the pipeline action, we will get the result as\nI1: A\u2190 A + 5 = 4 + 5 = 9\nI2:B\u2190 3 x A = 3 x 4 = 12 \nDue to the data dependency, these two instructions can not be performed in parallel.\nTherefore, no two operations that depend on each other can be performed in parallel. For correct\nexecution, it is required to satisfy the following:\n\uf0b7The operation of the fetch stage must not depend on the operation performed during the\nsame clock cycle by the execution stage.\n\uf0b7The operation of fetching an instruction must be independent of the execution results of the\nprevious instruction.\n\uf0b7The dependency of data arises when the destination of one instruction is used as a source in\na subsequent instruction.\nBranching\nIn general when we are executing a program the next instruction to be executed is brought from the\nnext memory location. Therefore, in pipeline organization, we are fetching instructions one after\nanother.\nBut in case of conditional branch instruction, the address of the next instruction to be fetched\ndepends on the result of the execution of the instruction."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "Since the execution of next instruction depends on the previous branch instruction, sometimes it\nmay  be  required  to  invalidate  several  instruction  fetches.  Consider  the  following  instruction\nexecution sequence:\nIn this instruction sequence, consider that I3 is a conditional branch instruction.\nThe result of the instruction will be available at clock cycle 5. But by that time the fetch unit has\nalready fetched the instruction I4 and I5.\nIf the branch condition is false, then branch won't take place and the next instruction to be executed\nis I4 which is already fetched and available for execution.\nNow consider that when the condition is true, we have to execute the instruction I10 after clock\ncycle 5, it is known that branch condition is true and now instruction I 10 has to be executed.\nBut already the processor has fetched instruction I4 and I5 it is required to invalidate these two\nfetched instruction and the pipe line must be loaded with new destination instruction I 10.\nDue to this reason, the pipeline will stall for some time. The time lost due to branch instruction is\noften referred as branch penalty."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "The effect of branch takes place is shown in the figure in the previous slide. Due to the effect of\nbranch takes place, the instruction I4 and I5 which has already been fetched is not executed and new\ninstruction I 10 is fetched at clock cycle 6.\nThere is not effective output in clock cycle 7 and 8, and so the branch penalty is 2. The branch\npenalty depends on the number of stages in the pipeline. More numbers of stages results in more\nbranch penalty.\nDealing with Branches:\nOne of the major problems in designing an instruction pipe line is assuming a steady flow of\ninstructions to the initial stages of the pipeline. The primary problem is the conditional brancho\ninstruction until the instruction is actually executed, it is impossible to determine whether the\nbranch will be taken or not.\nA variety of approaches have been taken for dealing with conditional branches:\n\uf0b7Multiple streams\n\uf0b7Prefetch branch target\n\uf0b7Loop buffer\n\uf0b7Branch prediction\n\uf0b7Delayed branch\nMultiple streams\nA single pipeline suffers a penalty for a branch instruction because it must choose one of two\ninstructions to fetch next and sometimes it may make the wrong choice.\nA brute-force approach is to replicate the initial portions of the pipeline and allow the pipeline to\nfetch both instructions, making use of two streams.\nThere are two problems with this approach.\n\uf0b7With multiple pipelines there are contention delays for access to the registers and to memory\n\uf0b7Additional branch instructions may enter the pipeline (either stream) before the original\nbranch decision is resolved. Each such instruction needs as additional stream.\nPrefetch Branch target\nWhen a conditional branch is recognized, the target of the branch is prefetced, in addition to the\ninstruction following the branch. This target is then saved until the branch instruction is executed. If\nthe branch is taken, the target has already been prefetched,.\nLoop Buffer:\nA top buffer is a small, very high speed memory maintained by the instruction fetch stage of the\npipeline and containing the most recently fetched instructions, in sequence. If a branch is to be\ntaken, the hardware first cheeks whether the branch target is within the buffer. If so, the next\ninstruction is fetched from the buffer.\nThe loop buffer has three benefits:\n1. With the use of prefetching, the loop buffer will contain some instruction sequentially ahead of\nthe  current  instruction  fetch  address.  Thus,  instructions  fetched  in  sequence  will  be  available\nwithout the usual memory access time.\n2. If a branch occurs to a target just a few locations ahead of the address of the branch instruction,\nthe target will already be in the buffer. This is usual for the common occurrence of IF-THEN and\nIF-THEN-ELSE sequences.\n3. This strategy is particularly well suited for dealing with loops, or iterations; hence the name loop\nbuffer. If the loop buffer is large enough to contain all the instructions in a loop, then those\ninstructions need to be fetched from memory only once, for the first iteration. For subsequent\niterations, all the needed instructions are already in the buffer.\nThe loop buffer is similar in principle to a cache dedicated to instructions. The differences are that\nthe loop buffer only retains instructions in sequence and is much smaller in size and hence lower in\ncost.\nBranch Prediction :\nVarious techniques can be used to predict whether a branch will be taken or not. The most common\ntechniques are:\n\uf0b7Predict never taken\n\uf0b7Predict always taken\n\uf0b7Predict by opcode\n\uf0b7Taken/not taken switch\n\uf0b7Branch history table.\nThe first three approaches are static; they do not depend on the execution history upto the time of\nthe conditional branch instructions. The later two approaches are dynamic- they depend on the\nexecution history.\nPredict  never  taken  always  assumes  that  the  branch  will  not  be  taken  and  continue  to  fetch\ninstruction in sequence. Predict always taken assumes that the branch will be taken and always fetch\nthe branet target In these two approaches it is also possible to minimize the effect of a wrong\ndecision.\nIf the fetch of an instruction after the branch will cause a page fault or protection violation, the\nprocessor  halts  its  prefetching  until  it  is  sure  that  the  instruction  should  be  fetched.  Studies\nanalyzing program behaviour have shown that conditional branches are taken more than 50% of the\ntime, and so if the cost of prefetching from either path is the same, then always prefetching from the\nbranch target address should give better performance than always prefetching from the sequential\npath.\nHowever, in a paged machine, prefetching the branch target is more likely to cause a page fault than\nprefetching the next instruction in the sequence and so this performance penalty should be taken\ninto account.\nPredict by opcode approach makes the decision based on the opcade of the branch instruction. The\nprocessor assumes that the branch will be taken for certain branch opcodes and not for others.\nStudies reported in showed that success rate is greater than 75% with the strategy.\nDynamic branch strategies attempt to improve the accuracy of prediction by recording the history of\nconditional branch instructions in a program. Scheme to maintain the history information:\n\uf0b7One or more bits can be associated with each conditional branch instruction that reflect the\nrecent history of the instruction.\n\uf0b7These bits are referred to as a taken/not taken switch that directs the processor to make a\nparticular decision the next time the instruction is encountered.\n\uf0b7Generally these history bits are not associated with the instruction in main memory. It will\nunnecessarily increase the size of the instruction. With a single bit we can record whether\nthe last execution of this instruction resulted a branch or not.\n\uf0b7With only one bit of history, an error in prediction will occur twice for each use of the loop:\nonce for entering the loop. And once for exiting.\nIf two bits are used, they can be used to record the result of the last two instances of the execution\nof the associated instruction.\nThe history information is not kept in main memory, it can be kept in a temporary high speed\nmemory. One possibility is to associate these bits with any conditional branch instruction that is in a\ncache. When the instruction is replaced in the cache, its history is lost. Another possibility is to\nmaintain a small table for recently executed branch instructions with one or more bits in each entry.\nThe branch history table is a small cache memory associated with the instruction fetch stage of the\npipeline. Each entry in the table consists of three elements:\n\uf0b7The address of the branch instruction.\n\uf0b7Some member of history bits that record the state of use of that instruction.\n\uf0b7Information about the target instruction, it may be the address of the target instruction, or\nmay be the target instruction itself."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "Consider that the instruction Ij is a branch instruction. The processor begins fetching instruction\nIj+1 before it determine whether the current instruction, Ij , is a branch instruction.\nWhen execution of is completed and a branch must be made, the processor must discard the\ninstruction that was fetched and now fetch the instruction at the branch target.\nThe location following a branch instruction is called a branch delay slot. There may be more than\none branch delay slot, depending on the time it takes to execute a branch instruction.\nThe instructions in the delay slots are always fetched and at least partially executed before the\nbranch decision is made and the branch target address is computed.\nDelayed branching is a technique to minimize the penalty incurred as a result of conditional branch\ninstructions. The instructions in the delay slots are always fetched, so we can arrange the instruction\nin delay slots to be fully executed whether or not the branch is taken. The objective is to plane\nuseful instruction in these slots. If no useful instructions can be placed in the delay slots, these slots\nmust  be  filled  with  NOP (no  operation)  instructions.  While  feeling  up  the  delay  slots  with\ninstructions, it is required to maintain the original semantics of the program.\nFor example consider the following code segments\nHere register R2 is used as a counter to determine the number of times the contents of register R1\nare sifted left. Consider a processor with a two-stage pipeline and one delay slot. During the\nexecution phase of the instruction I3 the fetch unit will fetch the instruction I4. After evaluating the\nbranch condition only, it will be clear whether instruction I1 or I4 will be executed next.\nThe nature of the code segment says that it will remain in the top depending on the initial value of\nR2 and when it becomes zero, it will come out from the loop and execute the instruction I4. During\nthe loop execution, every time there is a wrong fetch of instruction I4. The code segment can be\nrecognized without disturbing the original meaning of the program."
  },
  {
    "source": "Lecture 7 - PIPELINING.pdf",
    "text": "In this case, the shift instruction is fetched while the branch instruction is being executed. After\nevaluating  the  branch  condition,  the  processor  fetches  the  instruction  at  LOOP or  at  NEXT,\ndepending on whether the branch condition is true or false, respectively.\nIn either case, it completes execution of the shift instruction. Logically the program is executed as if\nthe branch instruction was placed after the shift instruction. That is, branching takes place one\ninstruction  later  than  where  the  branch  instruction  appears  in  the  instruction  sequence  in  the\nmemory, hence the name \u201cdelayed branch\u201d ."
  },
  {
    "source": "Microprocessor.pdf",
    "text": "First Section a) What is the difference between ENIAC and stored-computer program? 1. ENIAC (Electronic Numerical Integrator and Computer): o Designed in the 1940s as one of the first electronic general-purpose computers. o Programmed through manual rewiring and plugboards. o It did not have the concept of a stored program. 2. Stored-Computer Program: o Proposed by John von Neumann, the stored-program architecture allows programs and data to be stored in the same memory. o Instructions are fetched and executed sequentially from memory. Key Difference: ENIAC lacked the stored-program capability, whereas modern computers use the stored-program concept for flexibility.  b) Discuss the use of PC register. \u2022 The Program Counter (PC) register is used to store the address of the next instruction to be executed by the processor. \u2022 It is incremented automatically during instruction execution. \u2022 If a jump or branch occurs, the PC is updated to point to the target address instead. Role: \u2022 Keeps track of program execution flow. \u2022 Essential for sequential execution and branching in programs.  c) What is the difference between instruction fetch and execution and interrupts? 1. Instruction Fetch: o Fetching the next instruction from memory based on the address stored in the Program Counter. o This step includes retrieving the binary code (opcode). 2. Instruction Execution: o Decoding the fetched instruction and performing the intended operation. o Operations could involve ALU computations, data transfer, or branching. 3. Interrupts: o An external or internal signal that temporarily halts the normal instruction execution. o Interrupt service routines (ISRs) execute to handle the interrupt. o Examples: hardware interrupts (e.g., I/O completion) or software interrupts. \nDifference: \u2022 Fetch and Execution are part of normal CPU operation, while interrupts cause deviation from the normal execution cycle.  d) Discuss the difference between ALU and Control Unit. 1. ALU (Arithmetic Logic Unit): o Performs arithmetic (addition, subtraction) and logic (AND, OR, NOT) operations. o It is a computation unit. 2. Control Unit (CU): o Directs the operations of the processor by generating control signals. o It fetches, decodes, and sends signals to coordinate data flow between CPU, memory, and I/O devices. Difference: \u2022 ALU performs computations, while the Control Unit manages the execution and control of instructions.  e) Define cache mapping function. \u2022 A cache mapping function determines how memory blocks are mapped to cache lines. Key Mapping Techniques: 1. Direct Mapping: o Each memory block is mapped to a single cache line using a modulo operation. 2. Associative Mapping: o A memory block can be placed in any cache line (fully flexible). 3. Set-Associative Mapping: o Combines both approaches. Cache lines are divided into sets, and a memory block can map to any line within a set. Example (Given in question): \u2022 A computer system uses four-way set-associative mapping with 128-byte cache and 32-bit addresses. o Divide address into tag, set index, and block offset. o Use these fields to determine cache location. Diagram Explanation: \n\u2022 The diagram would include blocks for tag comparison, set index lookup, and a block offset for byte access.  Second Section a) Mention five steps of instruction execution cycles performed by CPU. 1. Fetch: Retrieve the instruction from memory. 2. Decode: Interpret the instruction (opcode and operands). 3. Execute: Perform the operation (ALU computation, data transfer, etc.). 4. Memory Access: Access memory if the instruction involves load/store operations. 5. Write Back: Write the result back to a register or memory.  b) Discuss the difference between user-visible registers and control-status registers. 1. User-Visible Registers: o Accessible directly by programs. o Examples: General-purpose registers, data registers, and address registers. 2. Control-Status Registers: o Used by the CPU to control operations and track status. o Examples: Program Counter (PC), Instruction Register (IR), Status Register (flags). Difference: \u2022 User-visible registers are for program data manipulation, while control-status registers are for CPU operation management.  c) Explain the concept of program execution. \u2022 Program execution involves fetching, decoding, and executing instructions stored in memory. \u2022 It follows a sequence of instruction cycles: o Fetch \u2192 Decode \u2192 Execute \u2192 Write Back. \u2022 During execution, the PC advances, and operations are performed by ALU and registers.  d) Define cache mapping function and explain the differences among direct, associative, and set-associative mappings. \nCache Mapping Types: 1. Direct Mapping: o One memory block maps to one specific cache line. o Formula: Cache Line = (Memory Block Address) MOD (Number of Cache Lines). 2. Associative Mapping: o Memory block can go to any cache line. o Requires tag comparison for all cache lines. 3. Set-Associative Mapping: o Cache is divided into sets, and each set has multiple lines. o Formula: Set Number = (Memory Block Address) MOD (Number of Sets). Difference Summary: Mapping Type Flexibility Complexity Cost Direct Low Simple Low Associative High Complex High Set-Associative Medium Moderate Moderate  e) What do you understand by the term pipelining? \u2022 Pipelining is a technique used in CPUs to improve instruction throughput by overlapping instruction execution. \u2022 Each instruction cycle is divided into stages (Fetch, Decode, Execute, etc.). \u2022 Multiple instructions are executed simultaneously but in different stages. Example: Cycle Instruction 1 Instruction 2 Instruction 3 1 Fetch   2 Decode Fetch  3 Execute Decode Fetch Benefit: Increases CPU performance by executing more instructions per unit time."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "DAR ES SALAAM INSTITUTE OF TECHNOLOGY\nDEPARTMENT OF COMPUTER STUDIES\nCOU07302 MICROPROCESSOR AND COMPUTER ARCHITECTURE\nLecture 1\nby \nE. Kondela\n1. INTRODUCTION:\nMicroprocessor an integrated circuit that contains all the functions of a central processing unit of a\ncomputer.\nMicroprocessor is any of a type of miniature electronic device that contains the arithmetic, logic\nand control circuitry necessary to perform the functions of a digital computer\u2019s central processing\nunit. In effect, this kind of integrated circuit can interpret and execute program instructions as well\nas handle arithmetic operations.\nComputer architecture  is a set of rules and methods that describe the functionality, organization,\nand implementation of computer systems. The architecture of a system refers to its structure in\nterms of separately specified components of that system and their interrelationships.\n2. EVOLUTION OF COMPUTERS"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "2.1 First Generation: Vacuum Tubes\n2.1.1  ENIAC: The  ENIAC  (Electronic  Numerical  Integrator  And  Computer),  designed  and\nconstructed at\nthe University of Pennsylvania, was the world\u2019s first general-purpose electronic digital computer.\nThe project was a response to U.S needs during World War II. John Mauchly, a professor of\nelectrical engineering  at the University of Pennsylvania, and John Eckert, one of his graduate\nstudents,  proposed  to  build  a  general-purpose  computer  using  vacuum  tubes  for  the  BRL\u2019s\napplication. In 1943, the Army accepted this proposal, and work began on the ENIAC. The resulting\nmachine  was  enormous,  weighing  30  tons,  occupying  1500  square  feet  of  floor  space,  and\ncontaining more than 18,000 vacuum tubes. When operating, it consumed 140 kilowatts of power. It\nwas also substantially faster than any electromechanical computer, capable of 5000 additions per\nsecond. The ENIAC was completed in 1946, too late to be used in the war effort. The use of the\nENIAC for a purpose other than that for which it was built demonstrated its general-purpose nature.\nThe ENIAC continued to operate under BRL management until 1955, when it was disassembled.\n2.1.2 THE VON NEUMANN MACHINE  The task of entering and altering programs for the\nENIAC was extremely tedious. The programming process can be easy if the program could be\nrepresented in a form suitable for storing in memory alongside the data. Then, a computer could get\nits instructions by reading them from memory, and a program could be set or altered by setting the\nvalues of a portion of memory.\nIn 1946, von Neumann and his colleagues began the design of a new stored-program computer,\nreferred  to  as  the  IAS  computer,  at  the  Princeton  Institute  for  Advanced  Studies.  The  IAS\ncomputer,although not completed until 1952,is the prototype of all subsequent general-purpose\ncomputers.\nFigure 1 shows the general structure of the IAS computer). \nThe IAS computer consists of\n1.A main memory, which stores both data and instruction\n2.An arithmetic and logic unit (ALU) capable of operating on binary data\n3.A control unit, which interprets the instructions in memory and causes them to be executed\n4.Input and output (I/O) equipment operated by the control unit\nThis structure was outlined in von Neumann\u2019s earlier proposal, which is worth quoting at this\npoint:\n1.Because the device is primarily a computer, it will have to perform the elementary \noperations of arithmetic most frequently. At any rate a central arithmetical part of the device \nwill probably have to exist and this constitutes the first specific part: CA.\n2.The logical control of the device, that is, the proper sequencing of its operations, can be \nmost efficiently carried out by a central control organ. By the central control and the organs \nwhich perform it form the second specific part: CC"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "3.Any device which is to carry out long and complicated sequences of operations (specifically \nof calculations) must have a considerable memory . . . At any rate, the total memory \nconstitutes the third specific part of the device: M.\n4.The device must have organs to transfer . . . information from R into its specific parts C and \nM. These organs form its input, the fourth specific part: I\n5.The device must have organs to transfer . . . from its specific parts C and M into R. These \norgans form its output, the fifth specific part: O.\nThe control unit operates the IAS by fetching instructions from memory and executing them one at\na time. A more detailed structure diagram is shown in Figure 2. This figure reveals that both the\ncontrol unit and the ALU contain storage locations, called registers, defined as follows:\n1.Memory buffer register (MBR): Contains a word to be stored in memory or sent to the I/O \nunit, or is used to receive a word from memory or from the I/O unit.\n2.Memory address register (MAR): Specifies the address in memory of the word to be written \nfrom or read into the MBR.\n3.Instruction register (IR): Contains the 8-bit opcode instruction being executed.\n4.Instruction buffer register (IBR): Employed to hold temporarily the right-hand instruction \nfrom a word in memory.\n5.Program counter (PC): Contains the address of the next instruction-pair to be fetched from \nmemory.\n6.Accumulator (AC) and multiplier quotient (MQ): Employed to hold temporarily operands \nand results of ALU operations.\nFigure 2 Expanded Structure of IAS Computer\nCOMMERCIAL COMPUTERS  The  1950s  saw  the  birth  of  the  computer  industry  with  two\ncompanies, Sperry and IBM, dominating the marketplace. In 1947, Eckert and Mauchly formed the"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Eckert-Mauchly  Computer  Corporation  to  manufacture  computers  commercially.  Their  first\nsuccessful machine was the UNIV AC I (Universal Automatic Computer), which was commissioned\nby the Bureau of the Census for the 1950 calculations.The Eckert-Mauchly Computer Corporation\nbecame part of the UNIV AC division of Sperry-Rand Corporation, which went on to build a series\nof successor machines.\n1.The UNIV AC I was the first successful commercial computer. It was intended for both\nscientific and commercial applications.\n2.The UNIV AC II, which had greater memory capacity and higher performance than the\nUNIV AC I, was delivered in the late 1950s and illustrates several trends that have remained\ncharacteristic of the computer industry.\nThe UNIV AC division also began development of the 1100 series of computers, which was to\nbe its major source of revenue. This series illustrates a distinction that existed at one time. The first\nmodel, the UNIV AC 1103, and its successors for many years were primarily intended for scientific\napplications, involving long and complex calculations.\n2.2 The Second Generation: Transistors\nThe first major change in the electronic computer came with the replacement of the vacuum tube by\nthe transistor. The transistor is smaller, cheaper, and dissipates less heat than a vacuum tube but can\nbe used in the same way as a vacuum tube to construct computers. Unlike the vacuum tube, which\nrequires wires, metal plates, a glass capsule, and a vacuum, the transistor is a solid- state device,\nmade from silicon.\nThe transistor was invented at Bell Labs in 1947 and by the 1950s had launched an electronic\nrevolution.  It was  not  until  the  late  1950s,  however,  that  fully  transistorized  computers  were\ncommercially available.\nThe use of the transistor defines the second generation of computers. It has become widely accepted\nto classify computers into generations based on the fundamental hardware technology employed\n(Table 1)\nTable 1 Computer Generations\nTHE IBM 7094 From the introduction of the 700 series in 1952 to the introduction of the last\nmember of the 7000 series in 1964, this IBM product line underwent an evolution that is typical of\ncomputer products. Successive members of the product line show increased performance, increased\ncapacity, and/or lower cost.\n2.3 The Third Generation: Integrated Circuits\nIn  1958  came  the  achievement  that  revolutionized  electronics  and  started  the  era  of\nmicroelectronics: the invention of the integrated circuit. It is the integrated circuit that defines the\nthird generation of computers.\nMICROELECTRONICS:  Microelectronics  means,  literally,  \u201csmall  electronics.\u201d  Since  the\nbeginnings  of  digital  electronics  and  the  computer  industry,  there  has  been  a  persistent  and\nconsistent trend toward the reduction in size of digital electronic circuits."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "IBM SYSTEM/360:  By 1964, IBM had a firm grip on the computer market with its 7000 series of\nmachines. In that year, IBM announced the System/360, a new family of computer products. DEC \nPDP-8: In the same year that IBM shipped its first System/360, another momentous first shipment\noccurred: PDP-8 from Digital Equipment Corporation (DEC).At a time when the average computer\nrequired an air-conditioned room,the PDP-8 (dubbed a minicomputer by the industry, after the\nminiskirt of the day) was small enough that it could be placed on top of a lab bench or be built into\nother equipment. It could not do everything the mainframe could, but at $16,000, it was cheap\nenough for each lab technician to have one. In contrast, the System/360 series of mainframe\ncomputers introduced just a few months before cost hundreds of thousands of dollars.\n2.4 Later Generations\nTable 1 suggests that there have been a number of later generations, based on advances in integrated\ncircuit  technology.  With  the  introduction  of  large-scale  integration  (LSI),  more  than  1000\ncomponents can be placed on a single integrated circuit chip. Very-large-scale integration (VLSI)\nachieved more than 10,000 components per chip, while current ultra-large-scale integration (ULSI)\nchips can contain more than one million components.\nSEMICONDUCTOR  MEMORY:  The  first  application  of  integrated  circuit  technology  to\ncomputers was construction of the processor (the control unit and the arithmetic and logic unit) out\nof integrated circuit chips. But it was also found that this same technology could be used to\nconstruct memories.\nMICROPROCESSORS:  Just as the density of elements on memory chips has continued to rise,so\nhas the density of elements on processor chips. As time went on,more and more elements were\nplaced on each chip, so that fewer and fewer chips were needed to construct a single computer\nprocessor.\nA breakthrough was achieved in 1971,when Intel developed its 4004.The 4004 was the first chip to\ncontain all of the components of a CPU on a single chip.\nThe next major step in the evolution of the microprocessor was the introduction in 1972 of the Intel\n8008. This was the first 8-bit microprocessor and was almost twice as complex as the 4004.\nNeither of these steps was to have the impact of the next major event: the introduction in 1974 of\nthe Intel 8080.This was the first general-purpose microprocessor. Whereas the 4004 and the 8008\nhad been designed for specific applications, the 8080 was designed to be the CPU of a general-\npurpose microcomputer\nAbout the same time, 16-bit microprocessors began to be developed. However, it was not until the\nend of the 1970s that powerful, general-purpose 16-bit microprocessors appeared. One of these was\nthe 8086.\n3. EVOLUTION OF INTEL X86 ARCHITECTURE:\nWe have Two computer families: the Intel x86 and the ARM architecture. The current x86 offerings\nrepresent the results of decades of design effort on complex instruction set computers\n(CISCs).\nThe x86 incorporates  the sophisticated design  principles  once found only  on mainframes  and\nsupercomputers and serves as an excellent example of CISC design. An alternative approach to\nprocessor design in the reduced instruction set computer (RISC). The ARM architecture is used in a\nwide variety of embedded systems and is one of the most powerful and best-designed RISC-based\nsystems on the market.\nIn terms of market share, Intel has ranked as the number one maker of microprocessors for non-\nembedded  systems  for  decades,  a  position  it  seems  unlikely  to  yield.  Interestingly,  as\nmicroprocessors have grown faster and much more complex, Intel has actually picked up the pace.\nIntel used to develop microprocessors one after another, every four years.\nIt is worthwhile to list some of the highlights of the evolution of the Intel product line:\n1.8080: The world\u2019s first general-purpose microprocessor. This was an 8-bit machine, with an\n8-bit data path to memory. The 8080 was used in the first personal computer, the Altair.\n2.8086: A far more powerful, 16-bit machine. In addition to a wider data path and larger\nregisters, the 8086 sported an instruction cache, or queue, that prefetches a few instructions\nbefore they are executed. A variant of this processor, the 8088, was used in IBM\u2019s first\npersonal computer, securing the success of Intel. The 8086 is the first appearance of the x86\narchitecture.\n3.80286: This extension of the 8086 enabled addressing a 16-MByte memory instead of just 1\nMbyte.\n4.80386: Intel\u2019s first 32-bit machine, and a major overhaul of the product. With a 32-bit\narchitecture, the 80386 rivaled the complexity and power of minicomputers and mainframes\nintroduced just a few years earlier. This was the first Intel processor to support multitasking,\nmeaning it could run multiple programs at the same time.\n5.80486: The 80486 introduced the use of much more sophisticated and powerful cache\ntechnology and sophisticated instruction pipelining. The 80486 also offered a built-in math\ncoprocessor, offloading complex math operations from the main CPU.\n6.Pentium: With the Pentium, Intel introduced the use of superscalar techniques, which allow\nmultiple instructions to execute in parallel.\n7.Pentium Pro: The Pentium Pro continued the move into superscalar organization begun with\nthe Pentium, with aggressive use of register renaming, branch prediction, data flow analysis,\nand speculative execution.\n8.Pentium II: The Pentium II incorporated Intel MMX technology, which is designed\nspecifically to process video, audio, and graphics data efficiently.\n9.Pentium III: The Pentium III incorporates additional floating-point instructions to support\n3D graphics software.\n10.Pentium 4: The Pentium 4 includes additional floating-point and other enhancements for\n8 multimedia.\n11.Core:  This  is  the  first  Intel  x86  microprocessor  with  a  dual  core,  referring  to  the\nimplementation of two processors on a single chip.\n12.Core 2: The Core 2 extends the architecture to 64 bits. The Core 2 Quad provides four\nprocessors on a single chip.\nOver 30 years after its introduction in 1978, the x86 architecture continues to dominate the\nprocessor market outside of embedded systems. Although the organization and technology\nof  the  x86  machines  has  changed  dramatically  over  the  decades,  the  instruction  set\narchitecture has evolved to remain backward compatible with earlier versions. Thus, any\nprogram written on an older version of the x86 architecture can execute on newer versions.\nAll changes to the instruction set architecture have involved additions to the instruction set,\nwith no subtractions. The rate of change has been the addition of roughly one instruction per\nmonth  added  to  the  architecture  over  the  30  years.  so  that  there  are  now  over  500\ninstructions in the instruction set.\n13.The x86 provides an excellent illustration of the advances in computer hardware over the\npast 30 years. The 1978 8086 was introduced with a clock speed of 5 MHz and had 29,000\ntransistors. A quad-core Intel Core 2 introduced in 2008 operates at 3 GHz, a speedup of a\nfactor of 600, and has 820 million transistors, about 28,000 times as many as the 8086. Yet\nthe Core 2 is in only a slightly larger package than the 8086 and has a comparable cost.\n4. DESIGNING FOR PERFORMANCE\nYear by year, the cost of computer systems continues to drop dramatically, while the performance\nand capacity of those systems continue to rise equally dramatically. Desktop applications that\nrequire the great power of today\u2019s microprocessor-based systems include\n1.Image processing\n2.Speech recognition\n3.Videoconferencing\n4.Multimedia authoring\n5.V oice and video annotation of files\n6.Simulation modeling\nMicroprocessor Speed\nThe evolution of Microprocessors continues to bear out Moore\u2019s law. So long as this law holds,\nchipmakers can unleash a new generation of chips every three years\u2014with four times as many\ntransistors. In microprocessors, the addition of new circuits, and the speed boost that comes from\nreducing the distances between them, has improved performance four- or fivefold every three years\nor so since Intel launched its x86 family in 1978. The more elaborate techniques for feeding\nthe monster into contemporary processors are the following:\n1.Branch prediction: The processor looks ahead in the instruction code fetched from memory\nand predicts which branches, or groups of instructions, are likely to be processed next\n2.Data flow analysis: The processor analyzes which instructions are dependent on each other\u2019s\nresults, or data, to create an optimized schedule of instructions\n3.Speculative execution: Using branch prediction and data flow analysis, some processors\nspeculatively execute instructions ahead of their actual appearance in the program execution,\nholding the results in temporary locations.\nPerformance Balance\nWhile processor power has  raced ahead at breakneck speed, other critical components of the\ncomputer have not kept up.The result is a need to look for performance balance: an adjusting of the\norganization and architecture to compensate for the mismatch among the capabilities of the various\ncomponents.\nThe interface between processor and main memory is  the most crucial pathway in the entire\ncomputer because it is responsible for carrying a constant flow of program instructions and data\nbetween memory chips and the processor.\nThere are a number of ways that a system architect can attack this problem, all of which are\nreflected in contemporary computer designs. Consider the following examples:\n1.Increase the number of bits that are retrieved at one time by making DRAMs \u201cwider\u201d rather\nthan \u201cdeeper\u201d and by using wide bus data paths.\n2.Change the DRAM interface to make it more efficient by including a cache7 or other\nbuffering scheme on the DRAM chip.\n3.Reduce  the  frequency  of  memory  access  by  incorporating  increasingly  complex  and\nefficient cache structures between the processor and main memory.\n4.Increase the interconnect bandwidth between processors and memory by using higher- speed\nbuses and by using a hierarchy of buses to buffer and structure data flow.\nImprovements in Chip Organization and Architecture\nThere are three approaches to achieving increased processor speed:\n1.Increase the hardware speed of the processor.\n2.Increase the size and speed of caches that are interposed between the processor and main\nmemory. In particular, by dedicating a portion of the processor chip itself to the cache, cache\naccess times drop significantly.\n3.Make changes to the processor organization and architecture that increase the effective\nspeed of instruction execution.\nHowever, as clock speed and logic density increase, a number of obstacles become more\nsignificant:\n1.Power: As the density of logic and the clock speed on a chip increase, so does the power\ndensity. \n2.RC delay: The speed at which electrons can flow on a chip between transistors is limited by\nthe  resistance  and  capacitance  of  the  metal  wires  connecting  them;  specifically,  delay\nincreases as the RC product increases. As components on the chip decrease in size, the wire\ninterconnects become thinner, increasing resistance. Also, the wires are closer together,\nincreasing capacitance.\n3.Memory latency: Memory speeds lag processor speeds.\nBeginning in the late 1980s, and continuing for about 15 years, two main strategies have\nbeen used to increase performance beyond what can be achieved simply by increasing clock\nspeed. First, there has been an increase in cache capacity. Second, the instruction execution\nlogic within a processor has become increasingly complex to enable parallel execution of\ninstructions within the processor.\nTwo noteworthy design approaches have been pipelining and superscalar. A pipeline works\nmuch as an assembly line in a manufacturing plant enabling different stages of execution of\ndifferent instructions to occur at the same time along the pipeline. A superscalar approach in\nessence allows multiple pipelines within a single processor so that instructions that do not\ndepend on one another can be executed in parallel.\n5. COMPUTER COMPONENTS\nVirtually  all  contemporary  computer  designs  are  based  on  concepts  developed  by  John  von\nNeumann at the Institute for Advanced Studies, Princeton. Such a design is referred to as the von\nNeumann architecture and is based on three key concepts:\n1.Data and instructions are stored in a single read\u2013write memory.\n2.The contents of this memory are addressable by location, without regard to the type of data\ncontained there.\n3.Execution occurs in a sequential fashion (unless explicitly modified) from one instruction to\nthe next.\nIf there is a particular computation to be performed, a configuration of logic components designed\nspecifically for that computation could be constructed. The resulting \u201cprogram\u201d is in the form of\nhardware and is termed a hardwired program.\nNow consider this alternative. Suppose we construct a general-purpose configuration of arithmetic\nand logic functions. This set of hardware will perform various functions on data depending on\ncontrol signals applied to the hardware. In the original case of customized hardware, the system\naccepts data and produces results (Figure 3a). With general-purpose hardware, the system accepts\ndata and control signals and produces results.\nThus, instead of rewiring the hardware for each new program, the programmer merely needs to\nsupply a new set of control signals by providing a unique code for each possible set of control\nsignals, and let us add to the general-purpose hardware a segment that can accept a code and\ngenerate control signals (Figure 3b). To distinguish this new method of programming, a sequence of\ncodes or instructions is called software.\n3 Hardware and Software approaches\nFigure 3b indicates two major components of the system: an instruction interpreter and a module of\ngeneral-purpose arithmetic and logic functions. These two constitute the CPU. Data and instructions\nmust be put into the system. For this we need some sort of input module. A means of reporting\nresults is needed, and this is in the form of an output module. Taken together, these are referred to as\nI/O components.\nThere must be a place to store temporarily both instructions and data. That module is called\nmemory,  or  main  memory  to  distinguish  it  from  external  storage  or  peripheral  devices.  V on\nNeumann pointed out that the same memory could be used to store both instructions and data.\nFigure 4 illustrates these top-level components and suggests the interactions among them. The CPU\nexchanges data with memory. For this purpose, it typically makes use of two internal (to the CPU)\nregisters: a memory address register (MAR), which specifies the address in memory for the next\nread or write, and a memory buffer register (MBR), which contains the data to be written into\nmemory or receives the data read from memory. Similarly, an I/O address register (I/OAR) specifies\na particular I/O device. An I/O buffer (I/OBR) register is used for the exchange of data between an\nI/O module and the CPU."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "A memory module consists of a set of locations, defined by sequentially numbered addresses. Each\nlocation contains a binary number that can be interpreted as either an instruction or data. An I/O\nmodule transfers data from external devices to CPU and memory, and vice versa. It contains\ninternal buffers for temporarily holding these data until they can be sent on.\nFigure 4. Computer Components\n6. COMPUTER FUNCTIONS\nThe basic function performed by a computer is execution of a program, which consists of a set of\ninstructions stored in memory. Instruction processing consists of two steps: The processor reads"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "(fetches) instructions from memory one at a time and executes each instruction. Program execution\nconsists of repeating the process of instruction fetch and instruction execution.\nThe processing required for a single instruction is called an instruction cycle. Using the simplified\ntwo-step description given previously, the instruction cycle is depicted in Figure 5. The two steps\nare referred to as the fetch cycle and the execute cycle. Program execution halts only if the machine\nis turned off, some sort of unrecoverable error occurs, or a program instruction that halts the\ncomputer is encountered.\nFigure 5 Basic Instruction Cycle\na) Instruction Fetch and Execute\nAt the beginning of each instruction cycle, the processor fetches an instruction from memory. The\nprogram counter (PC) holds the address of the instruction to be fetched next, the processor always\nincrements the PC after each instruction fetch so that it will fetch the next instruction in sequence.\nFor example, consider a computer in which each instruction occupies one 16-bit word of memory. If\nthe program counter is set to location 300. The processor will next fetch the instruction at location\n300. On next instruction cycles, it will fetch instructions from locations 301,302,303,and so on.\nThe fetched instruction is loaded into a register in the processor known as the instruction register\n(IR). The processor interprets the instruction and performs the required action. In general, these\nactions fall into four categories:\n1.Processor-memory: Data may be transferred from processor to memory or from memory to\nprocessor.\n2.Processor-I/O:  Data  may  be  transferred  to  or from  a  peripheral  device  by transferring\nbetween the processor and an I/O module.\n3.Data processing: The processor may perform some arithmetic or logic operation on data.\n4.Control: An instruction may specify that the sequence of execution be altered. For example,\nthe processor may fetch an instruction from location 149, which specifies that the next\ninstruction be from location 182. The processor will remember this fact by setting the\nprogram counter to 182.Thus,on the next fetch cycle, the instruction will be fetched from\nlocation 182 rather than 150.\nAn instruction\u2019s execution may involve a combination of these actions. Consider a simple example\nusing a hypothetical machine that includes the characteristics listed in Figure 6. The processor\ncontains a single data register, called an accumulator (AC). Both instructions and data are 16 bits\nlong. Thus, it is convenient to organize memory using 16-bit words. The instruction format provides\n4 bits for the opcode, so that there can be as many as 2^4 = 16 different opcodes, and up to 2^12 =\n4096 (4K) words of memory can be directly addressed."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 6 Characteristics of a Hypothetical machine\nFigure 7 illustrates a partial program execution, showing the relevant portions of memory and\nprocessor registers.1 The program fragment shown adds the contents of the memory word at\naddress 940 to the contents of the memory word at address 941 and stores the result in later\nlocation.\nFigure 7 Example of Program Execution\nThree instructions are required:\n1.The PC contains 300, the address of the first instruction. This instruction (the value 1940 in\nhexadecimal) is loaded into the instruction register IR and the PC is incremented.\n2.The first 4 bits (first hexadecimal digit) in the IR indicate that the AC is to be loaded. The\nremaining 12 bits (three hexadecimal digits) specify the address (940) from which data are\nto be loaded.\n3.The next instruction (5941) is fetched from location 301 and the PC is incremented.\n4.The old contents of the AC and the contents of location 941 are added and the result is\nstored in the AC.\n5.The next instruction (2941) is fetched from location 302 and the PC is incremented.\n6.The contents of the AC are stored in location 941.\nFor example, the PDP-11 processor includes an instruction, expressed symbolically as ADD B,A,\nthat stores the sum of the contents of memory locations B and A into memory location A. A single\ninstruction cycle with the following steps occurs:\n1.Fetch the ADD instruction.\n2.Read the contents of memory location A into the processor."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "3.Read the contents of memory location B into the processor. In order that the contents of A\nare not lost, the processor must have at least two registers for storing memory values, rather\nthan a single accumulator.\n4.Add the two values.\n5.Write the result from the processor to memory location A.\nFigure 8 provides a more detailed look at the basic instruction cycle of Figure 1.5.The figure is in\nthe form of a state diagram The states can be described as follows:\nFigure 8 Instruction Cycle State Diagram\n1.Instruction address calculation (iac): Determine the address of the next instruction to be\nexecuted.\n2.Instruction fetch (if): Read instruction from its memory location into the processor.\n3.Instruction operation decoding (iod): Analyze instruction to determine type of operation to\nbe performed and operand(s) to be used.\n4.Operand address calculation (oac): If the operation involves reference to an operand in\nmemory or available via I/O, then determine the address of the operand.\n5.Operand fetch (of): Fetch the operand from memory or read it in from I/O.\n6.Data operation (do): Perform the operation indicated in the instruction.\n7.Operand store (os): Write the result into memory or out to I/O.\nb) Interrupts\nVirtually all computers provide a mechanism by which other modules (I/O, memory) may interrupt\nthe normal processing of the processor. Interrupts are provided primarily as a way to improve\nprocessing efficiency. Table 1.2 lists the most common classes of interrupts.\nTable 2 Classes of Interrupts\nFigure 1.9a illustrates this state of affairs. The user program performs a series of WRITE calls\ninterleaved with processing. Code segments 1, 2, and 3 refer to sequences of instructions that do not\ninvolve I/O. The WRITE calls are to an I/O program that is a system utility and that will perform\nthe actual I/O operation. The I/O program consists of three sections:"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "1.A sequence of instructions, labeled 4 in the figure, to prepare for the actual I/O operation.\nThis may include copying the data to be output into a special buffer and preparing the\nparameters for a device command.\n2.The actual I/O command. Without the use of interrupts, once this command is issued, the\nprogram must wait for the I/O device to perform the requested function (or periodically poll\nthe device). The program might wait by simply repeatedly performing a test operation to\ndetermine if the I/O operation is done.\n3.A sequence of instructions, labeled 5 in the figure, to complete the operation. This may\ninclude setting a flag indicating the success or failure of the operation.\nFigure 9 Program Flow of Control Without and With Interrupts\nINTERRUPTS AND THE INSTRUCTION CYCLE With interrupts, the processor can be engaged\nin executing other instructions while an I/O operation is in progress.\nConsider the flow of control in Figure 9b. As before, the user program reaches a point at which it\nmakes a system call in the form of a WRITE call. After these few instructions have been executed,\ncontrol returns to the user program. Meanwhile, the external device is busy accepting data from\ncomputer memory and printing it. This I/O operation is conducted concurrently with the execution\nof instructions in the user program.\nWhen the external device becomes ready to accept more data from the processor,\u2014the I/O module\nfor that external device sends an interrupt request signal to the processor. The processor responds by\nsuspending operation of the current program, branching off to a program to service that particular\nI/O device, known as an interrupt handler, and resuming the original execution after the device is\nserviced. The points at which such interrupts occur are indicated by an asterisk in Figure 9b.\nFrom the point of view of the user program, an interrupt is just that: an interruption of the normal\nsequence of execution. When the interrupt processing is completed, execution resumes (Figure\n1.10)."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 10 Transfer of Control via Interrupts\nTo accommodate interrupts, an interrupt cycle is added to the instruction cycle, as shown in\nFigure 11.\nFigure 11Instruction Cycle with Interrupts\nIn the interrupt cycle, the processor checks to see if any interrupts have occurred. If no interrupts\nare pending, the processor proceeds to the fetch cycle and fetches the next instruction of the current\nprogram. If an interrupt is pending, the processor does the following:\n1.It suspends execution of the current program being executed and saves its context\n2.It sets the program counter to the starting address of an interrupt handler routine.\nThe processor now proceeds to the fetch cycle and fetches the first instruction in the interrupt\nhandler program, which will service the interrupt. When the interrupt handler routine is completed,\nthe processor can resume execution of the user program at the point of interruption.\nConsider Figure 12, which is a timing diagram based on the flow of control in Figures 9a\nand 9b. Figure 9c indicates this state of affairs. In this case, the user program reaches the second\nWRITE call before the I/O operation spawned by the first call is complete. The result is that the\nuser program is hung up at that point. When the preceding I/O operation is completed, this new\nWRITE call may be processed, and a new I/O operation may be started. Figure 1.13 shows the\ntiming for this situation with and without the use of interrupts. We can see that there is still a gain in\nefficiency because part of the time during which the I/O operation is underway overlaps with the\nexecution of user instructions."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 12 Program Timing: Short I/O Wait\nFigure 13 Program Timing: Long I/O Wait"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 14 shows a revised instruction cycle state diagram that includes interrupt cycle\nprocessing\nMULTIPLE INTERRUPTS Multiple interrupts can occur. Two approaches can be taken to dealing\nwith multiple interrupts.\nThe first is to disable interrupts while an interrupt is being processed. A disabled interrupt simply\nmeans that the processor can and will ignore that interrupt request signal. Thus, when a user\nprogram  is  executing  and  an  interrupt  occurs,  interrupts  are  disabled  immediately.  After  the\ninterrupt handler routine completes, interrupts are enabled before resuming the user program and\nthe processor checks to see if additional interrupts have occurred. This approach is nice and simple,\nas interrupts are handled in strict sequential order (Figure 15a).\nThe drawback to the preceding approach is that it does not take into account relative priority or\ntime-critical needs\nA second approach is to define priorities for interrupts and to allow an interrupt of higher priority to\ncause a lower-priority interrupt handler to be itself interrupted (Figure 15b). As an example of this\nsecond approach, consider a system with three I/O devices: a printer, a disk, and a communications\nline, with increasing priorities of 2, 4, and 5, respectively.\nFigure 15 Transfer of Control with Multiple Interrupts"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 16 Example Time Sequence of Multiple Interrupts\nA user program begins at t = 0. At t = 10, a printer interrupt occurs; user information is placed on\nthe system stack and execution continues at the printer interrupt service routine (ISR). While this\nroutine is still executing, at t = 15, a communications interrupt occurs. Because the communications\nline has higher priority than the printer, the interrupt is honored. The printer ISR is interrupted, its\nstate is pushed onto the stack,and execution continues at the communications ISR.While this routine\nis executing, a disk interrupt occurs (t = 20). Because this interrupt is of lower priority, it is simply\nheld, and the communications ISR runs to completion.\nWhen the communications ISR is complete (t = 25), the previous processor state is restored, which\nis the execution of the printer ISR. However, before even a single instruction in that routine can be\nexecuted, the processor honors the higher-priority disk interrupt and control transfers to the disk\nISR. Only when that routine is complete (t = 35) is the printer ISR resumed. When that routine\ncompletes (t = 40), control finally returns to the user program.\nI/O Function \nAn I/O module (e.g., a disk controller) can exchange data directly with the processor. Just as the\nprocessor can initiate a read or write with memory, designating the address of a specific location,\nthe processor can also read data from or write data to an I/O module\nIn some cases, it is desirable to allow I/O exchanges to occur directly with memory. In such a case,\nthe processor grants to an I/O module the authority to read from or write to memory, so that the I/O-\nmemory transfer can occur without tying up the processor. During such a transfer, the I/O module\nissues  read  or  write  commands  to  memory,  relieving  the  processor  of  responsibility  for  the\nexchange. This operation is known as direct memory access (DMA)."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "7. BUS INTERCONNECTION\nA bus is a communication pathway connecting two or more devices. A key characteristic of a bus is\nthat  it  is  a  shared  transmission  medium.  Multiple  devices  connect  to  the  bus,  and  a  signal\ntransmitted by any one device is available for reception by all other devices attached to the bus. If\ntwo devices transmit during the same time period, their signals will overlap and become garbled.\nThus, only one device at a time can successfully transmit.\nTypically, a bus consists of multiple communication pathways, or lines. Each line is capable of\ntransmitting signals representing binary 1 and binary 0. An 8-bit unit of data can be transmitted over\neight bus lines. A bus that connects major computer components (processor, memory, I/O) is called\na system bus.\nFigure 17Bus Interconnection Schemes\nOn any bus the lines can be classified into three functional groups (Figure 1.17): data, address, and\ncontrol lines. In addition, there may be power distribution lines that supply power to the attached\nmodules.\nThe data lines provide a path for moving data among system modules. These lines, collectively, are\ncalled the data bus.\nThe address lines are used to designate the source or destination of the data on the data bus. For\nexample, on an 8-bit address bus, address 01111111 and below might reference locations in a\nmemory module (module 0) with 128 words of memory, and address 10000000 and above refer to\ndevices attached to an I/O module (module 1). \nThe control lines are used to control the access to and the use of the data and address lines. Control\nsignals transmit both command and timing information among system modules. Timing\nsignals indicate the validity of data and address information. Command signals specify operations\nto be performed. Typical control lines include\n1.Memory write: Causes data on the bus to be written into the addressed location\n2.Memory read: Causes data from the addressed location to be placed on the bus\n3.I/O write: Causes data on the bus to be output to the addressed I/O port\n4.I/O read: Causes data from the addressed I/O port to be placed on the bus\n5.Transfer ACK: Indicates that data have been accepted from or placed on the bus\n6.Bus request: Indicates that a module needs to gain control of the bus\n7.Bus grant: Indicates that a requesting module has been granted control of the bus\n8.Interrupt request: Indicates that an interrupt is pending\n9.Interrupt ACK: Acknowledges that the pending interrupt has been recognized\n10.Clock: Is used to synchronize operations\n11.Reset: Initializes all modules\nThe operation of the bus is as follows. If one module wishes to send data to another, it must do two\nthings: (1) obtain the use of the bus, and (2) transfer data via the bus. If one module wishes to\nrequest data from another module, it must (1) obtain the use of the bus, and (2) transfer a request to\nthe other module over the appropriate control and address lines. It must then wait for that second\nmodule to send the data."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "The classic physical arrangement of a bus is depicted in Figure 18.\nFigure 18 Typical Physical Realization of a Bus Architecture\nIn this example, the bus consists of two vertical columns of conductors. Each of the major system\ncomponents occupies one or more boards and plugs into the bus at these slots. Thus, an on-chip bus\nmay connect the processor and cache memory, whereas an on-board bus may connect the processor\nto main memory and other components.\nThis arrangement is most convenient. A small computer system may be acquired and then expanded\nlater (more memory, more I/O) by adding more boards. If a component on a board fails, that board\ncan easily be removed and replaced.\nMultiple-Bus Hierarchies\nIf a great number of devices are connected to the bus, performance will suffer. There are two main\ncauses:\n1.In general, the more devices attached to the bus, the greater the bus length and hence the\ngreater the propagation delay.\n2.The bus may become a bottleneck as the aggregate data transfer demand approaches the\ncapacity of the bus.\nMost computer systems use multiple buses, generally laid out in a hierarchy. A typical traditional\nstructure is shown in Figure 19a. There is a local bus that connects the processor to a cache memory\nand that may support one or more local devices The cache memory is connected to a system bus to\nwhich all of the main memory modules are attached. It is possible to connect I/O controllers directly\nonto the system bus. A more efficient solution is to make use of one or more expansion buses for\nthis purpose. This arrangement allows the system to support a wide variety of I/O devices and at the\nsame time insulate memory-to-processor traffic from I/O traffic. \nFigure 19a shows some typical examples of I/O devices that might be attached to the expansion bus.\nNetwork connections include local area networks (LANs), wide area networks (WANs), SCSI\n(small  computer  system  interface),  serial  port..  This  traditional  bus  architecture  is  reasonably\nefficient but begins to break down as higher and higher performance is seen in the I/O devices. In\nresponse to these growing demands, a common approach taken by industry is to build a high-speed\nbus that is closely integrated with the rest of the system, requiring only a bridge between the\nprocessor\u2019s bus and the high-speed bus. This arrangement is sometimes known as a mezzanine\narchitecture.\nFigure 19b shows a typical realization of this approach. Again,there is a local bus that connects the\nprocessor to a cache controller, which is in turn connected to a system bus that supports main\nmemory. The cache controller is integrated into a bridge, or buffering device, that connects to the\nhigh-speed bus. This bus supports connections to high-speed LANs, video and graphics workstation\ncontrollers, SCSI and FireWireLower-speed devices are still supported off an expansion bus, with"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "an interface buffering traffic between the expansion bus and the high-speed bus. The advantage of\nthis arrangement is that the high-speed bus brings high-demand devices into closer integration with\nthe processor and at the same time is independent of the processor.\nFigure 19 Example Bus Configuration\nElements of Bus Design\nFigure 19 Example Bus Confihuration\nThere are a few design elements that serve to classify and differentiate buses. Table 1.3 lists key\nelements.\n1.BUS TYPES Bus lines can be separated into two generic types: dedicated and multi- plexed.\nA dedicated bus line is permanently assigned either to one function or to a physical subset of\ncomputer components. Physical dedication refers to the use of multiple buses, each of which\nconnects only a subset of modules. The potential advantage of physical dedication is high\nthroughput, because there is less bus contention.A disadvantage is the increased size and\ncost of the system.\nAddress  and data information may be transmitted over the same set of lines  using an\nAddress Valid control line. At the beginning of a data transfer, the address is placed on the\nbus and the Address Valid line is activated. The address is then removed from the bus, and\nthe same bus connections are used for the subsequent read or write data transfer. This\nmethod of using the same lines for multiple purposes is known as time multiplexing.The\nadvantage of time multiplexing is the use of fewer lines, which saves space and, usually,\ncost. The disadvantage is that more complex circuitry is needed within each module.\n2.METHOD OF ABITRATION. The various methods can be roughly classified as being\neither centralized or distributed. In a centralized scheme, a single hardware device, referred\nto as a bus controller or arbiter, is responsible for allocating time on the bus. In a distributed\nscheme, there is no central controller. Rather, each module contains access control logic and\nthe modules act together to share the bus. With both methods of arbitration, the purpose is to"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "designate either the processor or an I/O module, as master. The master may then initiate a\ndata transfer (e.g., read or write) with some other device, which acts as slave for this\nparticular exchange.\n3.TIMING Buses use either synchronous timing or asynchronous timing. With synchronous\ntiming,  the  occurrence  of  events  on  the  bus  is  determined  by  a  clock.  A single  1\u20130\ntransmission is referred to as a clock cycle or bus cycle and defines a time slot.\nFigure 20 Timing of Synchronous Bus Operations\nFigure 19 shows a typical, but simplified, timing diagram for synchronous read and write. In this\nsimple example, the processor places a memory address on the address lines during the first clock\ncycle and may assert various status lines. Once the address lines have stabilized, the processor\nissues an address enable signal. For a read operation, the processor issues a read command at the\nstart of the second cycle. A memory module recognizes the address and, after a delay of one cycle,\nplaces the data on the data lines. The processor reads the data from the data lines and drops the read\nsignal. For a write operation, the processor puts the data on the data lines at the start of the second\ncycle, and issues a write command after the data lines have stabilized. The memory module copies\nthe information from the data lines during the third clock cycle. With asynchronous timing, the\noccurrence of one event on a bus follows and depends on the occurrence of a previous event."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 21 Timing of Asynchronous Bus Operations\nIn the simple read example of Figure 21a, the processor places address and status signals on the bus.\nAfter pausing for these signals to stabilize, it issues a read command, indicating the presence of\nvalid address and control signals. The appropriate memory decodes the address and responds by\nplacing the data on the data line. Once the data lines have stabilized, the memory module asserts the\nacknowledged line to signal the processor that the data are available. Once the master has read the\ndata from the data lines, it deasserts the read signal. This causes the memory module to drop the\ndata and acknowledge lines. Finally, once the acknowledge line is dropped, the master removes the\naddress information.\nFigure 21b shows a simple asynchronous write operation. In this case, the master places the data on\nthe data line at the same time that is puts signals on the status and address lines. The memory\nmodule responds to the write command by copying the data from the data lines and then asserting\nthe acknowledge line. The master then drops the write signal and the memory module drops the\nacknowledge signal\nSynchronous timing is simpler to implement and test. However, it is less flexible than\nasynchronous timing. With asynchronous timing, a mixture of slow and fast devices, using older\nand newer technology, can share a bus.\n\uf0b7BUS WIDTH The width of the data bus has an impact on system performance: The wider\nthe data bus, the greater the number of bits transferred at one time. The width of the address\nbus has an impact on system capacity: the wider the address bus, the greater the range of\nlocations that can be referenced.\n\uf0b7DATA TRANSFER TYPE Finally, a bus supports various data transfer types, as illustrated\nin Figure 22."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "Figure 22 Bus Data Transfer Types\nIn the case of a multiplexed address/data bus, the bus is first used for specifying the address and\nthen for transferring the data. For a read operation, there is typically a wait while the data are being\nfetched from the slave to be put on the bus. For either a read or a write, there may also be a delay if\nit is necessary to go through arbitration to gain control of the bus for the remainder of the operation.\nIn the case of dedicated address and data buses, the address is put on the address bus and remains\nthere while the data are put on the data bus. For a write operation, the master puts the data onto the\ndata bus as soon as the address has stabilized and the slave has had the opportunity to recognize its\naddress. For a read operation, the slave puts the data onto the data bus as soon as it has recognized\nits address and has fetched the data.\nA read\u2013modify\u2013write operation is simply a read followed immediately by a write to the same\naddress\nRead-after-write is an indivisible operation consisting of a write followed immediately by a read\nfrom the same address.\nSome bus systems also support a block data transfer. The first data item is transferred to or from the\nspecified address; the remaining data items are transferred to or from subsequent addresses"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "8. PCI (PERIPHERAL COMPONENT INTERCONNECT)\nThe peripheral component interconnect (PCI) is a popular high-bandwidth, processor-independent\nbus that can function as a peripheral bus. The current standard allows the use of up to 64 data lines\nat 66 MHz, for a raw transfer rate of 528 MByte/s, or 4.224 Gbps. It requires very few chips to\nimplement and supports other buses attached to the PCI bus.\nIntel began work on PCI in 1990 for its Pentium-based systems. The industry association, the PCI\nSpecial Interest Group (SIG), developed and further and maintained the compatibility of the PCI\nspecifications.  PCI  is  designed  to  support  a  variety  of  microprocessor-based  configurations,\nincluding both single- and multiple-processor systems. It makes use of synchronous timing and a\ncentralized arbitration scheme.\nFigure 23 Example PCI Configurations\nFigure 23a shows a typical use of PCI in a single-processor system. The bridge acts as a data buffer\nso that the speed of the PCI bus may differ from that of the processor\u2019s I/O capability. In a\nmultiprocessor system (Figure 1.23b), one or more PCI configurations may be connected by bridges\nto the processor\u2019s system bus. Again, the use of bridges keeps the PCI independent of the processor\nspeed yet provides the ability to receive and deliver data rapidly.\nBus Structure\nPCI may be configured as a 32- or 64-bit bus. There are 49 mandatory signal lines for PCI which\nare divided into the following functional groups:\n\u2022 System pins: Include the clock and reset pins.\n\u2022 Address and data pins: Include 32 lines that are time multiplexed for addresses and data.\nThe other lines in this group are used to interpret and validate the signal lines that carry the\naddresses and data."
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "\u2022 Interface control pins: Control the timing of transactions and provide coordination among\ninitiators and targets.\n\u2022 Arbitration pins: Unlike the other PCI signal lines, these are not shared lines. Rather, each\nPCI master has its own pair of arbitration lines that connect it directly to the PCI bus arbiter.\n\u2022 Error reporting pins: Used to report parity and other errors.\nIn addition, the PCI specification defines 51 optional signal lines, divided into the following\nfunctional groups:\n\u2022 Interrupt pins: These are provided for PCI devices that must generate requests for service.\nAs with the arbitration pins, these are not shared lines. Rather, each PCI device has its own\ninterrupt line or lines to an interrupt controller.\n\u2022 Cache support pins: These pins are needed to support a memory on PCI that can be cached\nin the processor or another device.\n\u2022 64-bit bus extension pins: Include 32 lines that are time multiplexed for ad dresses and\ndata and that are combined with the mandatory address/data lines to form a 64-bit address/data\nbus.\n\u2022 JTAG/boundary scan pins: These signal lines support testing procedures.\nPCI Commands\nBus activity occurs in the form of transactions between an initiator, or master, and a target.\nWhen a bus master acquires control of the bus, it determines the type of transaction that will occur\nnext The commands are as follows:\n\uf0b7Interrupt Acknowledge\n\uf0b7Special Cycle\n\uf0b7I/O Read\n\uf0b7I/O Write\n\uf0b7Memory Read\n\uf0b7Memory Read Line\n\uf0b7Memory Read Multiple\n\uf0b7Memory Write\n\uf0b7Memory Write and Invalidate\n\uf0b7Configuration Read\n\uf0b7Configuration Write\n\uf0b7Dual address Cycle\n1. Interrupt Acknowledge is a read command intended for the device that functions as an interrupt\ncontroller on the PCI bus.\n2. The Special Cycle command is used by the initiator to broadcast a message to one or more\ntargets.\n3. The I/O Read and Write commands are used to transfer data between the initiator and an I/O\ncontroller.\n4. The memory read and write commands are used to specify the transfer of a burst of data,\noccupying one or more clock cycles. The three memory read commands have the uses outlined in\nTable 1.4\nTable 1.4\nInterpretations of PCI Read Commands"
  },
  {
    "source": "Lecture 1 - Introduction microprocessor.pdf",
    "text": "5. The Memory Write command is used to transfer data in one or more data cycles to memory.\n6. The Memory Write and Invalidate command transfers data in one or more cycles to memory. In\naddition, it guarantees that at least one cache line is written.\nThe two configuration commands enable a master to read and update configuration parameters in a\ndevice connected to the PCI.\nThe  Dual  Address  Cycle  command  is  used  by  an  initiator  to  indicate  that  it  is  using  64-bit\naddressing.\nData Transfers\nEvery data transfer on the PCI bus is a single transaction consisting of one address phase and one or\nmore data phases.\nFigure 24 shows the timing of the read transaction. All events are synchronized to the falling\ntransitions of the clock, which occur in the middle of each clock cycle. Bus devices sample the bus\nlines on the rising edge at the beginning of a bus cycle. The following are the significant events,\nlabeled on the diagram:\na)Once a bus master has gained control of the bus, it may begin the transaction by asserting\nFRAME. This line remains asserted until the initiator is ready to complete the last data\nphase. The initiator also puts the start address on the address bus, and the read command on\nthe C/BE lines.\nb)At the start of clock 2, the target device will recognize its address on the AD lines.\nc)The initiator ceases driving the AD bus. A turnaround cycle (indicated by the two circular\narrows) is required on all signal lines that may be driven by more than one device, so that\nthe dropping of the address signal will prepare the bus for use by the target device. The\ninitiator changes the information on the C/BE lines to designate which AD lines are to be\nused for transfer for the currently addressed data (from 1 to 4 bytes). The initiator also\nasserts IRDY to indicate that it is ready for the first data item.\nd)The selected target asserts DEVSEL to indicate that it has recognized its address and will\nrespond. It places the requested data on the AD lines and asserts TRDY to indicate that valid\ndata are present on the bus.\ne)The initiator reads the data at the beginning of clock 4 and changes the byte enable lines as\nneeded in preparation for the next read.\nf)In  this  example,  the  target  needs  some  time  to  prepare  the  second  block  of  data  for\ntransmission. Therefore, it deasserts TRDY to signal the initiator that there will not be new\ndata during the coming cycle.Accordingly,the initiator does not read the data lines at the\nbeginning of the fifth clock cycle and does not change byte enable during that cycle. The\nblock of data is read at beginning of clock 6.\ng)During clock 6, the target places the third data item on the bus. However, in this example,\nthe initiator is not yet ready to read the data item (e.g., it has a temporary buffer full\ncondition). It therefore deasserts IRDY . This will cause the target to maintain the third data\nitem on the bus for an extra clock cycle.\nh)The initiator knows that the third data transfer is the last, and so it deasserts FRAME to\nsignal the target that this is the last data transfer. It also asserts IRDY to signal that it is ready\nto complete that transfer.\ni)The initiator deasserts IRDY , returning the bus to the idle state, and the target deasserts\nTRDY and DEVSEL.\nFigure 1.24 PCI Read Operation\nArbitration\nPCI makes use of a centralized, synchronous arbitration scheme in which each master has a unique\nrequest (REQ) and grant (GNT) signal. These signal lines are attached to a central arbiter (Figure\n25) and a simple request\u2013grant handshake is used to grant access to the bus.\nFigure 25 PCI Bus Arbiter\nFigure 26 is an example in which devices A and B are arbitrating for the bus. The following\nsequence occurs:\na. At some point before the start of clock 1, A has asserted its REQ signal.\nb. During clock cycle 1, B requests use of the bus by asserting its REQ signal.\nc. At the same time, the arbiter asserts GNT-A to grant bus access to A.\nd. Bus master A samples GNT-A at the beginning of clock 2 and learns that it has been granted bus\naccess. It also finds IRDY and TRDY deasserted, which indicates that the bus is idle. It also\ncontinues to assert REQ-A, because it has a second transaction to perform after this one.\ne. The bus arbiter samples all REQ lines at the beginning of clock 3 and makes an arbitration\ndecision to grant the bus to B for the next transaction. It then asserts GNT-B and deasserts GNT-A.\nB will not be able to use the bus until it returns to an idle state.\nf. A deasserts FRAME to indicate that the last data transfer is in progress.It puts the data on the data\nbus and signals the target with IRDY .The target reads the data at the beginning of the next clock\ncycle.\ng. At the beginning of clock 5, B finds IRDY and FRAME deasserted and so is able to take control\nof the bus by asserting FRAME. It also deasserts its REQ line, because it only wants to perform one\ntransaction.Subsequently, master A is granted access to the bus for its next transaction."
  }
]